\section*{Question 1: CG vs Gradient Descent}

Consider the system
\begin{equation*}
    A =
    \begin{bmatrix}
        2 & 0 \\
        0 & 8
    \end{bmatrix},
    \quad
    b =
    \begin{bmatrix}
        2 \\
        2
    \end{bmatrix}.
\end{equation*}

\begin{enumerate}
    \item Write the equivalent quadratic objective
        \begin{equation*}
            f(x) = \frac{1}{2} x^{\top} A x - b^{\top} x
        \end{equation*}

    \item Starting from \( x_{0} = {(0,0)}^{\top} \), compute:
        \begin{itemize}
            \item One step of steepest descent (take a step along \( -\nabla f\left(x_{0}\right) \) with exact line search).
            \item One step of conjugate gradient.
        \end{itemize}

    \item Compare the new iterates \( x_{1}^{\mathrm{GD}} \) and \( x_{1}^{\mathrm{CG}} \).

    \item Why does gradient descent exhibit a ``zig-zag'' pattern here, while CG terminates in at most 2 steps?
\end{enumerate}

\subsection*{Solution}

\subsection*{(1) Equivalent quadratic objective}

The equivalent quadratic objective \( f: \R^d \to \R \) is given by
\begin{equation*}
    \func{f}{\x}
    =
    \half \qf{\x}{\A}
    - \dotp{\b}{\x}
\end{equation*}

With
\(
    \x =
    \begin{bmatrix}
        x_1 & x_2
    \end{bmatrix}^\top
\), we get
\begin{equation*}
    \implies
    \func{f}{x_1, x_2}
    =
    \half
    \begin{bmatrix}
        x_1 & x_2
    \end{bmatrix}
    \begin{bmatrix}
        2 & 0 \\
        0 & 8
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\
        x_2
    \end{bmatrix}
    -
    \begin{bmatrix}
        2 & 2
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\
        x_2
    \end{bmatrix}
\end{equation*}
\begin{equation*}
    \therefore
    \boxed{
        \func{f}{x_1, x_2}
        =
        x_1^2 + 4 x_2^2 - 2 x_1 - 2 x_2,
        \quad \forall x_1, x_2 \in \R
    }
\end{equation*}

\begin{equation*}
    \A \xstar
    =
    \b
    \quad \iff \quad
    \xstar
    =
    \argmin{\x \in \R^d} \func{f}{\x}
\end{equation*}

\begin{equation*}
    \implies
    \xstar
    =
    \A^{-1} \b
    =
    \begin{bmatrix}
        2 & 0 \\
        0 & 8
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        2 \\
        2
    \end{bmatrix}
    =
    \frac{1}{16}
    \begin{bmatrix}
        8 & 0 \\
        0 & 2
    \end{bmatrix}
    \begin{bmatrix}
        2 \\
        2
    \end{bmatrix}
    =
    \frac{1}{16}
    \begin{bmatrix}
        16 \\
        4
    \end{bmatrix}
    \implies
    \boxed{
        \xstar
        =
        \begin{bmatrix}
            1 \\
            0.25
        \end{bmatrix}
    }
\end{equation*}

\subsection*{(2) One step of steepest descent and CG}

\paragraph{Steepest descent}

\begin{align*}
    \x^{k+1}
    & =
    \x^k - \alpha_k \grad{f}{\x^k},
    \quad
    \alpha_k
    =
    \argmin_{\alpha \in \R}
    \func{f}{\x^k - \alpha \grad{f}{\x^k}}
    \\ &
    \implies
    \alpha_k
    =
    \frac{\dotp{\grad{f}{\x^k}}{\grad{f}{\x^k}}}
    {\qf{\grad{f}{\x^k}}{\A}}
    > 0
    \\
    \implies
    \grad{f}{\x^0}
    & =
    A \x^0 - \b
    =
    -\b
    =
    \begin{bmatrix}
        -2 \\
        -2
    \end{bmatrix}
    \\
    \implies
    \alpha_0
    & =
    \frac{4}{
        \begin{bmatrix}
            -2 &
            -2
        \end{bmatrix}
        \begin{bmatrix}
            2 & 0 \\
            0 & 8
        \end{bmatrix}
        \begin{bmatrix}
            -2 \\
            -2
        \end{bmatrix}
    }
    =
    \frac{4}{
        \begin{bmatrix}
            -2 &
            -2
        \end{bmatrix}
        \begin{bmatrix}
            -4 \\
            -16
        \end{bmatrix}
    }
    =
    \frac{4}{40}
    =
    \frac{1}{10}
    \\
    \implies
    \x^1
    & =
    \x^0 - \alpha_0 \grad{f}{\x^0}
    =
    -\frac{1}{10}
    \begin{bmatrix}
        -2 \\
        -2
    \end{bmatrix}
    \implies
    \boxed{
        \x^1_{\mathrm{GD}}
        =
        \frac{1}{5}
        \begin{bmatrix}
            1 \\
            1
        \end{bmatrix}
    }
\end{align*}

\paragraph{Conjugate gradient}

\begin{align*}
    \x^{k+1}
    & =
    \x^k + \alpha_k \u_k,
    \quad
    \u_0
    =
    \r_0,
    \quad
    \r_k
    =
    -\grad{f}{\x^k}
    =
    \b - \A \x^k
    \\
    \alpha_k
    & =
    \frac{\dotp{\r_k}{\r_k}}{\qf{\u_k}{\A}},
    \quad
    \u_{k+1}
    =
    -\r_{k+1} + \beta_k \u_k,
    \quad
    \beta_k
    =
    \frac{\dotp{\r_{k+1}}{\r_{k+1}}}{\dotp{\r_k}{\r_k}}
    \\
    \implies
    \r_0
    & =
    \b,
    \quad
    \u_0
    =
    \r_0
    =
    \b
    \implies
    \alpha_0
    =
    \frac{1}{10}
    \\
    \implies
    \x^1
    & =
    \x^0 + \alpha_0 \u_0
    =
    \frac{1}{10}
    \begin{bmatrix}
        2 \\
        2
    \end{bmatrix}
    \implies
    \boxed{
        \x^1_{\mathrm{CG}}
        =
        \frac{1}{5}
        \begin{bmatrix}
            1 \\
            1
        \end{bmatrix}
    }
\end{align*}

\subsection*{(3) Comparison of new iterates}

Both gradient descent and conjugate gradient yield the same new iterate after one step:

\subsection*{(4) Zig-zag pattern in GD}

Gradient descent exhibits a ``zig-zag'' pattern because it updates the solution in the direction of the steepest descent, which is orthogonal to the level curves of the objective function, which are ellipses in this case.
Each step of gradient descent tends to overshoot in one direction and then corrects in the perpendicular direction in the next step, leading to a zig-zag trajectory towards the minimum.

The conjugate gradient method, effetively transforms the coordinate system to align with the principal axes of the ellipses, thereby converging in atmost \( d \) steps for a \( d \)-dimensional problem.
