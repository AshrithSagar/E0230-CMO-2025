\section*{Question 3}

\paragraph{Problem:}
To solve the linear system \( \mathbf{A} \mathbf{x} = \mathbf{b}, \quad \mathbf{A} \in \mathbb{R}^{m \times n}, \;\; \mathbf{x}, \mathbf{b} \in \mathbb{R}^n \).

\( m = 32768, n = 16384 \).

\paragraph{(1). Posed as a convex optimisation problem:}

The linear system can be posed as the following convex optimisation problem, a.k.a.\ the \textit{least squares problem}:
\begin{equation*}
    \min_{\mathbf{x} \in \mathbb{R}^n} \; \frac{1}{2} \| \mathbf{A} \mathbf{x} - \mathbf{b} \|_2^2
    =
    \min_{\mathbf{x} \in \mathbb{R}^n} \; \frac{1}{2} {(\mathbf{A} \mathbf{x} - \mathbf{b})}^\top (\mathbf{A} \mathbf{x} - \mathbf{b})
    =
    \min_{\mathbf{x} \in \mathbb{R}^n} \; \left( \frac{1}{2} \mathbf{x}^\top \mathbf{A}^\top \mathbf{A} \mathbf{x} - \mathbf{b}^\top \mathbf{A} \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b} \right)
\end{equation*}

Convex quadratic functions \( f : \mathbb{R}^n \to \mathbb{R} \) are of the form
\begin{equation*}
    f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{Q} \mathbf{x} + \mathbf{h}^\top \mathbf{x} + c,
    \qquad
    \text{where } \;
    \mathbf{Q} \in \mathbb{R}^{n \times n},
    \mathbf{Q} \succeq 0,
    \quad
    \mathbf{h} \in \mathbb{R}^n,
    \quad
    c \in \mathbb{R}
\end{equation*}
and we can see that the least squares problem is a convex quadratic function with
\begin{equation*}
    \mathbf{Q} = \mathbf{A}^\top \mathbf{A},
    \quad
    \mathbf{h} = -\mathbf{A}^\top \mathbf{b},
    \quad
    c = \frac{1}{2} \mathbf{b}^\top \mathbf{b}
\end{equation*}
We can verify that \( \mathbf{Q} \succeq 0 \), since for any \( \mathbf{z} \in \mathbb{R}^n \),
\begin{equation*}
    \mathbf{z}^\top \mathbf{Q} \mathbf{z}
    =
    \mathbf{z}^\top \mathbf{A}^\top \mathbf{A} \mathbf{z}
    =
    {(\mathbf{A} \mathbf{z})}^\top (\mathbf{A} \mathbf{z})
    =
    \| \mathbf{A} \mathbf{z} \|_2^2
    \geq 0
\end{equation*}
by non-negativity of the squared Euclidean norm.

\paragraph{(2). Solution using optimisation techniques:}

\paragraph{(3). Solution for the optimisation problem:}
\begin{enumerate}[label= (\alph*).]
    \item \( m < n \):
        This is an underdetermined system, i.e., there are more variables than equations.
        The system may either have infinitely many solutions (if consistent) or no solution at all (if inconsistent).
        We can find the minimum-norm solution using the least squares approach, given by
        \begin{equation*}
            \mathbf{x}^* = \mathbf{A}^\top {(\mathbf{A} \mathbf{A}^\top)}^{-1} \mathbf{b}
        \end{equation*}
        provided \( \mathbf{A} \) has full row rank, i.e., \( \operatorname{rank}(\mathbf{A}) = m \).

    \item \( m = n \):
        This is a square system, i.e., the number of variables equals the number of equations.
        If \( \mathbf{A} \) is non-singular (invertible), i.e., \( \operatorname{rank}(\mathbf{A}) = n \), then there exists a unique solution given by
        \begin{equation*}
            \mathbf{x}^* = \mathbf{A}^{-1} \mathbf{b}
        \end{equation*}
        If \( \mathbf{A} \) is singular (non-invertible), then the system may either have infinitely many solutions (if consistent) or no solution at all (if inconsistent).

    \item \( m > n \):
        This is an overdetermined system, i.e., there are more equations than variables.
        The system may either have a unique solution (if consistent) or no solution at all (if inconsistent).
        We can find the least squares solution using the normal equations, given by
        \begin{equation*}
            \mathbf{x}^* = {(\mathbf{A}^\top \mathbf{A})}^{-1} \mathbf{A}^\top \mathbf{b}
        \end{equation*}
        provided \( \mathbf{A} \) has full column rank, i.e., \( \operatorname{rank}(\mathbf{A}) = n \).
\end{enumerate}

\paragraph{(4). Complexity comparisions:}
We're further given that \( \mathbf{A} \) is full rank and square.
Thereby, we can see that the inverse \( \mathbf{A}^{-1} \) exists, and the system has a unique solution.

\begin{enumerate}[label= (\alph*).]
    \item Matrix inversion:
        The method involves explicitly computing the inverse \( \mathbf{A}^{-1} \) and then multiplying it with \( \mathbf{b} \) to obtain the solution \( \mathbf{x}^* = \mathbf{A}^{-1} \mathbf{b} \).
        Computing the inverse of an \( n \times n \) matrix typically has a time complexity of \( \mathcal{O}(n^3) \), and the matrix-vector multiplication has a time complexity of \( \mathcal{O}(n^2) \).
        Therefore, the overall time complexity of this method is dominated by the matrix inversion step, resulting in a total time complexity of \( \boxed{ \mathcal{O}(n^3) } \).

    \item Optimisation techniques:
        The formulated convex optimisation problem can be solved using iterative methods, typically taking about \( \boxed{ \mathcal{O}(k n^2) } \) time complexity, where \( k \) is the number of iterations required for convergence, and since each iteration involves matrix-vector multiplications with a time complexity of \( \mathcal{O}(n^2) \).
        Typically, \( k \) depends on the condition number \( \kappa(\mathbf{A}) \) and the desired accuracy \( \epsilon \), as
        \begin{equation*}
            k = \mathcal{O} \left( \kappa(\mathbf{A}) \log \left( \frac{1}{\epsilon} \right) \right),
            \quad
            \text{where } \;
            \kappa(\mathbf{A}) = \frac{\lambda_{\max}(\mathbf{A})}{\lambda_{\min}(\mathbf{A})}
        \end{equation*}
        For well-conditioned matrices (i.e., \( \kappa(\mathbf{A}) \approx 1 \)), the method converges quickly, while ill-conditioned matrices (i.e., large \( \kappa(\mathbf{A}) \)) lead to slower convergence.
\end{enumerate}

\paragraph{(5). Table of times taken to solve the system:}
