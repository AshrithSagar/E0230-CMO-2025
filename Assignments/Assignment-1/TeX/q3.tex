\section*{Question 3}

\paragraph{Problem:}
To solve the linear system \( \mathbf{A} \mathbf{x} = \mathbf{b}, \quad \mathbf{A} \in \mathbb{R}^{m \times n}, \;\; \mathbf{x}, \mathbf{b} \in \mathbb{R}^n \).

\( m = 32768, n = 16384 \).

\paragraph{(1). Posed as a convex optimisation problem:}

The linear system can be posed as the following convex optimisation problem, a.k.a.\ the \textit{least squares problem}:
\begin{equation*}
    \min_{\mathbf{x} \in \mathbb{R}^n} \; \frac{1}{2} \| \mathbf{A} \mathbf{x} - \mathbf{b} \|_2^2
    =
    \min_{\mathbf{x} \in \mathbb{R}^n} \; \frac{1}{2} {(\mathbf{A} \mathbf{x} - \mathbf{b})}^\top (\mathbf{A} \mathbf{x} - \mathbf{b})
    =
    \min_{\mathbf{x} \in \mathbb{R}^n} \; \left( \frac{1}{2} \mathbf{x}^\top \mathbf{A}^\top \mathbf{A} \mathbf{x} - \mathbf{b}^\top \mathbf{A} \mathbf{x} + \frac{1}{2} \mathbf{b}^\top \mathbf{b} \right)
\end{equation*}

Convex quadratic functions \( f : \mathbb{R}^n \to \mathbb{R} \) are of the form
\begin{equation*}
    f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{Q} \mathbf{x} + \mathbf{h}^\top \mathbf{x} + c,
    \qquad
    \text{where } \;
    \mathbf{Q} \in \mathbb{R}^{n \times n},
    \mathbf{Q} \succeq 0,
    \quad
    \mathbf{h} \in \mathbb{R}^n,
    \quad
    c \in \mathbb{R}
\end{equation*}
and we can see that the least squares problem is a convex quadratic function with
\begin{equation*}
    \mathbf{Q} = \mathbf{A}^\top \mathbf{A},
    \quad
    \mathbf{h} = -\mathbf{A}^\top \mathbf{b},
    \quad
    c = \frac{1}{2} \mathbf{b}^\top \mathbf{b}
\end{equation*}
We can verify that \( \mathbf{Q} \succeq 0 \), since for any \( \mathbf{z} \in \mathbb{R}^n \),
\begin{equation*}
    \mathbf{z}^\top \mathbf{Q} \mathbf{z}
    =
    \mathbf{z}^\top \mathbf{A}^\top \mathbf{A} \mathbf{z}
    =
    {(\mathbf{A} \mathbf{z})}^\top (\mathbf{A} \mathbf{z})
    =
    \| \mathbf{A} \mathbf{z} \|_2^2
    \geq 0
\end{equation*}
by non-negativity of the squared Euclidean norm.

\paragraph{(2). Solution using optimisation techniques:}

\paragraph{(3). Solution for the optimisation problem:}
\begin{enumerate}[label= (\alph*).]
    \item \( m < n \):
        This is an underdetermined system, i.e., there are more variables than equations.
        The system may either have infinitely many solutions (if consistent) or no solution at all (if inconsistent).
        We can find the minimum-norm solution using the least squares approach, given by
        \begin{equation*}
            \mathbf{x}^* = \mathbf{A}^\top {(\mathbf{A} \mathbf{A}^\top)}^{-1} \mathbf{b}
        \end{equation*}
        provided \( \mathbf{A} \) has full row rank, i.e., \( \operatorname{rank}(\mathbf{A}) = m \).

    \item \( m = n \):
        This is a square system, i.e., the number of variables equals the number of equations.
        If \( \mathbf{A} \) is non-singular (invertible), i.e., \( \operatorname{rank}(\mathbf{A}) = n \), then there exists a unique solution given by
        \begin{equation*}
            \mathbf{x}^* = \mathbf{A}^{-1} \mathbf{b}
        \end{equation*}
        If \( \mathbf{A} \) is singular (non-invertible), then the system may either have infinitely many solutions (if consistent) or no solution at all (if inconsistent).

    \item \( m > n \):
        This is an overdetermined system, i.e., there are more equations than variables.
        The system may either have a unique solution (if consistent) or no solution at all (if inconsistent).
        We can find the least squares solution using the normal equations, given by
        \begin{equation*}
            \mathbf{x}^* = {(\mathbf{A}^\top \mathbf{A})}^{-1} \mathbf{A}^\top \mathbf{b}
        \end{equation*}
        provided \( \mathbf{A} \) has full column rank, i.e., \( \operatorname{rank}(\mathbf{A}) = n \).
\end{enumerate}

\paragraph{(4). Complexity comparisions:}
We're further given that \( \mathbf{A} \) is full rank and square.
Thereby, we can see that the inverse \( \mathbf{A}^{-1} \) exists, and the system has a unique solution.

\begin{enumerate}[label= (\alph*).]
    \item Matrix inversion:
        The method involves explicitly computing the inverse \( \mathbf{A}^{-1} \) and then multiplying it with \( \mathbf{b} \) to obtain the solution \( \mathbf{x}^* = \mathbf{A}^{-1} \mathbf{b} \).
        Computing the inverse of an \( n \times n \) matrix typically has a time complexity of \( \mathcal{O}(n^3) \), and the matrix-vector multiplication has a time complexity of \( \mathcal{O}(n^2) \).
        Therefore, the overall time complexity of this method is dominated by the matrix inversion step, resulting in a total time complexity of \( \boxed{ \mathcal{O}(n^3) } \).

    \item Optimisation techniques:
        The formulated convex optimisation problem can be solved using iterative methods, typically taking about \( \boxed{ \mathcal{O}(k n^2) } \) time complexity, where \( k \) is the number of iterations required for convergence, and since each iteration involves matrix-vector multiplications with a time complexity of \( \mathcal{O}(n^2) \).
        Typically, \( k \) depends on the condition number \( \kappa(\mathbf{A}) \) and the desired accuracy \( \epsilon \), as
        \begin{equation*}
            k = \mathcal{O} \left( \kappa(\mathbf{A}) \log \left( \frac{1}{\epsilon} \right) \right),
            \quad
            \text{where } \;
            \kappa(\mathbf{A}) = \frac{\lambda_{\max}(\mathbf{A})}{\lambda_{\min}(\mathbf{A})}
        \end{equation*}
        For well-conditioned matrices (i.e., \( \kappa(\mathbf{A}) \approx 1 \)), the method converges quickly (\( \sim \mathcal{O}(n^2) \)), while ill-conditioned matrices (i.e., large \( \kappa(\mathbf{A}) \)) lead to slower convergence.
\end{enumerate}

\newpage
\paragraph{(5). Table of times taken to solve the system:}

Well-conditioned matrices (low condition number, \( \kappa(\mathbf{A}) \approx 1 \)) were generated by
\begin{equation*}
    \mathbf{A} = \mathbf{I} + \delta \mathbf{E},
\end{equation*}
where \( \mathbf{I} \) is the identity matrix of order \( m \), \( \mathbf{E} \) is a random matrix with entries drawn from a standard normal distribution, and \( \delta \) is a small positive scalar (here \( 0.001 \)).

% chktex-file 44
\begin{table}[!h]
    \centering
    \caption{
        Timing results for solving linear systems
    }{
        (Optimisation column refers to Steepest gradient descent with Wolfe line search)
    }
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Dimensions} & \textbf{Generation} & \textbf{Matrix inversion} & \textbf{Optimisation} & \textbf{numpy.linalg.solve} \\
        \midrule
        \( 2 \times 2 \)        & 1.110 ms    & 912 \( \mu \)s         & 2.324 ms   & 264 \( \mu \)s       \\
        \( 4 \times 4 \)        & 227 \( \mu \)s      & 235 \( \mu \)s         & 2.148 ms   & 293 \( \mu \)s       \\
        \( 8 \times 8 \)        & 280 \( \mu \)s      & 322 \( \mu \)s         & 2.131 ms   & 240 \( \mu \)s       \\
        \( 16 \times 16 \)      & 243 \( \mu \)s      & 235 \( \mu \)s         & 1.980 ms   & 255 \( \mu \)s       \\
        \( 32 \times 32 \)      & 242 \( \mu \)s      & 1.864 ms       & 2.433 ms   & 255 \( \mu \)s       \\
        \( 64 \times 64 \)      & 277 \( \mu \)s      & 530 \( \mu \)s         & 2.095 ms   & 426 \( \mu \)s       \\
        \( 128 \times 128 \)    & 429 \( \mu \)s      & 569 \( \mu \)s         & 2.087 ms   & 334 \( \mu \)s       \\
        \( 256 \times 256 \)    & 2.600 ms    & 903 \( \mu \)s         & 2.314 ms   & 502 \( \mu \)s       \\
        \( 512 \times 512 \)    & 3.461 ms    & 4.071 ms       & 2.551 ms   & 1.602 ms     \\
        \( 1024 \times 1024 \)  & 13.380 ms   & 21.172 ms      & 3.554 ms   & 5.631 ms     \\
        \( 2048 \times 2048 \)  & 51.792 ms   & 177.080 ms     & 16.507 ms  & 52.555 ms    \\
        \( 4096 \times 4096 \)  & 218.557 ms  & 1.167 s        & 66.361 ms  & 335.253 ms   \\
        \( 8192 \times 8192 \)  & 866.019 ms  & 8.447 s        & 405.616 ms & 2.605 s      \\
        \( 16384 \times 16384 \)& 4.628 s     & 1 min 26 s     & 7.863 s    & 26.643 s     \\
        \bottomrule
    \end{tabular}
\end{table}
