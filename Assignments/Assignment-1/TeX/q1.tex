\section*{Question 1}

\paragraph{Problem:}
We're given the following unconstrained optimisation problem:
\begin{equation*}
    \min_{\mathbf{x} \in \mathbb{R}^2} f(\mathbf{x}),
    \quad
    f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{Q} \mathbf{x} + \mathbf{b}^T \mathbf{x},
    \quad
    \mathbf{Q} \succ 0,
    \quad
    b =
    \begin{bmatrix}
        1 \\ 1
    \end{bmatrix}
\end{equation*}

\paragraph{(1). Exact line search:}

The optimal values obtained from the steepest gradient descent with exact line search are tabulated below.

% chktex-file 44
\begin{table}[!h]
    \centering
    \begin{tabular}{ccc}
        \toprule
        \textbf{Matrix} & \( \mathbf{x}^* \) & \( f(\mathbf{x}^*) \) \\
        \midrule

        \( Q_a \)
        &
        \(
            \begin{bmatrix}
                -0.845470 \\ -1.277985
            \end{bmatrix}
        \)
        &
        \( -1.061727 \) \\

        \( Q_b \)
        &
        \(
            \begin{bmatrix}
                -0.887593 \\ -0.083041
            \end{bmatrix}
        \)
        &
        \( -0.485317 \) \\

        \( Q_c \)
        &
        \(
            \begin{bmatrix}
                -2.267822 \\ -3.651999
            \end{bmatrix}
        \)
        &
        \( -2.959910 \) \\

        \( Q_d \)
        &
        \(
            \begin{bmatrix}
                -0.617076 \\ -0.362782
            \end{bmatrix}
        \)
        &
        \( -0.489929 \) \\

        \( Q_e \)
        &
        \(
            \begin{bmatrix}
                -3.481422 \\ -0.734369
            \end{bmatrix}
        \)
        &
        \( -1.373526 \) \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{(2). Analytical solution:}

\paragraph{(3). Convergence behaviour:}

The convergence of steepest descent with exact line search depends on the condition number, which is defined as
\begin{equation*}
    \kappa(Q) = \frac{\lambda_{\max}(Q)}{\lambda_{\min}(Q)}
\end{equation*}
where \( \lambda_{\max}(Q) \) and \( \lambda_{\min}(Q) \) are the largest and smallest eigenvalues of \( Q \), respectively.
If \( \kappa(Q) \approx 1 \) (i.e., well-conditioned), the method converges very quickly, while large \( \kappa(Q) \) (i.e., ill-conditioned) leads to slow convergence due to zig-zagging along narrow valleys.
The theoretical contraction factor is
\begin{equation*}
    \|x^{(k+1)} - x^*\|_Q \;\leq\; \rho \, \|x^{(k)} - x^*\|_Q,
    \quad \text{where } \rho = \frac{\kappa - 1}{\kappa + 1},
    \quad \|z\|_Q = \sqrt{z^T Q z}
\end{equation*}
