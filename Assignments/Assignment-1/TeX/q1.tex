\section*{Question 1}

\paragraph{Problem:}
We're given the following unconstrained optimisation problem:
\begin{equation*}
    \min_{\mathbf{x} \in \mathbb{R}^2} f(\mathbf{x}),
    \quad
    f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{Q} \mathbf{x} + \mathbf{b}^T \mathbf{x},
    \quad
    \mathbf{Q} \succ 0,
    \quad
    b =
    \begin{bmatrix}
        1 \\ 1
    \end{bmatrix}
\end{equation*}

\subsection*{(1). Exact line search:}

The exact line search method for convex quadratic functions is summarised in Algorithm 1.

\begin{algorithm}[H]
    \caption{
        Exact line search for convex quadratic functions
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Choose a descent direction \( \mathbf{p}^{(k)} \) from the set of descent directions
        \begin{equation*}
            \mathcal{DS} \left( \mathbf{x}^{(k)} \right) = \left\{ \mathbf{p} \in \mathbb{R}^d \;\big|\; {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p} < 0 \right\}
        \end{equation*}
        One such choice is the steepest descent direction (Cauchy direction):
        \begin{equation*}
            \mathbf{p}^{(k)} = -\nabla f(\mathbf{x}^{k})
        \end{equation*}

        Choose the step length \( \alpha_k \geq 0 \) by solving the one-dimensional optimisation problem
        \begin{equation*}
            \alpha_k = \argmin_{\alpha \geq 0} g_k(\alpha),
            \qquad \text{where } \;
            g_k(\alpha) \triangleq f\left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
        \end{equation*}
        For convex quadratic functions, the exact line search has a closed-form solution:
        \begin{equation*}
            \alpha_k = -\frac{{\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)}}{{\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
        \end{equation*}

        Update the current point: \( \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \mathbf{x}^{(k)} \)\;}
\end{algorithm}

\newpage
The optimal values obtained from the steepest gradient descent with exact line search are tabulated below.

% chktex-file 44
\begin{table}[!h]
    \centering
    \begin{tabular}{ccc}
        \toprule
        \textbf{Matrix} & \( \mathbf{x}^* \) & \( f(\mathbf{x}^*) \) \\
        \midrule

        \( Q_a \)
        &
        \(
            \begin{bmatrix}
                -0.845470 \\ -1.277985
            \end{bmatrix}
        \)
        &
        \( -1.061727 \) \\

        \( Q_b \)
        &
        \(
            \begin{bmatrix}
                -0.887593 \\ -0.083041
            \end{bmatrix}
        \)
        &
        \( -0.485317 \) \\

        \( Q_c \)
        &
        \(
            \begin{bmatrix}
                -2.267822 \\ -3.651999
            \end{bmatrix}
        \)
        &
        \( -2.959910 \) \\

        \( Q_d \)
        &
        \(
            \begin{bmatrix}
                -0.617076 \\ -0.362782
            \end{bmatrix}
        \)
        &
        \( -0.489929 \) \\

        \( Q_e \)
        &
        \(
            \begin{bmatrix}
                -3.481422 \\ -0.734369
            \end{bmatrix}
        \)
        &
        \( -1.373526 \) \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{(2). Analytical solution:}

The analytical solution for the minimum point \( \mathbf{x}^* \) can be derived from the first-order necessary condition of a local minimum, i.e., by setting the gradient of \( f(\mathbf{x}) \) to zero and solving for \( \mathbf{x} \):
\begin{equation*}
    \nabla f(\mathbf{x})
    = \mathbf{Q} \mathbf{x} + \mathbf{b} = \mathbf{0}
    \quad \implies \quad
    \mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{b}
\end{equation*}

The plot of the norm \( \|\mathbf{x}^{(k)} - \mathbf{x}^*\|_2 \) versus iteration \( k \) for each matrix \( \mathbf{Q} \) is shown below.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/q1.png}
\end{figure}

\newpage
\paragraph{(3). Convergence behaviour:}

The convergence of steepest descent with exact line search depends on the condition number, which is defined as
\begin{equation*}
    \kappa(Q) = \frac{\lambda_{\max}(Q)}{\lambda_{\min}(Q)}
\end{equation*}
where \( \lambda_{\max}(Q) \) and \( \lambda_{\min}(Q) \) are the largest and smallest eigenvalues of \( Q \), respectively.
If \( \kappa(Q) \approx 1 \) (i.e., well-conditioned), the method converges very quickly, while large \( \kappa(Q) \) (i.e., ill-conditioned) leads to slow convergence due to zig-zagging along narrow valleys.
The theoretical contraction factor is
\begin{equation*}
    \|x^{(k+1)} - x^*\|_Q \;\leq\; \rho \, \|x^{(k)} - x^*\|_Q,
    \quad \text{where } \rho = \frac{\kappa - 1}{\kappa + 1},
    \quad \|z\|_Q = \sqrt{z^T Q z}
\end{equation*}
