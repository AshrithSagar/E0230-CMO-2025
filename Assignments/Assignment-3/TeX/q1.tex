\section*{Question 1: LASSO Regression and KKT Conditions}

\paragraph{Given:}
Sparsity-promoting constrained optimisation problem:
\begin{equation}\label{eq:sparsity-promoting-constrained-optimisation}
    \begin{aligned}
        \minimize_{\bfbeta \in \R^n}
        \quad &
        \func{f}{\bfbeta}
        \\ \subjectto \quad &
        \norm{\bfbeta}_1 \leq t,
        \qquad
        t > 0
    \end{aligned}
\end{equation}

\quad
(Practical form):
\begin{equation}\label{eq:sparsity-promoting-constrained-optimisation-practical}
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1,
    \qquad
    \lambda > 0
\end{equation}
\quad
with regularisation parameter \( \lambda > 0 \).

LASSO regression problem:
\begin{equation}\label{eq:lasso}
    \minimize_{\bfbeta \in \R^n}
    \quad
    \half \norm{\bfX \bfbeta - \y}_2^2
    + \lambda \norm{\bfbeta}_1,
    \qquad
    \bfX \in \R^{m \times n},
    \; \y \in \R^m,
    \; \lambda > 0
\end{equation}
with number of samples \( m = 50 \), and number of features \( n = 15 \).

\subsection*{(1) Derivation of KKT Conditions of \peqref{eq:sparsity-promoting-constrained-optimisation}}

The Lagrangian function of the constrained optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation} is given by
\begin{equation*}
    \func{\calL}{\bfbeta, \lambda}
    =
    \func{f}{\bfbeta}
    + \lambda \pbrac[\big]{\norm{\bfbeta}_1 - t},
    \qquad
    \lambda \geq 0
\end{equation*}
with Lagrange multiplier \( \lambda \geq 0 \) for the constraint \( \norm{\bfbeta}_1 \leq t \).

The gradient of the Lagrangian function with respect to \( \bfbeta \) can be computed as
\begin{equation*}
    \func{\nabla_\bfbeta \, \calL}{\bfbeta, \lambda}
    =
    \func{\nabla_\bfbeta \, f}{\bfbeta}
    + \lambda \; \nabla_\bfbeta \bbrac[\Big]{\norm{\bfbeta}_1 - t}
    =
    \func{\nabla_\bfbeta \, f}{\bfbeta}
    + \lambda \; \nabla_\bfbeta \, \norm{\bfbeta}_1
\end{equation*}

The necessary conditions for optimality are given by the Karush-Kuhn-Tucker (KKT) conditions;
If \( \bfbeta^\ast \) and \( \lambda^\ast \) are optimal, then they must satisfy the KKT conditions, given by
\begin{equation*}
    \begin{aligned}
        \func{\nabla_\bfbeta \, \calL}{\bfbeta^\ast, \lambda^\ast}
        & = \zero
        \\
        \lambda^\ast \pbrac[\big]{\norm{\bfbeta^\ast}_1 - t}
        & = 0
    \end{aligned}
    \qquad
    \implies
    \boxed{
        \begin{aligned}
            \func{\nabla_\bfbeta \, f}{\bfbeta^\ast}
            + \lambda^\ast \; \nabla_\bfbeta \, \norm{\bfbeta^\ast}_1
            & = \zero
            \\
            \lambda^\ast \pbrac[\big]{\norm{\bfbeta^\ast}_1 - t}
            & = 0
        \end{aligned}
    }
\end{equation*}

In addition, we have that
\(
    \nabla_\bfbeta \, \norm{\bfbeta}_1
    =
    \func{\operatorname{sign}}{\bfbeta}
\),
where
\begin{equation*}
    \bbrac{\func{\operatorname{sign}}{\bfbeta}}_i
    =
    \begin{cases}
        1, & \text{if } \bbrac{\bfbeta}_i > 0
        \\
        -1, & \text{if } \bbrac{\bfbeta}_i < 0
        \\
        \text{undefined}, & \text{if } \bbrac{\bfbeta}_i = 0
    \end{cases},
    \qquad i = 1, 2, \ldots, n
\end{equation*}

\subsection*{(2) Equivalence of \peqref{eq:sparsity-promoting-constrained-optimisation} and \peqref{eq:sparsity-promoting-constrained-optimisation-practical}}

We use the method of Lagrange multipliers.
The constrained optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation} can be transformed into an unconstrained optimisation problem by introducing a Lagrange multiplier \( \lambda \geq 0 \) for the constraint \( \norm{\bfbeta}_1 \leq t \).
As seen earlier, the Lagrangian function for the constrained optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation} is given by
\begin{equation*}
    \func{\calL}{\bfbeta, \lambda}
    =
    \func{f}{\bfbeta}
    + \lambda \pbrac[\big]{\norm{\bfbeta}_1 - t}
    =
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1
    - \lambda t
\end{equation*}
\begin{equation*}
    \therefore
    \qquad
    \begin{aligned}
        \minimize_{\bfbeta \in \R^n}
        \quad &
        \func{f}{\bfbeta}
        \\ \subjectto \quad &
        \norm{\bfbeta}_1 \leq t,
        \quad
        t > 0
    \end{aligned}
    \qquad \equiv \qquad
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{\calL}{\bfbeta, \lambda}
\end{equation*}
Since \( - \lambda t \) is a constant with respect to \( \bfbeta \), minimising the Lagrangian function \( \func{\calL}{\bfbeta, \lambda} \) with respect to \( \bfbeta \) is equivalent to minimising the function
\(
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1
\)
with respect to \( \bfbeta \), i.e.,
\begin{equation*}
    \implies
    \qquad
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{\calL}{\bfbeta, \lambda}
    \qquad \equiv \qquad
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1
\end{equation*}
which is exactly the practical form of the sparsity-promoting optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation-practical}.

\subsection*{(3) Sparsity of LASSO Regression solution}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]
    {../Codes/figures/Q1-3-Sparsity-LASSO-24233.jpeg}% chktex 8
    \caption{
        Sparsity of \( \beta^\ast \) in LASSO Regression.
        As the regularisation parameter \( \lambda \) increases, the number of non-zero coefficients in the LASSO regression solution \( \beta^\ast \) decreases, demonstrating the sparsity-promoting effect of the \( \ell_1 \)-norm regularisation.
    }
\end{figure}

\subsection*{(4) Verification of KKT conditions for optimal \( \beta^\ast \)}

For \( \lambda = 0.01 \), the estimated coefficients \( \beta^\ast \) obtained are
\begin{verbatim}
[ 2.42893529 -0.0821506  -0.16435941  0.13798978  1.80153319 -0.14876385
  0.02172414 -0.1001286  -0.19886349 -0.17817296  0.18376004  0.01026027
  2.27626349  0.328471    0.1730505 ]
\end{verbatim}

For \( \lambda = 0.1 \), the estimated coefficients \( \beta^\ast \) obtained are
\begin{verbatim}
[ 2.42544676 -0.07602968 -0.15159893  0.13839134  1.79015623 -0.13976299
  0.02467282 -0.09358195 -0.19649837 -0.170963    0.17615336  0.00766717
  2.2706338   0.31608294  0.16572506]
\end{verbatim}

For \( \lambda = 1 \), the estimated coefficients \( \beta^\ast \) obtained are
\begin{verbatim}
[ 2.39034708e+00 -1.00862398e-02 -2.90320126e-02  1.43143486e-01
  1.67398859e+00 -5.09159445e-02  5.27888873e-02 -2.72017447e-02
 -1.73514244e-01 -1.02827788e-01  9.56998788e-02  2.14513594e-21
  2.21051100e+00  1.93916743e-01  9.02177795e-02]
\end{verbatim}

\subsection*{(5) Effect of highly correlated features on LASSO Regression}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]
    {../Codes/figures/Q1-5-Sparsity-LASSO-DupFeature-24233.jpeg}% chktex 8
    \caption{
        Sparsity of \( \beta^\ast \) in LASSO Regression with a duplicated feature (here, column 5 is duplicated).
    }
\end{figure}
