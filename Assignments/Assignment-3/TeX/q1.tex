\section*{Question 1: LASSO Regression and KKT Conditions}

\paragraph{Given:}
Sparsity-promoting constrained optimisation problem:
\begin{equation}\label{eq:sparsity-promoting-constrained-optimisation}
    \begin{aligned}
        \minimize_{\bfbeta \in \R^n}
        \quad &
        \func{f}{\bfbeta}
        \\ \subjectto \quad &
        \norm{\bfbeta}_1 \leq t,
        \qquad
        t > 0
    \end{aligned}
\end{equation}

\quad
(Practical form):
\begin{equation}\label{eq:sparsity-promoting-constrained-optimisation-practical}
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1,
    \qquad
    \lambda > 0
\end{equation}
\quad
with regularisation parameter \( \lambda > 0 \).

LASSO regression problem:
\begin{equation}\label{eq:lasso}
    \minimize_{\bfbeta \in \R^n}
    \quad
    \half \norm{\bfX \bfbeta - \y}_2^2
    + \lambda \norm{\bfbeta}_1,
    \qquad
    \bfX \in \R^{m \times n},
    \; \y \in \R^m,
    \; \lambda > 0
\end{equation}
with number of samples \( m = 50 \), and number of features \( n = 15 \).

\subsection*{(1) Derivation of KKT Conditions of \peqref{eq:sparsity-promoting-constrained-optimisation}}

The Lagrangian function of the constrained optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation} is given by
\begin{equation*}
    \func{\calL}{\bfbeta, \lambda}
    =
    \func{f}{\bfbeta}
    + \lambda \pbrac[\big]{\norm{\bfbeta}_1 - t},
    \qquad
    \lambda \geq 0
\end{equation*}
with Lagrange multiplier \( \lambda \geq 0 \) for the constraint \( \norm{\bfbeta}_1 \leq t \).

The gradient of the Lagrangian function with respect to \( \bfbeta \) can be computed as
\begin{equation*}
    \func{\nabla_\bfbeta \, \calL}{\bfbeta, \lambda}
    =
    \func{\nabla_\bfbeta \, f}{\bfbeta}
    + \lambda \; \nabla_\bfbeta \bbrac[\Big]{\norm{\bfbeta}_1 - t}
    =
    \func{\nabla_\bfbeta \, f}{\bfbeta}
    + \lambda \; \nabla_\bfbeta \, \norm{\bfbeta}_1
\end{equation*}

The necessary conditions for optimality are given by the Karush-Kuhn-Tucker (KKT) conditions;
If \( \bfbeta^\ast \) and \( \lambda^\ast \) are optimal, then they must satisfy the KKT conditions, given by
\begin{equation*}
    \begin{aligned}
        \func{\nabla_\bfbeta \, \calL}{\bfbeta^\ast, \lambda^\ast}
        & = \zero
        \\
        \lambda^\ast \pbrac[\big]{\norm{\bfbeta^\ast}_1 - t}
        & = 0
    \end{aligned}
    \qquad
    \implies
    \boxed{
        \begin{aligned}
            \func{\nabla_\bfbeta \, f}{\bfbeta^\ast}
            + \lambda^\ast \; \nabla_\bfbeta \, \norm{\bfbeta^\ast}_1
            & = \zero
            \\
            \lambda^\ast \pbrac[\big]{\norm{\bfbeta^\ast}_1 - t}
            & = 0
        \end{aligned}
    }
\end{equation*}

In addition, we have the subgradient as
\begin{equation*}
    \nabla_\bfbeta \, \norm{\bfbeta}_1
    =
    \begin{cases}
        1, & \text{if } \bbrac{\bfbeta}_i > 0
        \\
        -1, & \text{if } \bbrac{\bfbeta}_i < 0
        \\
        [-1, 1], & \text{if } \bbrac{\bfbeta}_i = 0
    \end{cases},
    \qquad i = 1, 2, \ldots, n
\end{equation*}

\subsection*{(2) Equivalence of \peqref{eq:sparsity-promoting-constrained-optimisation} and \peqref{eq:sparsity-promoting-constrained-optimisation-practical}}

We use the method of Lagrange multipliers.
The constrained optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation} can be transformed into an unconstrained optimisation problem by introducing a Lagrange multiplier \( \lambda \geq 0 \) for the constraint \( \norm{\bfbeta}_1 \leq t \).
As seen earlier, the Lagrangian function for the constrained optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation} is given by
\begin{equation*}
    \func{\calL}{\bfbeta, \lambda}
    =
    \func{f}{\bfbeta}
    + \lambda \pbrac[\big]{\norm{\bfbeta}_1 - t}
    =
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1
    - \lambda t
\end{equation*}
\begin{equation*}
    \therefore
    \qquad
    \begin{aligned}
        \minimize_{\bfbeta \in \R^n}
        \quad &
        \func{f}{\bfbeta}
        \\ \subjectto \quad &
        \norm{\bfbeta}_1 \leq t,
        \quad
        t > 0
    \end{aligned}
    \qquad \equiv \qquad
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{\calL}{\bfbeta, \lambda}
\end{equation*}
Since \( - \lambda t \) is a constant with respect to \( \bfbeta \), minimising the Lagrangian function \( \func{\calL}{\bfbeta, \lambda} \) with respect to \( \bfbeta \) is equivalent to minimising the function
\(
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1
\)
with respect to \( \bfbeta \), i.e.,
\begin{equation*}
    \implies
    \qquad
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{\calL}{\bfbeta, \lambda}
    \qquad \equiv \qquad
    \minimize_{\bfbeta \in \R^n}
    \quad
    \func{f}{\bfbeta}
    + \lambda \norm{\bfbeta}_1
\end{equation*}
which is exactly the practical form of the sparsity-promoting optimisation problem in \peqref{eq:sparsity-promoting-constrained-optimisation-practical}.

\newpage
\subsection*{(3) Sparsity of LASSO Regression solution}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]
    {../Codes/figures/Q1-3-Sparsity-LASSO-24233.jpeg}% chktex 8
    \caption{
        Sparsity of \( \beta^\ast \) in LASSO Regression.
        As the regularisation parameter \( \lambda \) increases, the number of non-zero coefficients in the LASSO regression solution \( \beta^\ast \) decreases, demonstrating the sparsity-promoting effect of the \( \ell_1 \)-norm regularisation.
    }
\end{figure}

For \( \lambda = 0.01 \), the estimated coefficients \( \beta^\ast \) obtained are
\begin{verbatim}
[ 2.42893529 -0.0821506  -0.16435941  0.13798978  1.80153319 -0.14876385
  0.02172414 -0.1001286  -0.19886349 -0.17817296  0.18376004  0.01026027
  2.27626349  0.328471    0.1730505 ]
\end{verbatim}

For \( \lambda = 0.1 \), the estimated coefficients \( \beta^\ast \) obtained are
\begin{verbatim}
[ 2.42544676 -0.07602968 -0.15159893  0.13839134  1.79015623 -0.13976299
  0.02467282 -0.09358195 -0.19649837 -0.170963    0.17615336  0.00766717
  2.2706338   0.31608294  0.16572506]
\end{verbatim}

For \( \lambda = 1 \), the estimated coefficients \( \beta^\ast \) obtained are
\begin{verbatim}
[ 2.39034708e+00 -1.00862398e-02 -2.90320126e-02  1.43143486e-01
  1.67398859e+00 -5.09159445e-02  5.27888873e-02 -2.72017447e-02
 -1.73514244e-01 -1.02827788e-01  9.56998788e-02  2.14513594e-21
  2.21051100e+00  1.93916743e-01  9.02177795e-02]
\end{verbatim}

\newpage
\subsection*{(4) Verification of KKT conditions for optimal \( \beta^\ast \)}

For \( \lambda = 0.01 \), the residual \( \pbrac{\bfX \beta^\ast - \y} \) and gradient \( \dotp{\bfX}{\pbrac{\bfX \beta^\ast - \y}} \) respectively are
\begin{verbatim}
[ 0.27906611  0.30735772 -0.07400798 -0.21453129  0.40423741  0.17552416
  0.46164546 -0.35671177  0.20424238  0.48974195 -0.39455506 -0.2797047
 -0.11443163  0.52188669  0.23231429 -0.01246941  0.6397258  -0.20340596
 -0.32000668  0.09736558  0.39834473  0.01119577  0.14848126  0.53054913
 -0.22672293  0.01669374 -0.10093836 -0.62469666 -0.35862479  0.13146829
  0.26718118  0.92025994 -0.12886891  0.162096    0.2682515  -0.17349895
 -0.82497091 -0.07614388 -0.01321095 -0.67548456  0.30903005  0.34999142
  0.02364681 -0.47076441 -0.26800765 -0.18671772  0.19787046 -0.03805798
  0.31172933 -0.11393497]
[-0.01  0.01  0.01 -0.01 -0.01  0.01 -0.01  0.01  0.01  0.01 -0.01 -0.01
 -0.01 -0.01 -0.01]
\end{verbatim}

For \( \lambda = 0.1 \), the residual \( \pbrac{\bfX \beta^\ast - \y} \) and gradient \( \dotp{\bfX}{\pbrac{\bfX \beta^\ast - \y}} \) respectively are
\begin{verbatim}
[ 0.26779729  0.30168898 -0.06813849 -0.20831275  0.40718449  0.18360129
  0.47497596 -0.37410827  0.19951182  0.46962704 -0.38749465 -0.27852618
 -0.10333258  0.51666772  0.24826256 -0.02047764  0.63381238 -0.19680066
 -0.32414693  0.10045459  0.42454573  0.00115188  0.15582766  0.51459336
 -0.21663822  0.01806736 -0.09115342 -0.63912848 -0.35320847  0.1397563
  0.25471758  0.92021717 -0.13830227  0.15053965  0.26853859 -0.21112191
 -0.82709515 -0.07271357  0.005116   -0.64788282  0.32115776  0.36617175
  0.02671521 -0.48482926 -0.26673393 -0.21064144  0.20477912 -0.03477054
  0.30670347 -0.08998157]
[-0.1  0.1  0.1 -0.1 -0.1  0.1 -0.1  0.1  0.1  0.1 -0.1 -0.1 -0.1 -0.1
 -0.1]
\end{verbatim}

For \( \lambda = 1 \), the residual \( \pbrac{\bfX \beta^\ast - \y} \) and gradient \( \dotp{\bfX}{\pbrac{\bfX \beta^\ast - \y}} \) respectively are
\begin{verbatim}
[ 0.15471848  0.23868775 -0.00402674 -0.14654693  0.43315943  0.26705604
  0.58507223 -0.53164534  0.17735213  0.24757887 -0.33738435 -0.2708471
  0.00740536  0.48172205  0.4123128  -0.08480357  0.54848484 -0.10218745
 -0.37591057  0.13911179  0.69193688 -0.1105446   0.22100673  0.36023855
 -0.12237722  0.03113544  0.00979083 -0.77129737 -0.29197266  0.22197056
  0.14102803  0.94591744 -0.21982808  0.05137512  0.26173185 -0.58668418
 -0.84126793 -0.0391666   0.17174096 -0.37927227  0.45065179  0.52780213
  0.05816314 -0.62089612 -0.26220618 -0.45880655  0.27354903  0.00973597
  0.25878235  0.14445547]
[-1.          1.          1.         -1.         -1.          1.
 -1.          1.          1.          1.         -1.         -0.60263634
 -1.         -1.         -1.        ]
\end{verbatim}

As can be seen from the above gradient values, for all values of \( \lambda \in \set{0.01, 0.1, 1} \), we have that \( \abs{\dotp{\bfX}{\pbrac{\bfX \beta^\ast - \y}}} \leq \lambda \) within numerical tolerance.
Thus, the KKT conditions are satisfied by the optimal solution \( \beta^\ast \) obtained from LASSO regression in \peqref{eq:lasso}.

\newpage
\subsection*{(5) Effect of highly correlated features on LASSO Regression}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]
    {../Codes/figures/Q1-5-Sparsity-LASSO-DupFeature-24233.jpeg}% chktex 8
    \caption{
        Sparsity of \( \beta^\ast \) in LASSO Regression with a duplicated feature (here, column 5 is duplicated).
    }
\end{figure}

When a feature is duplicated in the dataset, LASSO regression tends to distribute the coefficient values between the original and duplicated features.
As a result, the coefficients for both the original and duplicated features may be non-zero, but their magnitudes are typically smaller compared to when only one of the features is present.
This behavior can lead to less interpretable models, as it becomes challenging to determine the true importance of each feature when they are highly correlated or duplicated.
