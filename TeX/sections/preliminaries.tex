
\chapter{Real analysis}

\section{Metric space}

A \textbf{metric space} \( (S, d) \) is a set \( S \) together with a \textit{metric/distance function} \( d: S \times S \to \R_{\geq 0} \) that satisfies the following axioms for all \( x, y, z \in S \):
\begin{itemize}
    \item \( \func{d}{x, y} = 0 \iff x = y \)
    \item \textit{Symmetry}: \( \func{d}{x, y} = \func{d}{y, x} \)
    \item \textit{Triangle inequality}: \( d(x, z) \leq \func{d}{x, y} + \func{d}{y, z} \)
\end{itemize}

\section{Norm}

A \textbf{norm} is a real-valued function \( \eta: V \to \R_{\geq 0} \) that satisfies
\begin{itemize}
    \item \( \func{\eta}{x} \geq 0, \quad \forall x \in V \)
    \item \( \func{\eta}{x} = 0 \iff x = 0 \)
    \item \( \func{\eta}{s x} = \abs{s} \, \func{\eta}{x}, \quad \forall x \in V, s \in F \)
    \item \( \func{\eta}{x + y} \leq \func{\eta}{x} + \func{\eta}{y}, \quad \forall x, y \in V \)
\end{itemize}

\section{Vector norm}

A \textbf{vector norm} is a norm defined on a vector space.

A \textbf{normed vector space} \( (\V, \norm{\,\cdot\,}) \) is a vector space \( \V \) over a field \( \mathbb{F} \) (usually \( \R \) or \( \mathbb{C} \)) equipped with a function \( \norm{\,\cdot\,}: \V \to \R_{\geq 0} \) that satisfies the following properties for all \( \u, \v \in \V \) and \( a \in \mathbb{F} \):
\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\v} \geq 0 \)
    \item \textit{Definiteness}: \( \norm{\v} = 0 \iff \v = \zero \)
    \item \textit{Homogeneity}: \( \norm{a \v} = \abs{a} \, \norm{\v} \)
    \item \textit{Triangle inequality}: \( \norm{\u + \v} \leq \norm{\u} + \norm{\v} \)
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}

\begin{equation*}
    \norm{\x}_p
    =
    {\left( \sum_{i = 1}^{d} \abs{x_i}^p \right)}^{\frac{1}{p}},
    \quad p \geq 1
\end{equation*}

\subsubsection{\( L_1 \) norm (Manhattan norm)}

\begin{equation*}
    \norm{\x}_1
    =
    \abs{x_1} + \abs{x_2} + \cdots + \abs{x_d}
    =
    \sum_{i = 1}^{d} \abs{x_i}
\end{equation*}

\subsubsection{\( L_2 \) norm (Euclidean norm)}

\begin{equation*}
    \norm{\x}_2
    =
    \sqrt{x_1^2 + x_2^2 + \cdots + x_d^2}
    =
    \sqrt{\dotp{\x}{\x}}
    \triangleq
    \norm{\x}
\end{equation*}

\subsubsection{\( L_{\infty} \) norm (Maximum norm)}

\begin{equation*}
    \norm{\x}_{\infty}
    =
    \max \set{\abs{x_1}, \abs{x_2}, \ldots, \abs{x_d}}
    =
    \max_{1 \leq i \leq d} \abs{x_i}
\end{equation*}

\subsection{Energy norm}

For \( \Q \in \PD \), the \textbf{energy norm} (or \( \Q \)-norm) is defined as
\begin{equation*}
    \norm{\x}_{\Q}
    =
    \sqrt{\qf{\x}{\Q}}
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item \( \norm{\x}_{\I} = \norm{\x}_2 \)

    \item \( \norm{\x}_{\Q} = \norm{\Q^\half \x}_2 \)

    \item \( \norm{\x}_{\Q} = \norm{\x}_{\Q^\half} \)

    \item \( \lambda_{\min}(\Q) \; \norm{\x}^2 \leq \norm{\x}_{\Q}^2 \leq \lambda_{\max}(\Q) \; \norm{\x}^2 \)

    \item \( \norm{\x}_{\Q} = \norm{\Q \x}_{\Qinv} \)

    \item \( \norm{\x}_{\Q} = \norm{\Qinv \x}_{\Q} \)
\end{itemize}

\section{Matrix norm}

A \textbf{matrix norm} is a norm defined on a space of matrices.

\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\A} \geq 0 \)

    \item \textit{Definiteness}: \( \norm{\A} = 0 \iff \A = \zero \)

    \item \textit{Homogeneity}: \( \norm{a \A} = \abs{a} \, \norm{\A} \)

    \item \textit{Triangle inequality}: \( \norm{\A + \B} \leq \norm{\A} + \norm{\B} \)
\end{itemize}

\begin{itemize}
    \item \textit{Sub-multiplicativity}: \( \norm{\A \B} \leq \norm{\A} \, \norm{\B} \)

    \item \textit{Consistency}: \( \norm{\A \x} \leq \norm{\A} \, \norm{\x}, \quad \forall \x \in \R^d \)

    \item \textit{Transpose invariance}: \( \norm{\A} = \norm{\A^\top} \)

    \item \textit{Continuity}: \( \norm{\A} \) is continuous with respect to the entries of \( \A \)
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}

\begin{equation*}
    \norm{\A}_p
    =
    \sup_{\x \neq \zero} \frac{\norm{\A \x}_p}{\norm{\x}_p}
    =
    \max_{\norm{\x}_p = 1} \norm{\A \x}_p,
    \quad p \geq 1
\end{equation*}

\subsubsection{\( L_1 \) norm (Maximum absolute column sum norm)}

\begin{equation*}
    \norm{\A}_1
    =
    \max_{1 \leq j \leq d} \sum_{i = 1}^{d} \abs{a_{ij}}
    =
    \max_{1 \leq j \leq d} \norm{\A_{\ast j}}_1
\end{equation*}

\subsubsection{\( L_2 \) norm (Spectral norm)}

\begin{equation*}
    \norm{\A}_2
    =
    \sqrt{\lambda_{\max}(\A^\top \A)}
    =
    \sigma_{\max}(\A)
\end{equation*}

For \( \A \in \SD, \norm{\A}_2 = \rho(\A) = \abs{\lambda_{\max}(\A)} \).

\subsubsection{\( L_{\infty} \) norm (Maximum absolute row sum norm)}

\begin{equation*}
    \norm{\A}_{\infty}
    =
    \max_{1 \leq i \leq d} \sum_{j = 1}^{d} \abs{a_{ij}}
    =
    \max_{1 \leq i \leq d} \norm{\A_{i \ast}}_1
\end{equation*}

\subsection{Frobenius norm (Euclidean norm)}

\begin{equation*}
    \norm{\A}_F
    =
    \sqrt{\sum_{i = 1}^{d} \sum_{j = 1}^{d} a_{ij}^2}
    =
    \sqrt{\trace{\A^\top \A}}
    =
    \sqrt{\sum_{i = 1}^{d} \sigma_i^2(\A)}
    =
    \norm{\operatorname{vec}(\A)}_2
\end{equation*}

The vector \( \operatorname{vec}(\A) \) is the \textbf{vectorization} of matrix \( \A \), obtained by stacking its columns into a single column vector, i.e.,
\begin{equation*}
    \A
    =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1d} \\
        a_{21} & a_{22} & \cdots & a_{2d} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{d1} & a_{d2} & \cdots & a_{dd}
    \end{bmatrix}
    \in \R^{d \times d}
    \implies
    \operatorname{vec}(\A)
    =
    \begin{bmatrix}
        a_{11} \\
        a_{21} \\
        \vdots \\
        a_{d1} \\
        a_{12} \\
        a_{22} \\
        \vdots \\
        a_{d2} \\
        \vdots \\
        a_{1d} \\
        a_{2d} \\
        \vdots \\
        a_{dd}
    \end{bmatrix}
    \in \R^{d^2}
\end{equation*}

\subsection{Spectral radius}

The \textbf{spectral radius} of a matrix \( \A \in \R^{d \times d} \) with eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \) is defined as
\begin{equation*}
    \rho(\A)
    =
    \max \set{ \abs{\lambda_1}, \abs{\lambda_2}, \ldots, \abs{\lambda_d} }
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item \( \rho(\A) = \rho(\A^\top) \)

    \item \( \rho(\A^k) = {\left[ \rho(\A) \right]}^k, \quad k \in \N \)

    \item \( \rho(\inv{\A}) = \frac{1}{\rho(\A)}, \quad \text{if } \A \text{ is invertible} \)

    \item \( \rho(\A) \leq \norm{\A}, \quad \forall \text{ matrix norms } \norm{\,\cdot\,} \)

    \item (Gelfand's formula) \( \rho(\A) = \lim_{k \to \infty} {\left( \norm{\A^k} \right)}^{\frac{1}{k}}, \quad \forall \text{ matrix norms } \norm{\,\cdot\,} \)

    \item \( \rho(\A) \leq \norm{\A^k}^{\frac{1}{k}}, \quad \forall k \in \N, \; \forall \text{ sub-multiplicative matrix norms } \norm{\,\cdot\,} \)

    \item \( \rho(\A \B) \leq \rho(\A) \, \rho(\B) \), if \( \A \) and \( \B \) commute, i.e., \( \A \B = \B \A \)

    \item \( \A \in \R^{d \times d} \implies \rho(\A) \leq \norm{\A}_2 \)

    \item \( \A \in \SD \implies \rho(\A) = \norm{\A}_2 \)

    \item \( \A \in \SD \implies \norm{\A \mathbf{x}}_2 \leq \rho(\A) \, \norm{\x}_2, \quad \forall \x \in \R^d \)
\end{itemize}

\section{Neighborhood}\label{sec:neighborhood}

A \textbf{neighborhood} of a point \( \x \in \R^d \) is an open ball centered at \( \x \) with radius \( r > 0 \), defined as
\begin{equation*}
    B_{r}(\x)
    =
    \set{\z \in \R^d \given \norm{\z - \x} < r},
    \quad r > 0,
    \; \x \in \R^d
\end{equation*}

\section{Interior point}

A point \( \x \in S \) is an \textbf{interior point} of set \( S \subseteq \R^d \) if there exists a neighborhood of \( \x \) that is entirely contained within \( S \), i.e., there exists an \( r > 0 \) such that \( B_{r}(\x) \subseteq S \).

\section{Limit point}

A point \( \x \in \R^d \) is a \textbf{limit point} of set \( S \subseteq \R^d \) if every neighborhood of \( \x \) contains at least one point of \( S \) different from \( \x \) itself, i.e., for every \( r > 0 \), there exists a point \( \y \in S \setminus \set{\x} \) such that \( \y \in B_{r}(\x) \).
\begin{equation*}
    \forall r > 0, \; B_{r}(\x) \cap (S \setminus \set{\x}) \neq \emptyset
\end{equation*}

Trivially, every interior point of \( S \) is also a limit point of \( S \).

\section{Open set}

A set \( S \subseteq \R^d \) is \textbf{open} if every point in \( S \) is an interior point of \( S \).

\section{Closed set}

A set \( S \subseteq \R^d \) is \textbf{closed} if it contains all its limit points, i.e., if \( \x \) is a limit point of \( S \), then \( \x \in S \).

\section{Bounded set}

A set \( S \subseteq \R^d \) is \textbf{bounded} if there exists \( \z \in \R^d, M \in \R \) such that \( \norm{\z - \x} < M, \; \forall \x \in S \).

\section{Infimum}

The \textbf{infimum} (greatest lower bound) of a set \( S \subseteq \R \) is the largest real number \( m \) such that \( m \leq x, \; \forall x \in S \).

\section{Supremum}

The \textbf{supremum} (least upper bound) of a set \( S \subseteq \R \) is the smallest real number \( M \) such that \( M \geq x, \; \forall x \in S \).

\section{Closure}

The \textbf{closure} of a set \( S \subseteq \R^d \) is the smallest closed set containing \( S \), which can be obtained by adding all limit points of \( S \) to \( S \) itself.
\begin{equation*}
    \operatorname{closure}{(S)} = S \cup \set{\text{all limit points of } S}
\end{equation*}

\section{Convergent sequence}

A sequence of real numbers \( \set{a_n}_{n = 1}^{\infty} \) is said to \textbf{converge} to a limit \( L \in \R \) if
\begin{equation*}
    a_n \to L
    \quad \iff \quad
    \lim_{n \to \infty} a_n = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall n > N \implies \abs{a_n - L} < \epsilon
\end{equation*}

\section{Cauchy sequence}

A sequence of real numbers \( \set{a_k}_{k = 1}^{\infty} \) is a \textbf{Cauchy sequence} if
\begin{equation*}
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall m, n > N \implies \abs{a_n - a_m} < \epsilon
\end{equation*}

In \( \R \), every Cauchy sequence is convergent, and every convergent sequence is Cauchy, since \( \R \) is complete.

\chapter{Calculus}

\section{Limits}

\begin{definition}{\( \epsilon \)--\( \delta \) definition of limit of a function}{}
    The \textbf{limit} of a function \( f: D \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( D \):
    \begin{equation*}
        \lim_{\x \to \x_0} \func{f}{\x} = L
        \quad \iff \quad
        \forall \epsilon > 0, \
        \exists \delta > 0, \
        \text{s.t.} \
        0 < \norm{\x - \x_0} < \delta, \,
        \x \in D
        \implies
        \abs{\func{f}{\x} - L} < \epsilon
    \end{equation*}
    i.e., \( \func{f}{\x} \) can be made arbitrarily close to \( L \) by making \( \x \) sufficiently close to \( \x_0 \).
\end{definition}

Alternate notation~\psecref{sec:neighborhood}:
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \x \in D \cap B_{\delta}(\x_0) \setminus \set{\x_0}
    \implies
    \func{f}{\x} \in B_{\epsilon}(L)
\end{equation*}

\begin{definition}{Sequence definition of limit of a function}{}
    Sequence definition of \textbf{limit} of a function \( f: D \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( D \):
    \begin{equation*}
        \lim_{\x \to \x_0} \func{f}{\x} = L
        \,\iff\,
        \lim_{n \to \infty} \abs{\func{f}{\x_n} - L} = 0
        \,\iff\,
        \forall \set{\x_n}_{n = 1}^{\infty} \subseteq D \setminus \set{\x_0},
        \; \x_n \to \x_0
        \implies
        \func{f}{\x_n} \to L
    \end{equation*}
\end{definition}

\section{Continuity}

For a function \( f: D \subseteq \R^d \to \R \),
\begin{equation*}
    \lim_{\x \to \mathbf{c}} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \abs{\func{f}{\x} - L} < \epsilon, \
    \forall \x \in D \cap B_{\delta}(\mathbf{c}) \setminus \set{\mathbf{c}}
\end{equation*}

\section{Continuous functions}

A function \( f: D \subseteq \R^d \to \R \) is \textbf{continuous} at a point \( \x_0 \in D \) iff
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = \func{f}{\x_0}
\end{equation*}

\section{Derivative}

The \textbf{derivative} of a function \( f: \R \to \R \) at a point \( x \in \R \) is defined as
\begin{equation*}
    f'(x) = \lim_{h \to 0} \frac{\func{f}{x + h} - \func{f}{x}}{h}
\end{equation*}

The \textbf{gradient} of a function \( f: \R^d \to \R \) at a point \( \x = {\left(x_1, x_2, \ldots, x_d\right)}^\top \in \R^d \) is defined as
\begin{equation*}
    \grad{f}{\x} = {\left( \frac{\partial f}{\partial x_1}(\x), \frac{\partial f}{\partial x_2}(\x), \ldots, \frac{\partial f}{\partial x_d}(\x) \right)}^\top
    \in \R^d
\end{equation*}

The \textbf{Hessian} of a function \( f: \R^d \to \R \) at a point \( \x = {\left(x_1, x_2, \ldots, x_d\right)}^\top \in \R^d \) is defined as
\begin{equation*}
    \hess{f}{\x} =
    \begin{bmatrix}
        \cfrac{\partial^2 f}{\partial x_1^2}(\x) & \cfrac{\partial^2 f}{\partial x_1 \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_1 \partial x_d}(\x) \\
        \cfrac{\partial^2 f}{\partial x_2 \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_2^2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_2 \partial x_d}(\x) \\
        \vdots & \vdots & \ddots & \vdots \\
        \cfrac{\partial^2 f}{\partial x_d \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_d \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_d^2}(\x)
    \end{bmatrix}
    \in \R^{d \times d}
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item If \( f \) is differentiable at \( x \), then \( f \) is continuous at \( x \).

    \item \( \func{f}{\x} = \half \qf{\x}{\A}, \; \A = \u \v^\top, \; \u, \v \in \R^d, \; \u \neq \v \implies \hess{f}{\x} = \half (\A + \A^\top) = \half (\u \v^\top + \v \u^\top) \).
\end{itemize}

\section{Fundamental theorem of calculus}\label{sec:fundamental-theorem-of-calculus}

\begin{theorem}{First fundamental theorem of calculus}{}
    Let \( f: [a, b] \to \R \) be a continuous function.
    Then the function \( F: [a, b] \to \R \) defined by
    \begin{equation*}
        \func{F}{x} = \int_{a}^{x} \func{f}{t} \, dt
    \end{equation*}
    is continuous on \( [a, b] \), differentiable on \( (a, b) \), and
    \begin{equation*}
        \func{F'}{x} = \func{f}{x},
        \quad \forall x \in (a, b)
    \end{equation*}
\end{theorem}

This means that the integral of \( f \) defines an antiderivative of \( f \), i.e., an antiderivative of \( f \) can be constructed by integrating \( f \).

\begin{theorem}{Second fundamental theorem of calculus / Newton-Leibniz theorem}{}
    Let \( f: [a, b] \to \R \) be a continuous function.

    Let \( F \) be any antiderivative of \( f \) on \( [a, b] \), i.e., \( \func{F'}{x} = \func{f}{x}, \; \forall x \in (a, b) \).
    Then
    \begin{equation*}
        \int_{a}^{b} \func{f}{x} \, dx = \func{F}{b} - \func{F}{a}
    \end{equation*}
\end{theorem}

This tells us that we can evaluate definite integrals using antiderivatives.

\section{Intermediate value theorem}

\begin{theorem}{Intermediate value theorem}{}
    Let \( f: D \subseteq \R \to \R \) be continuous on the closed interval \( [a, b] \subseteq D \), then \( f \) takes on any given value between \( \func{f}{a} \) and \( \func{f}{b} \) at some point within the interval, i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \min \set{\func{f}{a}, \func{f}{b}} \leq N \leq \max \set{\func{f}{a}, \func{f}{b}}
        \implies
        \exists c \in [a, b] \text{ such that } \func{f}{c} = N
    \end{equation*}
\end{theorem}

\section{Wierstrass extreme value theorem}

If a function \( f: S \to \R \) is continuous on a set \( S \subseteq \R^d \) that is closed and bounded, then \( f \) attains its maximum and minimum values on \( S \), i.e., there exist points \( \x_{\min}, \x_{\max} \in S \) such that
\begin{equation*}
    \func{f}{\x_{\min}} \leq \func{f}{\x} \leq \func{f}{\x_{\max}}, \quad \forall \x \in S
\end{equation*}

\section{Taylor's theorem}

\subsection{Univariate Taylor's theorem}

Let \( f: \R \to \R \) be \( (n+1) \) times continuously differentiable on an interval containing \( a \) and \( x \).
Then, for some \( \xi \) between \( a \) and \( x \),
\begin{align*}
    \func{f}{x}
    & =
    \func{f}{a} + f'(a)(x - a) + \frac{f''(a)}{2!}{(x - a)}^2 + \cdots + \frac{f^{(n)}(a)}{n!}{(x - a)}^n + R_{n+1}(x)
    \\ & =
    \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!} {(x - a)}^k + R_{n+1}(x)
\end{align*}
where the remainder term \( R_{n+1}(x) \) is given by the Lagrange form:
\begin{equation*}
    R_{n+1}(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} {(x - a)}^{n+1}
\end{equation*}

\subsection{Multivariate Taylor's theorem}\label{sec:multivariate-taylor-theorem}

\section{Levels of smoothness}

A function \( f: \R^d \to \R \) is said to be of \textbf{class} \( \C^k \) if all partial derivatives of \( f \) up to and including order \( k \in \N \) exist and are continuous.

If a function is of class \( \C^k \; \forall k \in \N \), it is said to be of class \( \C^{\infty} \) or \textbf{smooth}.

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \C^0 \), then \( f \) is continuous.

    \item If \( f \in \C^k \), then \( f \in \C^{k-1} \), for \( k \geq 1 \).

    \item (Schwarz's theorem) If \( f \in \C^2 \), then \( \hess{f}{\x} \in \mathbf{S}_d\), i.e., \( \hess{f}{\x} \) is symmetric.
        \begin{equation*}
            \frac{\partial^2 f}{\partial x_i \partial x_j}
            =
            \frac{\partial^2 f}{\partial x_j \partial x_i},
            \quad \forall i, j = 1, 2, \ldots, d
        \end{equation*}

    \item If \( f, g \in \C^k \), then \( f + g, f \cdot g \in \C^k \).

    \item If \( f \in \C^k \), then \( \frac{\partial f}{\partial x_i} \in \C^{k-1}, \; \forall i = 1, 2, \ldots, d \).
\end{itemize}

\section{Lipschitz}

A function \( f: \R^d \to \R \) is said to be \( L \)-\textbf{smooth} if it is differentiable and there exists a constant \( L > 0 \) such that
\begin{equation*}
    \norm{\grad{f}{\x} - \grad{f}{\y}}_2
    \leq
    L \; \norm{\x - \y}_2,
    \quad \forall \x, \y \in \R^d
\end{equation*}

The constant \( L \) is called the \textbf{Lipschitz constant} of the gradient \( \nabla f \).

Here, we denote \( f \in \C^{1}_L \) to mean that \( f \) is \( L \)-smooth and of class \( \C^1 \).

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \C^{2} \), then \( f \in \C^{1}_L \) iff \( \hess{f}{\x} \preceq L \I, \; \forall \x \in \R^d \).

    \item If \( f \in \C^{1}_L \), then
        \begin{equation*}
            \abs{ \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x) }
            \leq
            \frac{L}{2} \norm[\big]{\y - \x}^2,
            \quad \forall \x, \y \in \R^d
        \end{equation*}

        \begin{proof}
            Follows from \( \func{f}{\x} \pm \frac{L}{2} \norm{\x}^2 \) being convex functions, given \( f \in \C^{1}_L \).
        \end{proof}
\end{itemize}

\chapter{Linear Algebra}

\section{Vectors}

A \textbf{vector} \( \x \in \R^d \) is an ordered tuple of \( d \) real numbers, i.e.,
\begin{equation*}
    \x
    =
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_d
    \end{bmatrix}^\top,
    \quad x_i \in \R,
    \; \forall i = 1, 2, \ldots, d
\end{equation*}

\subsection{Linear combinations}

A \textbf{linear combination} of a set of vectors \( \set{\v_i}_{i = 1}^{n} \subseteq \R^d \) is defined as
\begin{equation*}
    \sum_{i = 1}^{n} \alpha_i \v_i,
    \quad \alpha_i \in \R,
    \; \forall i \in \set{1, 2, \ldots, n}
\end{equation*}

\subsection{Linear independence}

A set of vectors \( \set{\v_i}_{i = 1}^{n} \subseteq \R^d \) are said to be \textbf{linearly independent} if
\begin{equation*}
    \sum_{i = 1}^{n} \alpha_i \v_i = \zero,
    \quad \alpha_i \in \R,
    \; \forall i \in \set{1, 2, \ldots, n}
    \quad \implies \quad
    \alpha_i = 0,
    \; \forall i \in \set{1, 2, \ldots, n}
\end{equation*}

Otherwise, they are said to be \textbf{linearly dependent}.

\section{Linear span}

The \textbf{linear span} / \textbf{linear hull} of a set of vectors \( \set{\v_i}_{i = 1}^{n}, \v_i \in \R^d \) is defined as the set of all possible linear combinations of the vectors, i.e.,
\begin{equation*}
    \spanset{\v_i}_{i = 1}^{n}
    =
    \set{
        \sum_{i = 1}^{n} \alpha_i \v_i
        \given
        \alpha_i \in \R,
        \; \forall i \in \set{1, 2, \ldots, n}
    }
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \spanset{\v_i}_{i = 1}^{n} \) is a subspace of \( \R^d \).

    \item If \( \set{\v_i}_{i = 1}^{n} \) are linearly independent, then \( \dim{\spanset{\v_i}_{i = 1}^{n}} = n \).

    \item If \( n < d \), then \( \spanset{\v_i}_{i = 1}^{n} \subsetneq \R^d \), i.e., \( \spanset{\v_i}_{i = 1}^{n} \) is a proper subspace of \( \R^d \).

    \item If \( n = d \) and \( \set{\v_i}_{i = 1}^{n} \) are linearly independent, then \( \spanset{\v_i}_{i = 1}^{n} = \R^d \).

    \item If \( n > d \), then \( \set{\v_i}_{i = 1}^{n} \) are linearly dependent.

    \item \( \spanset{\e_i}_{i = 1}^{d} = \R^d  \).
\end{itemize}

\section{Basis}

A \textbf{basis} of a subspace \( \V \subseteq \R^d \) is a set of linearly independent vectors \( \set{\v_i}_{i = 1}^{k} \subseteq \V \) such that
\begin{equation*}
    \V
    =
    \spanset{\v_i}_{i = 1}^{k}
\end{equation*}
i.e., the basis is a linearly independent spanning set of \( \V \).

The vectors \( \set{\v_i}_{i = 1}^{k} \) are called the \textbf{basis vectors} of \( \V \).

The number of vectors \( k \) in the basis is called the \textbf{dimension} of the subspace \( \V \), denoted as \( \dim{\V} = k \).

\subsection{Properties}

\begin{itemize}
    \item Any vector \( \x \in \V \) can be expressed (uniquely) as a linear combination of the basis vectors.
        \begin{equation*}
            \x
            =
            \sum_{i = 1}^{k} c_i \v_i,
            \quad c_i \in \R,
            \; \forall i \in \set{1, 2, \ldots, k}
        \end{equation*}
        The coefficients \( c_i \) are called the \textbf{coordinates}/\textbf{components} of \( \x \) with respect to the basis \( \set{\v_i}_{i = 1}^{k} \).

    \item Every vector space \( \V \subseteq \R^d \) has a basis.

    \item A vector space can have several bases; however, all bases of a vector space have the same dimension.\\
        This is known as the \textbf{dimension theorem} for (finite-dimensional) vector spaces.\\
        (Can be shown by Steinitz exchange lemma).

    \item The orthogonal complement of \( \V \) is given by
        \begin{equation*}
            \V^\perp
            =
            \set{
                \x \in \R^d \given \dotp{\x}{\v_i} = 0,
                \; \forall i = 1, 2, \ldots, k
            }
        \end{equation*}
\end{itemize}

\section{Orthonormal basis}

A set of vectors \( \set{\v_i}_{i = 1}^{k} \subseteq \R^d \) is said to form an \textbf{orthonormal basis} of a subspace \( \V \subseteq \R^d \) if
\begin{itemize}
    \item \textit{Basis}: \( \V = \spanset{\v_i}_{i = 1}^{k} \)
    \item \textit{Orthogonality}: \( \dotp{\v_i}{\v_j} = 0, \; \forall i \neq j \)
    \item \textit{Normality}: \( \norm{\v_i}_2 = 1, \; \forall i \)
\end{itemize}

\section{Dot product}

The \textbf{dot product} (or inner product) of two vectors \( \x, \y \in \R^d \) is defined as
\begin{equation*}
    \dotp{\x}{\y}
    =
    \sum_{i = 1}^{d} x_i y_i
    \quad \in \R
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \textit{Commutative}: \( \dotp{\x}{\y} = \dotp{\y}{\x}, \; \forall \x, \y \in \R^d \).

    \item \textit{Distributive}: \( \dotp{\x}{(\y + \z)} = \dotp{\x}{\y} + \dotp{\x}{\z}, \; \forall \x, \y, \z \in \R^d \).

    \item \textit{Associative}: \( \dotp{(a \x)}{\y} = \dotp{\x}{(a \y)} = a \left( \dotp{\x}{\y} \right), \; \forall \x, \y \in \R^d, \; a \in \R \).

    \item \textit{Definiteness}: \( \dotp{\x}{\x} = \norm{\x}_2^2 \geq 0, \; \forall \x \in \R^d, \; \text{and } \dotp{\x}{\x} = 0 \iff \x = \zero \).

    \item \textit{Orthogonality}: \( \x \perp \y \iff \dotp{\x}{\y} = 0 \).

    \item Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality}: \( \abs{\dotp{\x}{\y}} \leq \norm{\x}_2 \, \norm{\y}_2, \; \forall \x, \y \in \R^d \).

    \item \( \dotp{\x}{\y} = \norm{\x}_2 \, \norm{\y}_2 \cos \theta \), where \( \theta \) is the angle between \( \x \) and \( \y \).

    \item \( \dotp{\x}{\y} = \trace{\outp{\y}{\x}}, \; \forall \x, \y \in \R^d \).

    \item \( \dotp{\x}{\y} = \norm{\x}_2^2, \iff \x = \y \).
\end{itemize}

\section{Outer product}

The \textbf{outer product} of two vectors \( \x \in \R^m \) and \( \y \in \R^n \) is defined as
\begin{equation*}
    \outp{\x}{\y} =
    \begin{bmatrix}
        x_1 y_1 & x_1 y_2 & \cdots & x_1 y_n \\
        x_2 y_1 & x_2 y_2 & \cdots & x_2 y_n \\
        \vdots & \vdots & \ddots & \vdots \\
        x_m y_1 & x_m y_2 & \cdots & x_m y_n
    \end{bmatrix}
    \in \R^{m \times n}
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \rank{\outp{\x}{\y}} = 1, \; \forall \x \in \R^m \setminus \set{\zero}, \; \forall \y \in \R^n \setminus \set{\zero} \).

    \item \( \outp{\x}{\x} \in \mathbf{S}_d^+, \; \forall \x \in \R^d \).

    \item \( \operatorname{spec}(\outp{\x}{\x}) = \set{\norm{\x}_2^2, 0^{(d-1)}} \).
        Eigenvectors: \( \set{\x} \cup \underbrace{ \set{\v \given \dotp{\v}{\x} = 0} }_{\x^\perp} \).

        Eigenvalues of \( \outp{\x}{\x} \) are \( \norm{\x}_2^2 \) and \( 0 \) (with multiplicity \( d-1 \)).
        The eigenvector corresponding to \( \norm{\x}_2^2 \) is \( \x \); the eigenspace corresponding to \( 0 \) is the orthogonal complement \( \x^\perp \).
\end{itemize}

\section{Matrices}

A \textbf{matrix} \( \A \in \R^{m \times n} \) is a rectangular array of real numbers with \( m \) rows and \( n \) columns, i.e.,
\begin{equation*}
    \A
    =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix},
    \quad a_{ij} \in \R,
    \; \forall i \in \set{1, 2, \ldots, m},
    \; \forall j \in \set{1, 2, \ldots, n}
\end{equation*}

\subsection{Symmetric Matrices}\label{sec:symmetric-matrices}

A matrix \( \A \in \R^{d \times d} \) is symmetric if \( \A = \A^\top \), i.e., \( a_{ij} = a_{ji}, \; \forall i, j \).

\paragraph{Properties}

\begin{itemize}
    \item \( \A, \B \in \SD \implies \A + \B \in \SD \).
        \quad \( \because {(\A + \B)}^\top = \A^\top + \B^\top = \A + \B \).

    \item \( \A \in \SD, c \in \R \implies c \A \in \SD \).
        \quad \( \because {(c \A)}^\top = c \A^\top = c \A \).

    \item \( \A \in \SD \implies \A^k \in \SD, \; k \in \N \).
        \quad \( \because {(\A^k)}^\top = {(\A^\top)}^k = \A^k \).

    \item \( \A \in \SD \implies \inv{\A} \in \SD, \text{ if } \A \text{ is invertible} \).
        \quad \( \because {(\inv{\A})}^\top = \inv{(\A^\top)} = \inv{\A} \).
\end{itemize}

\subsection{Symmetric Positive Semi-definite Matrices}\label{sec:symmetric-positive-semi-definite-matrices}

A symmetric matrix \( \A \in \SD \) is \textbf{positive semi-definite} (PSD), denoted as \( \A \succeq \zero \), if
\begin{equation*}
    \x^\top \A \x \geq 0,
    \quad \forall \x \in \R^d
\end{equation*}

\subsection{Symmetric Positive Definite Matrices}\label{sec:symmetric-positive-definite-matrices}

A symmetric matrix \( \A \in \SD \) is \textbf{positive definite} (PD), denoted as \( \A \succ \zero \), if
\begin{equation*}
    \x^\top \A \x > 0,
    \quad \forall \x \in \R^d \setminus \set{\zero}
\end{equation*}

\section{Eigenvalues and Eigenvectors}

\begin{equation*}
    \A\v = \lambda \v
\end{equation*}
where
\( \A \in \R^{d \times d} \), \quad
\( \lambda \in \R \longrightarrow \) Eigenvalue, \quad
\( \v \in \R^d \setminus \set{\zero} \longrightarrow \) corresponding Eigenvector.

The set of all eigenvalues of \( \A \) is called the \textbf{spectrum} of \( \A \).

\subsection{Properties}

\begin{itemize}
    \item \( \trace{\A} = \sum_{i = 1}^{d} a_{ii} = \sum_{i = 1}^{d} \lambda_i \).

    \item \( \determinant{\A} = \prod_{i = 1}^{d} \lambda_i \).

    \item \( \A \in \SD \implies \A \) has an orthonormal basis of eigenvectors in \( \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \A \in \SD \implies \operatorname{spec}(\A) \subseteq \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \A \in \SD \implies \dotp{\v_i}{\v_j} = 0, \; \text{if } \lambda_i \neq \lambda_j \) \, \psecref{sec:spectral-decomposition-theorem}
\end{itemize}

\section{Spectral Decomposition Theorem}\label{sec:spectral-decomposition-theorem}

\begin{theorem}{Spectral Decomposition Theorem}{}
    For any \( \A \in \SD \), there exists an orthonormal basis of eigenvectors \( \set{\v_i}_{i = 1}^{d}, \v_i \in \R^d \setminus \set{\zero} \), with corresponding eigenvalues \( \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d, \; \lambda_i \in \R \), such that
    \begin{equation*}
        \A = \sum_{i = 1}^{d} \lambda_i \outp{\v_i}{\v_i}
        = \V \boldsymbol{\Lambda} \V^\top
        = \V \boldsymbol{\Lambda} \inv{\V}
    \end{equation*}
    where \( \V =
        \begin{bmatrix}
            \v_1 & \v_2 & \cdots & \v_d
        \end{bmatrix}
    \in \R^{d \times d} \) is an orthogonal matrix (i.e., \( \dotp{\V}{\V} = \outp{\V}{\V} = \I \)) and \( \boldsymbol{\Lambda} = \diag{\lambda_1, \lambda_2, \ldots, \lambda_d} \in \R^{d \times d} \) is a diagonal matrix containing the eigenvalues of \( \A \).
\end{theorem}

\begin{proof}
    The eigenvalues of a real symmetric matrix are all real
    (Can be shown considering Hermitian matrices, Fundamental theorem of algebra, and properties of complex conjugates).

    The eigenvectors corresponding to distinct eigenvalues are orthogonal.
    This can be shown using the fact that for any two eigenvectors \( \v_i \) and \( \v_j \) corresponding to distinct eigenvalues \( \lambda_i \) and \( \lambda_j \), we have
    \begin{align*}
        \v_i^\top \A \v_j
        & =
        \v_i^\top (\A \v_j)
        =
        \lambda_j \dotp{\v_i}{\v_j},
        \qquad \text{Similarly, } \;
        \v_j^\top \A \v_i
        =
        \lambda_i \v_j^\top \v_i
        \\
        \text{Now, } \;
        \v_i^\top \A \v_j
        & =
        \v_i^\top \A^\top \v_j
        =
        {(\A \v_i)}^\top \v_j
        =
        {(\lambda_i \v_i)}^\top \v_j
        =
        \lambda_i \dotp{\v_i}{\v_j}
        =
        \lambda_i \v_j^\top \v_i
        =
        \v_j^\top \A \v_i
        \\
        \implies
        \lambda_j \dotp{\v_i}{\v_j}
        & =
        \lambda_i \dotp{\v_i}{\v_j}
        \implies
        (\lambda_j - \lambda_i) \dotp{\v_i}{\v_j}
        = 0
        \implies
        \dotp{\v_i}{\v_j} = 0,
        \; \because
        \lambda_i \neq \lambda_j,
        \;\;
        \therefore
        \v_i, \v_j \text{ are orthogonal.}
    \end{align*}

    For repeated eigenvalues, if any, we can always find a set of orthonormal eigenvectors using the Gram-Schmidt process.
    Thereby, we can always find \( d \) orthonormal eigenvectors of \( \A \), thus forming an orthonormal basis for \( \R^d \).
    This gives that \( \V \) is an orthogonal matrix \( \implies \dotp{\V}{\V} = \I \implies \inv{\V} = \V^\top \).

    Now, with \( \boldsymbol{\Lambda} = \diag{\lambda_1, \lambda_2, \ldots, \lambda_d} \) and \( \V = [\v_1, \v_2, \ldots, \v_d] \), we have
    \begin{equation*}
        \A \V
        =
        [\A \v_1, \A \v_2, \ldots, \A \v_d]
        =
        [\lambda_1 \v_1, \lambda_2 \v_2, \ldots, \lambda_d \v_d]
        =
        \V \boldsymbol{\Lambda}
        \implies
        \A
        =
        \V \boldsymbol{\Lambda} \inv{\V}
        =
        \V \boldsymbol{\Lambda} \V^\top
    \end{equation*}
\end{proof}

\begin{itemize}
    \item The identity matrix \( \I \in \R^{d \times d} \) has eigenvalues \( \lambda_i = 1, \; \forall i \) and the corresponding eigenvectors can be chosen as any orthonormal basis of \( \R^d \), commonly taken as the standard basis vectors \( \mathbf{e}_i \in \R^d \).

    \item \( \A \in \PSD \iff \A \in \SD, \text{ and } \lambda_i \geq 0, \; \forall i \).

    \item \( \A \in \PD \iff \A \in \SD, \text{ and } \lambda_i > 0, \; \forall i \).

    \item \( \A \in \PD, \; \kappa(\A) = 1 \iff \A = c \I, \; c > 0 \).
\end{itemize}

\section{Condition number}

The \textbf{condition number} of a matrix \( \A \in \SD \) is given by
\begin{equation*}
    \kappa(\A) = \frac{\abs{\lambda_{\max}(\A)}}{\abs{\lambda_{\min}(\A)}}
\end{equation*}
where \( \lambda_{\max}(\A) \) and \( \lambda_{\min}(\A) \) are the largest and smallest eigenvalues of \( \A \) respectively.

\chapter{Inequalities}

\section{AM-GM inequality}

\begin{theorem}{AM-GM inequality}{}
    For any non-negative real numbers \( a_1, a_2, \ldots, a_n \geq 0 \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \frac{1}{n} \sum_{i = 1}^{n} a_i
        \geq
        {\left( \prod_{i = 1}^{n} a_i \right)}^{\frac{1}{n}}
        \qquad \text{i.e.,} \quad
        \overbrace{
            \left( \frac{a_1 + a_2 + \cdots + a_n}{n} \right)
        }^{\text{Arithmetic Mean (AM)}}
        \geq
        \overbrace{
            \sqrt[n]{a_1 a_2 \ldots a_n}
        }^{\text{Geometric Mean (GM)}}
    \end{equation*}
    with equality iff \( a_1 = a_2 = \cdots = a_n \).
\end{theorem}

\begin{proof}
    Can be shown by mathematical induction.
\end{proof}

\section{Cauchy-Schwarz inequality}\label{sec:cauchy-schwarz-inequality}

\begin{theorem}{Cauchy-Schwarz inequality}{}
    For any vectors \( \x, \y \in \R^d \), we have
    \vspace{-1em}
    \begin{equation*}
        \abs{\dotp{\x}{\y}} \leq \norm{\x}_2 \; \norm{\y}_2
    \end{equation*}
    with equality iff \( \x \) and \( \y \) are linearly dependent, i.e., \( \exists c \in \R \) such that \( \x = c \y \) or \( \y = c \x \).
\end{theorem}

\begin{proof}
    Follows in the \hyperlink{line-point-minimisation}{problem} of minimising the distance between a point and a line in \( \R^d \).
\end{proof}

\section{H{\"o}lder's inequality}

\begin{theorem}{H{\"o}lder's inequality}{}
    For any vectors \( \x, \y \in \R^d \) and for any H{\"o}lder conjugates \( p, q > 1 \) such that \( \frac{1}{p} + \frac{1}{q} = 1 \), we have
    \vspace{-1em}
    \begin{equation*}
        \abs{\dotp{\x}{\y}} \leq \norm{\x}_p \; \norm{\y}_q
    \end{equation*}
    with equality iff \( \exists c > 0 \) such that \( \abs{x_i}^p = c \abs{y_i}^q, \; \forall i = 1, 2, \ldots, d \).
\end{theorem}

\section{Kantorovich inequality}\label{sec:kantorovich-inequality}

\begin{theorem}{Kantorovich inequality}{}
    Let \( p_i \geq 0, \; 0 < x_{\min} \leq x_i \leq x_{\max}, \; \forall i \in \set{1, 2, \ldots, n} \), then we have
    \vspace{-1em}
    \begin{equation*}
        \left( \sum_{i = 1}^{n} p_i x_i \right) \left( \sum_{i = 1}^{n} \frac{p_i}{x_i} \right)
        \leq
        \frac{{(x_{\max} + x_{\min})}^2}{4 x_{\max} x_{\min}}
        {\left( \sum_{i = 1}^{n} p_i \right)}^2
    \end{equation*}
    with equality iff \( x_i \in \set{x_{\min}, x_{\max}}, \; \forall i \in \set{1, 2, \ldots, n} \).
\end{theorem}

\begin{corollary}{Kantorovich inequality for real symmetric positive definite matrices~\citep{Luenberger1984}}{}
    For any \( \Q \in \PD \), with smallest eigenvalue \( \lambda_{\min} \) and largest eigenvalue \( \lambda_{\max} \), and for any \( \x \in \R^d \setminus \set{\zero} \), we have
    \begin{equation*}
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
    with equality iff \( \x \) is an eigenvector of \( \Q \) corresponding to either \( \lambda_{\min} \) or \( \lambda_{\max} \).
\end{corollary}

\begin{proof}
    Let the matrix \( \Q \) have eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \), with corresponding orthonormal eigenvectors \( \v_1, \v_2, \ldots, \v_d \).
    Since the eigenvectors form an orthonormal basis for \( \R^d \), any vector \( \x \in \R^d \) can be expressed as a linear combination of these eigenvectors as \( \x = \sum_{i = 1}^{d} c_i \v_i \), where each coefficient \( c_i \) is the projection of \( \x \) onto the eigenvector \( \v_i \), given by \( c_i = \v_i^\top \x \).
    Now,
    \begin{align*}
        \qf{\x}{\Q}
        & =
        {\left( \sum_{i = 1}^{d} c_i \v_i \right)}^\top \Q \left( \sum_{j = 1}^{d} c_j \v_j \right)
        =
        \sum_{i = 1}^{d} \sum_{j = 1}^{d} c_i c_j \qf{\v_i}{\Q}[\v_j]
        \\ & =
        \sum_{i = 1}^{d} c_i^2 \qf{\v_i}{\Q}
        +
        \sum_{i \neq j} c_i c_j \qf{\v_i}{\Q}[\v_j]
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i \underbrace{ \dotp{\v_i}{\v_i} }_{= 1}
        +
        \cancel{ \sum_{i \neq j} c_i c_j \lambda_j \underbrace{ \dotp{\v_i}{\v_j} }_{= 0} }
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i
        \\ &
        \text{Similarly, }
        \quad
        \qf{\x}{\Qinv}
        =
        \sum_{i = 1}^{d} c_i^2 \frac{1}{\lambda_i},
        \qquad
        \dotp{\x}{\x}
        =
        \qf{\x}{\I}
        =
        \sum_{i = 1}^{d} c_i^2 (1)
        =
        \sum_{i = 1}^{d} c_i^2
    \end{align*}
    Now, let \( p_i = c_i^2 \) and \( x_i = \lambda_i \) in Kantorovich inequality, we have
    \begin{equation*}
        \underbrace{ \left( \sum_{i = 1}^{d} c_i^2 \lambda_i \right) }_{\qf{\x}{\Q}}
        \underbrace{ \left( \sum_{i = 1}^{d} \frac{c_i^2}{\lambda_i} \right) }_{\qf{\x}{\Qinv}}
        \leq
        \frac{{(\lambda_{\max} + \lambda_{\min})}^2}{4 \lambda_{\max} \lambda_{\min}} \underbrace{ {\left( \sum_{i = 1}^{d} c_i^2 \right)}^2 }_{{(\dotp{\x}{\x})}^2}
        \implies
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
\end{proof}

\section{Polyak-\L{}ojasiewicz (PL) inequality}

\begin{definition}{Polyak-\L{}ojasiewicz (PL) inequality}{}
    A function \( f: D \subseteq \R^n \to \R \) on a convex domain \( D \) satisfies the \textbf{Polyak-\L{}ojasiewicz (PL) inequality} with parameter \( \mu > 0 \) if
    \begin{equation*}
        \half \norm[\Big]{\grad{f}{\x}}^2 \geq \mu \left( \func{f}{\x} - f^\ast \right)
        , \quad \forall \x \in D
    \end{equation*}
    where \( f^\ast = \inf_{\x \in D} \func{f}{\x} \).
    Such an \( f \) is said to be \( \mu \)-\textbf{PL} on \( D \).
\end{definition}

\section{Jensen's inequality}

\begin{theorem}{Jensen's inequality}{}
    For any convex function \( f: \R^d \to \R \), and for any set of points \( \set{\x_i}_{i = 1}^n, \; \x_i \in \R^d \) and weights \( \set{\alpha_i}_{i = 1}^n, \; \alpha_i \geq 0 \) such that \( \sum_{i = 1}^{n} \alpha_i = 1 \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\sum_{i = 1}^{n} \alpha_i \x_i}
        \leq
        \sum_{i = 1}^{n} \alpha_i \func{f}{\x_i}
    \end{equation*}
    with equality iff \( \x_1 = \x_2 = \cdots = \x_n \) or \( f \) is linear on the convex hull of \( \set{\x_i}_{i = 1}^n \).
\end{theorem}

\subsection{Convex hull}

The \textbf{convex hull} of a set of points \( \set{\x_i}_{i = 1}^n, \; \x_i \in \R^d \) is the smallest convex set that contains all the points.
It can be defined as the set of all convex combinations of the points, i.e.,
\begin{equation*}
    \func{\operatorname{conv}}{\set{\x_i}_{i = 1}^n}
    =
    \set{
        \sum_{i = 1}^{n} \alpha_i \x_i
        \given
        \alpha_i \geq 0,
        \; \sum_{i = 1}^{n} \alpha_i = 1
    }
\end{equation*}
