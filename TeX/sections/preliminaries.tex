
\chapter{Real analysis}

\section{Metric space}

A \textbf{metric space} \( (S, d) \) is a set \( S \) together with a \textit{metric/distance function} \( d: S \times S \to \R \) that satisfies the following axioms for all \( x, y, z \in S \):
\begin{itemize}
    \item \( \func{d}{x, y} = 0 \iff x = y \)
    \item \textit{Symmetry}: \( \func{d}{x, y} = \func{d}{y, x} \)
    \item \textit{Triangle inequality}: \( d(x, z) \leq \func{d}{x, y} + \func{d}{y, z} \)
\end{itemize}

\section{Norm}

A \textbf{normed vector space} \( (\mathbf{V}, \norm{\,\cdot\,}) \) is a vector space \( \mathbf{V} \) over a field \( \mathbb{F} \) (usually \( \R \) or \( \mathbb{C} \)) equipped with a function \( \norm{\,\cdot\,}: \mathbf{V} \to \R \) that satisfies the following properties for all \( \mathbf{u}, \mathbf{v} \in \mathbf{V} \) and \( a \in \mathbb{F} \):
\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\mathbf{v}} \geq 0 \)
    \item \textit{Definiteness}: \( \norm{\mathbf{v}} = 0 \iff \mathbf{v} = \zero \)
    \item \textit{Homogeneity}: \( \norm{a \mathbf{v}} = \abs{a} \, \norm{\mathbf{v}} \)
    \item \textit{Triangle inequality}: \( \norm{\mathbf{u} + \mathbf{v}} \leq \norm{\mathbf{u}} + \norm{\mathbf{v}} \)
\end{itemize}

Norm \( \eta: \R^d \to \R \), satisfies
\begin{itemize}
    \item \( \func{\eta}{\x} \geq 0 \)
    \item \( \func{\eta}{\x} = 0 \iff \x = \zero \)
    \item \( \func{\eta}{s \x} = \abs{s} \, \func{\eta}{\x} \), \quad \( s \in \R \)
    \item \( \func{\eta}{\x + \y} \leq \func{\eta}{\x} + \func{\eta}{\y} \)
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}
\begin{equation*}
    \norm{\x}_p
    =
    {\left( \sum_{i = 1}^{d} \abs{x_i}^p \right)}^{\frac{1}{p}},
    \quad p \geq 1
\end{equation*}

\paragraph{\( L_1 \) norm (Manhattan norm)}
\begin{equation*}
    \norm{\x}_1
    =
    \abs{x_1} + \abs{x_2} + \cdots + \abs{x_d}
    =
    \sum_{i = 1}^{d} \abs{x_i}
\end{equation*}

\paragraph{\( L_2 \) norm (Euclidean norm)}
\begin{equation*}
    \norm{\x}_2
    =
    \sqrt{x_1^2 + x_2^2 + \cdots + x_d^2}
    =
    \sqrt{\dotp{\x}{\x}}
    \triangleq
    \norm{\x}
\end{equation*}

\paragraph{\( L_{\infty} \) norm (Maximum norm)}
\begin{equation*}
    \norm{\x}_{\infty}
    =
    \max \{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_d} \}
    =
    \max_{1 \leq i \leq d} \abs{x_i}
\end{equation*}

\section{Neighborhood}\label{sec:neighborhood}

A \textbf{neighborhood} of a point \( \x \in \R^d \) is an open ball centered at \( \x \) with radius \( r > 0 \), defined as
\begin{equation*}
    B_{r}(\x) = \left\{ \mathbf{z} \in \R^d \;\big|\; \normbf{z - x} < r \right\}, \quad r > 0, \ \x \in \R^d
\end{equation*}

\section{Interior point}

A point \( \x \in S \) is an \textbf{interior point} of set \( S \subseteq \R^d \) if there exists a neighborhood of \( \x \) that is entirely contained within \( S \), i.e., there exists an \( r > 0 \) such that \( B_{r}(\x) \subseteq S \).

\section{Limit point}

A point \( \x \in \R^d \) is a \textbf{limit point} of set \( S \subseteq \R^d \) if every neighborhood of \( \x \) contains at least one point of \( S \) different from \( \x \) itself, i.e., for every \( r > 0 \), there exists a point \( \y \in S \setminus \{ \x \} \) such that \( \y \in B_{r}(\x) \).
\begin{equation*}
    \forall r > 0, \ B_{r}(\x) \cap (S \setminus \{ \x \}) \neq \emptyset
\end{equation*}

Trivially, every interior point of \( S \) is also a limit point of \( S \).

\section{Open set}

A set \( S \subseteq \R^d \) is \textbf{open} if every point in \( S \) is an interior point of \( S \).

\section{Closed set}

A set \( S \subseteq \R^d \) is \textbf{closed} if it contains all its limit points, i.e., if \( \x \) is a limit point of \( S \), then \( \x \in S \).

\section{Bounded set}

A set \( S \subseteq \R^d \) is \textbf{bounded} if there exists \( \mathbf{z} \in \R^d, M \in \R \) such that \( \normbf{z - x} < M, \ \forall \x \in S \).

\section{Infimum}

The \textbf{infimum} (greatest lower bound) of a set \( S \subseteq \R \) is the largest real number \( m \) such that \( m \leq x, \ \forall x \in S \).

\section{Supremum}

The \textbf{supremum} (least upper bound) of a set \( S \subseteq \R \) is the smallest real number \( M \) such that \( M \geq x, \ \forall x \in S \).

\section{Closure}

The \textbf{closure} of a set \( S \subseteq \R^d \) is the smallest closed set containing \( S \), which can be obtained by adding all limit points of \( S \) to \( S \) itself.
\begin{equation*}
    \operatorname{closure}{(S)} = S \cup \{ \text{all limit points of } S \}
\end{equation*}

\section{Convergent sequence}

A sequence of real numbers \( {\{ a_n \}}_{n=1}^{\infty} \) is said to \textbf{converge} to a limit \( L \in \R \) if
\begin{equation*}
    a_n \to L
    \quad \iff \quad
    \lim_{n \to \infty} a_n = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall n > N \implies \abs{a_n - L} < \epsilon
\end{equation*}

\section{Cauchy sequence}

A sequence of real numbers \( {\{ a_k \}}_{k=1}^{\infty} \) is a \textbf{Cauchy sequence} if
\begin{equation*}
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall m, n > N \implies \abs{a_n - a_m} < \epsilon
\end{equation*}

In \( \R \), every Cauchy sequence is convergent, and every convergent sequence is Cauchy, since \( \R \) is complete.

\chapter{Calculus}

\section{Limits}

(\( \epsilon \)--\( \delta \)) definition of \textbf{limit} of a function \( f: D \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( D \):
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    0 < \norm{\x - \x_0} < \delta, \,
    \x \in D
    \implies
    \abs{\func{f}{\x} - L} < \epsilon
\end{equation*}
i.e., \( \func{f}{\x} \) can be made arbitrarily close to \( L \) by making \( \x \) sufficiently close to \( \x_0 \).

Alternate notation~\psecref{sec:neighborhood}:
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \x \in D \cap B_{\delta}(\x_0) \setminus \{ \x_0 \}
    \implies
    \func{f}{\x} \in B_{\epsilon}(L)
\end{equation*}

Sequence definition of \textbf{limit} of a function \( f: D \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( D \):
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = L
    \; \iff \;
    \lim_{n \to \infty} \abs{\func{f}{\x_n} - L} = 0
    \; \iff \;
    \forall {\{ \x_n \}}_{n=1}^{\infty} \subseteq D \setminus \{ \x_0 \},
    \ \x_n \to \x_0
    \implies
    \func{f}{\x_n} \to L
\end{equation*}

\section{Continuity}

For a function \( f: D \subseteq \R^d \to \R \),
\begin{equation*}
    \lim_{\x \to \mathbf{c}} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \abs{\func{f}{\x} - L} < \epsilon, \
    \forall \x \in D \cap B_{\delta}(\mathbf{c}) \setminus \{ \mathbf{c} \}
\end{equation*}

\section{Continuous functions}

A function \( f: D \subseteq \R^d \to \R \) is \textbf{continuous} at a point \( \x_0 \in D \) iff
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = \func{f}{\x_0}
\end{equation*}

\section{Derivative}

The \textbf{derivative} of a function \( f: \R \to \R \) at a point \( x \in \R \) is defined as
\begin{equation*}
    f'(x) = \lim_{h \to 0} \frac{\func{f}{x + h} - \func{f}{x}}{h}
\end{equation*}

The \textbf{gradient} of a function \( f: \R^d \to \R \) at a point \( \x = {\left(x_1, x_2, \ldots, x_d\right)}^\top \in \R^d \) is defined as
\begin{equation*}
    \grad{f}{\x} = {\left( \frac{\partial f}{\partial x_1}(\x), \frac{\partial f}{\partial x_2}(\x), \ldots, \frac{\partial f}{\partial x_d}(\x) \right)}^\top
    \in \R^d
\end{equation*}

The \textbf{Hessian} of a function \( f: \R^d \to \R \) at a point \( \x = {\left(x_1, x_2, \ldots, x_d\right)}^\top \in \R^d \) is defined as
\begin{equation*}
    \hess{f}{\x} =
    \begin{bmatrix}
        \cfrac{\partial^2 f}{\partial x_1^2}(\x) & \cfrac{\partial^2 f}{\partial x_1 \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_1 \partial x_d}(\x) \\
        \cfrac{\partial^2 f}{\partial x_2 \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_2^2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_2 \partial x_d}(\x) \\
        \vdots & \vdots & \ddots & \vdots \\
        \cfrac{\partial^2 f}{\partial x_d \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_d \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_d^2}(\x)
    \end{bmatrix}
    \in \R^{d \times d}
\end{equation*}

\section{Intermediate value theorem}

For a function \( f: [a, b] \to \R \) that is continuous on the closed interval \( [a, b] \), then
\begin{equation*}
    \forall N \in (\func{f}{a}, \func{f}{b}), \ \exists \; c \in (a, b) \text{ such that } \func{f}{c} = N
\end{equation*}

\section{Wierstrass extreme value theorem}

If a function \( f: S \to \R \) is continuous on a set \( S \subseteq \R^d \) that is closed and bounded, then \( f \) attains its maximum and minimum values on \( S \), i.e., there exist points \( \x_{\min}, \x_{\max} \in S \) such that
\begin{equation*}
    \func{f}{\x_{\min}} \leq \func{f}{\x} \leq \func{f}{\x_{\max}}, \quad \forall \x \in S
\end{equation*}

\section{Taylor's theorem}

\subsection{Univariate Taylor's theorem}

Let \( f: \R \to \R \) be \( (n+1) \) times continuously differentiable on an interval containing \( a \) and \( x \).
Then, for some \( \xi \) between \( a \) and \( x \),
\begin{align*}
    \func{f}{x}
    & =
    \func{f}{a} + f'(a)(x - a) + \frac{f''(a)}{2!}{(x - a)}^2 + \cdots + \frac{f^{(n)}(a)}{n!}{(x - a)}^n + R_{n+1}(x)
    \\ & =
    \sum_{k=0}^{n} \frac{f^{(k)}(a)}{k!} {(x - a)}^k + R_{n+1}(x)
\end{align*}
where the remainder term \( R_{n+1}(x) \) is given by the Lagrange form:
\begin{equation*}
    R_{n+1}(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} {(x - a)}^{n+1}
\end{equation*}

\subsection{Multivariate Taylor's theorem}

\section{Levels of smoothness}

A function \( f: \R^d \to \R \) is said to be of \textbf{class} \( \C^k \) if all partial derivatives of \( f \) up to and including order \( k \in \N \) exist and are continuous.

If a function is of class \( \C^k \ \forall k \in \N \), it is said to be of class \( \C^{\infty} \) or \textbf{smooth}.

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \C^0 \), then \( f \) is continuous.

    \item If \( f \in \C^k \), then \( f \in \C^{k-1} \), for \( k \geq 1 \).

    \item (Schwarz's theorem) If \( f \in \C^2 \), then \( \hess{f}{\x} \in \mathbf{S}_d\), i.e., \( \hess{f}{\x} \) is symmetric.
        \begin{equation*}
            \frac{\partial^2 f}{\partial x_i \partial x_j}
            =
            \frac{\partial^2 f}{\partial x_j \partial x_i},
            \quad \forall i, j = 1, 2, \ldots, d
        \end{equation*}

    \item If \( f, g \in \C^k \), then \( f + g, f \cdot g \in \C^k \).

    \item If \( f \in \C^k \), then \( \frac{\partial f}{\partial x_i} \in \C^{k-1}, \ \forall i = 1, 2, \ldots, d \).
\end{itemize}

\section{Lipschitz}

A function \( f: \R^d \to \R \) is said to be \( L \)-\textbf{smooth} if it is differentiable and there exists a constant \( L > 0 \) such that
\begin{equation*}
    \norm{\grad{f}{\x} - \nabla \func{f}{\y}}_2
    \leq
    L \; \normbf{x - y}_2,
    \quad \forall \x, \y \in \R^d
\end{equation*}

The constant \( L \) is called the \textbf{Lipschitz constant} of the gradient \( \nabla f \).

Here, we denote \( f \in \C^{1}_L \) to mean that \( f \) is \( L \)-smooth and of class \( \C^1 \).

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \C^{2} \), then \( f \in \C^{1}_L \) iff \( \hess{f}{\x} \preceq L \I, \ \forall \x \in \R^d \).

    \item If \( f \in \C^{1}_L \), then
        \begin{equation*}
            \abs{ \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x) }
            \leq
            \frac{L}{2} \normbf{y - x}^2,
            \quad \forall \x, \y \in \R^d
        \end{equation*}

        \begin{proof}
            Follows from \( \func{f}{\x} \pm \frac{L}{2} \norm{\x}^2 \) being convex functions, given \( f \in \C^{1}_L \).
        \end{proof}
\end{itemize}

\chapter{Linear Algebra}

\section{Outer product}

The \textbf{outer product} of two vectors \( \x \in \R^m \) and \( \y \in \R^n \) is defined as
\begin{equation*}
    \x \y^\top =
    \begin{bmatrix}
        x_1 y_1 & x_1 y_2 & \cdots & x_1 y_n \\
        x_2 y_1 & x_2 y_2 & \cdots & x_2 y_n \\
        \vdots & \vdots & \ddots & \vdots \\
        x_m y_1 & x_m y_2 & \cdots & x_m y_n
    \end{bmatrix}
    \in \R^{m \times n}
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \operatorname{rank} (\x \y^\top) = 1, \; \forall \x \in \R^m \setminus \{ \zero \}, \; \forall \y \in \R^n \setminus \{ \zero \} \).

    \item \( \x \x^\top \in \mathbf{S}_d^+, \; \forall \x \in \R^d \).

    \item \( \operatorname{spec}(\x \x^\top) = \{ \norm{\x}_2^2, 0^{(d-1)} \} \).
        Eigenvectors: \( \{ \x \} \cup \underbrace{ \{ \mathbf{v} \mid \dotpbf{v}{x} = 0 \} }_{\x^\perp} \).

        Eigenvalues of \( \x \x^\top \) are \( \norm{\x}_2^2 \) and \( 0 \) (with multiplicity \( d-1 \)).
        The eigenvector corresponding to \( \norm{\x}_2^2 \) is \( \x \); the eigenspace corresponding to \( 0 \) is the orthogonal complement \( \x^\perp \).
\end{itemize}

\section{Matrices}

\subsection{Symmetric Matrices}

A matrix \( \mathbf{A} \in \R^{d \times d} \) is symmetric if \( \mathbf{A} = \mathbf{A}^\top \), i.e., \( a_{ij} = a_{ji}, \ \forall i, j \).

\subsection{Symmetric Positive Semi-definite Matrices}

A symmetric matrix \( \mathbf{A} \in \SD \) is \textbf{positive semi-definite} (PSD), denoted as \( \mathbf{A} \succeq 0 \), if
\begin{equation*}
    \x^\top \mathbf{A} \x \geq 0,
    \quad \forall \x \in \R^d
\end{equation*}

\subsection{Symmetric Positive Definite Matrices}

A symmetric matrix \( \mathbf{A} \in \SD \) is \textbf{positive definite} (PD), denoted as \( \mathbf{A} \succ 0 \), if
\begin{equation*}
    \x^\top \mathbf{A} \x > 0,
    \quad \forall \x \in \R^d \setminus \{ \zero \}
\end{equation*}

\section{Eigenvalues and Eigenvectors}

\begin{equation*}
    \mathbf{A}\mathbf{v} = \lambda \mathbf{v}
\end{equation*}
where
\( \mathbf{A} \in \R^{d \times d} \), \quad
\( \lambda \in \R \longrightarrow \) Eigenvalue, \quad
\( \mathbf{v} \in \R^d \setminus \{ \zero \} \longrightarrow \) corresponding Eigenvector.

The set of all eigenvalues of \( \mathbf{A} \) is called the \textbf{spectrum} of \( \mathbf{A} \).

\subsection{Properties}

\begin{itemize}
    \item \( \operatorname{trace}(\mathbf{A}) = \sum_{i = 1}^{d} a_{ii} = \sum_{i = 1}^{d} \lambda_i \).

    \item \( \det(\mathbf{A}) = \prod_{i = 1}^{d} \lambda_i \).

    \item \( \mathbf{A} \in \SD \implies \mathbf{A} \) has an orthonormal basis of eigenvectors in \( \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \mathbf{A} \in \SD \implies \operatorname{spec}(\mathbf{A}) \subseteq \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \mathbf{A} \in \SD \implies \mathbf{v}_i^\top \mathbf{v}_j = 0, \; \text{if } \lambda_i \neq \lambda_j \) \, \psecref{sec:spectral-decomposition-theorem}
\end{itemize}

\section{Spectral Decomposition Theorem}\label{sec:spectral-decomposition-theorem}

\begin{theorem}{Spectral Decomposition Theorem}{}
    For any \( \mathbf{A} \in \SD \), there exists an orthonormal basis of eigenvectors \( \{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d \} \), \; \( \mathbf{v}_i \in \R^d \setminus \{ \zero \} \), with corresponding eigenvalues \( \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d, \; \lambda_i \in \R \), such that
    \begin{equation*}
        \mathbf{A} = \sum_{i = 1}^{d} \lambda_i \mathbf{v}_i \mathbf{v}_i^\top
        = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top
    \end{equation*}
    where \( \mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d] \in \R^{d \times d} \) is an orthogonal matrix (i.e., \( \dotpbf{V}{V} = \I \)) and \( \boldsymbol{\Lambda} = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d) \in \R^{d \times d} \) is a diagonal matrix containing the eigenvalues of \( \mathbf{A} \).
\end{theorem}

\begin{proof}
    The eigenvalues of a real symmetric matrix are all real
    (Can be shown considering Hermitian matrices, Fundamental theorem of algebra, and properties of complex conjugates).

    The eigenvectors corresponding to distinct eigenvalues are orthogonal.
    This can be shown using the fact that for any two eigenvectors \( \mathbf{v}_i \) and \( \mathbf{v}_j \) corresponding to distinct eigenvalues \( \lambda_i \) and \( \lambda_j \), we have
    \begin{align*}
        \mathbf{v}_i^\top \mathbf{A} \mathbf{v}_j
        & =
        \mathbf{v}_i^\top (\mathbf{A} \mathbf{v}_j)
        =
        \lambda_j \mathbf{v}_i^\top \mathbf{v}_j,
        \qquad \text{Similarly, } \;
        \mathbf{v}_j^\top \mathbf{A} \mathbf{v}_i
        =
        \lambda_i \mathbf{v}_j^\top \mathbf{v}_i
        \\
        \text{Now, } \;
        \mathbf{v}_i^\top \mathbf{A} \mathbf{v}_j
        & =
        \mathbf{v}_i^\top \mathbf{A}^\top \mathbf{v}_j
        =
        {(\mathbf{A} \mathbf{v}_i)}^\top \mathbf{v}_j
        =
        {(\lambda_i \mathbf{v}_i)}^\top \mathbf{v}_j
        =
        \lambda_i \mathbf{v}_i^\top \mathbf{v}_j
        =
        \lambda_i \mathbf{v}_j^\top \mathbf{v}_i
        =
        \mathbf{v}_j^\top \mathbf{A} \mathbf{v}_i
        \\
        \implies
        \lambda_j \mathbf{v}_i^\top \mathbf{v}_j
        & =
        \lambda_i \mathbf{v}_i^\top \mathbf{v}_j
        \implies
        (\lambda_j - \lambda_i) \mathbf{v}_i^\top \mathbf{v}_j
        = 0
        \implies
        \mathbf{v}_i^\top \mathbf{v}_j = 0,
        \; \because
        \lambda_i \neq \lambda_j,
        \;\;
        \therefore
        \mathbf{v}_i, \mathbf{v}_j \text{ are orthogonal.}
    \end{align*}

    For repeated eigenvalues, if any, we can always find a set of orthonormal eigenvectors using the Gram-Schmidt process.
    Thereby, we can always find \( d \) orthonormal eigenvectors of \( \mathbf{A} \), thus forming an orthonormal basis for \( \R^d \).
    This gives that \( \mathbf{V} \) is an orthogonal matrix \( \implies \dotpbf{V}{V} = \I \implies \mathbf{V}^{-1} = \mathbf{V}^\top \).

    Now, with \( \boldsymbol{\Lambda} = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d) \) and \( \mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d] \), we have
    \begin{equation*}
        \mathbf{A} \mathbf{V}
        =
        [\mathbf{A} \mathbf{v}_1, \mathbf{A} \mathbf{v}_2, \ldots, \mathbf{A} \mathbf{v}_d]
        =
        [\lambda_1 \mathbf{v}_1, \lambda_2 \mathbf{v}_2, \ldots, \lambda_d \mathbf{v}_d]
        =
        \mathbf{V} \boldsymbol{\Lambda}
        \implies
        \mathbf{A}
        =
        \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^{-1}
        =
        \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top
    \end{equation*}
\end{proof}

\begin{itemize}
    \item The identity matrix \( \I \in \R^{d \times d} \) has eigenvalues \( \lambda_i = 1, \; \forall i \) and the corresponding eigenvectors can be chosen as any orthonormal basis of \( \R^d \), commonly taken as the standard basis vectors \( \mathbf{e}_i \in \R^d \).

    \item \( \mathbf{A} \in \PSD \iff \mathbf{A} \in \SD, \text{ and } \lambda_i \geq 0, \; \forall i \).

    \item \( \mathbf{A} \in \PD \iff \mathbf{A} \in \SD, \text{ and } \lambda_i > 0, \; \forall i \).

    \item \( \mathbf{A} \in \PD, \; \kappa(\mathbf{A}) = 1 \iff \mathbf{A} = c \I, \; c > 0 \).
\end{itemize}

\section{Condition number}

The \textbf{condition number} of a matrix \( \mathbf{A} \in \SD \) is given by
\begin{equation*}
    \kappa(\mathbf{A}) = \frac{\lambda_{\max}(\mathbf{A})}{\lambda_{\min}(\mathbf{A})}
\end{equation*}
where \( \lambda_{\max}(\mathbf{A}) \) and \( \lambda_{\min}(\mathbf{A}) \) are the largest and smallest eigenvalues of \( \mathbf{A} \) respectively.

\chapter{Inequalities}

\section{Cauchy-Schwarz inequality}\label{sec:cauchy-schwarz-inequality}

\begin{equation*}
    \abs{\dotp{\x}{\y}} \leq \norm{\x}_2 \; \norm{\y}_2, \quad \x, \y \in \R^d
\end{equation*}

\section{Kantorovich inequality}\label{sec:kantorovich-inequality}

\begin{theorem}{Kantorovich inequality}{}
    Let \( p_i \geq 0, \ 0 < x_{\min} \leq x_i \leq x_{\max}, \ \forall i \in \{1, 2, \ldots, n\} \), then we have
    \begin{equation*}
        \left( \sum_{i = 1}^{n} p_i x_i \right) \left( \sum_{i = 1}^{n} \frac{p_i}{x_i} \right)
        \leq
        \frac{{(x_{\max} + x_{\min})}^2}{4 x_{\max} x_{\min}}
        {\left( \sum_{i = 1}^{n} p_i \right)}^2
    \end{equation*}
\end{theorem}

\begin{corollary}{Kantorovich inequality for positive definite matrices~\citep{Luenberger1984}}{}
    For any \( \Q \in \PD \), with smallest eigenvalue \( \lambda_{\min} \) and largest eigenvalue \( \lambda_{\max} \), and for any \( \x \in \R^d \setminus \{ \zero \} \),
    \begin{equation*}
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
\end{corollary}

\begin{proof}
    Let the matrix \( \Q \) have eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \), with corresponding orthonormal eigenvectors \( \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d \).
    Since the eigenvectors form an orthonormal basis for \( \R^d \), any vector \( \x \in \R^d \) can be expressed as a linear combination of these eigenvectors as \( \x = \sum_{i = 1}^{d} c_i \mathbf{v}_i \), where each coefficient \( c_i \) is the projection of \( \x \) onto the eigenvector \( \mathbf{v}_i \), given by \( c_i = \mathbf{v}_i^\top \x \).
    Now,
    \begin{align*}
        \qf{\x}{\Q}
        & =
        {\left( \sum_{i = 1}^{d} c_i \mathbf{v}_i \right)}^\top \Q \left( \sum_{j = 1}^{d} c_j \mathbf{v}_j \right)
        =
        \sum_{i = 1}^{d} \sum_{j = 1}^{d} c_i c_j \mathbf{v}_i^\top \Q \mathbf{v}_j
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i
        \\ &
        \text{Similarly, }
        \quad
        \qf{\x}{\Qinv}
        =
        \sum_{i = 1}^{d} c_i^2 \frac{1}{\lambda_i},
        \qquad
        \dotp{\x}{\x}
        =
        \sum_{i = 1}^{d} c_i^2
    \end{align*}
    Now, let \( p_i = c_i^2 \) and \( x_i = \lambda_i \) in Kantorovich inequality, we have
    \begin{equation*}
        \left( \sum_{i = 1}^{d} c_i^2 \lambda_i \right)
        \left( \sum_{i = 1}^{d} \frac{c_i^2}{\lambda_i} \right)
        \leq
        \frac{{(\lambda_{\max} + \lambda_{\min})}^2}{4 \lambda_{\max} \lambda_{\min}} {\left( \sum_{i = 1}^{d} c_i^2 \right)}^2
        \implies
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
\end{proof}

\section{AM-GM inequality}

For any non-negative real numbers \( a_1, a_2, \ldots, a_n \geq 0 \),
\begin{equation*}
    \frac{a_1 + a_2 + \cdots + a_n}{n}
    \geq
    \sqrt[n]{a_1 a_2 \ldots a_n}
\end{equation*}
with equality iff \( a_1 = a_2 = \cdots = a_n \).

\section{Polyak-\L{}ojasiewicz (PL) inequality}

A function \( f: D \subseteq \R^n \to \R \) on a convex domain \( D \) satisfies the \textbf{Polyak-\L{}ojasiewicz (PL) inequality} with parameter \( \mu > 0 \) if
\begin{equation*}
    \half \norm[\Big]{\grad{f}{\x}}^2 \geq \mu \left( \func{f}{\x} - f^* \right)
    , \quad \forall \x \in D
\end{equation*}
where \( f^* = \inf_{\x \in D} \func{f}{\x} \).
Such an \( f \) is said to be \( \mu \)-\textbf{PL} on \( D \).
