
\chapter{Real analysis}

\section{Metric space}

A \textbf{metric space} \( (S, d) \) is a set \( S \) together with a \textit{metric/distance function} \( d: S \times S \to \R_{\geq 0} \) that satisfies the following axioms for all \( x, y, z \in S \):
\begin{itemize}
    \item \( \func{d}{x, y} = 0 \iff x = y \)
    \item \textit{Symmetry}: \( \func{d}{x, y} = \func{d}{y, x} \)
    \item \textit{Triangle inequality}: \( d(x, z) \leq \func{d}{x, y} + \func{d}{y, z} \)
\end{itemize}

\section{Norm}

A \textbf{norm} is a real-valued function \( \eta: V \to \R_{\geq 0} \) that satisfies
\begin{itemize}
    \item \( \func{\eta}{x} \geq 0, \quad \forall x \in V \)
    \item \( \func{\eta}{x} = 0 \iff x = 0 \)
    \item \( \func{\eta}{s x} = \abs{s} \, \func{\eta}{x}, \quad \forall x \in V, s \in F \)
    \item \( \func{\eta}{x + y} \leq \func{\eta}{x} + \func{\eta}{y}, \quad \forall x, y \in V \)
\end{itemize}

\section{Vector norm}

A \textbf{vector norm} is a norm defined on a vector space.

A \textbf{normed vector space} \( (\mathbf{V}, \norm{\,\cdot\,}) \) is a vector space \( \mathbf{V} \) over a field \( \mathbb{F} \) (usually \( \R \) or \( \mathbb{C} \)) equipped with a function \( \norm{\,\cdot\,}: \mathbf{V} \to \R_{\geq 0} \) that satisfies the following properties for all \( \mathbf{u}, \mathbf{v} \in \mathbf{V} \) and \( a \in \mathbb{F} \):
\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\mathbf{v}} \geq 0 \)
    \item \textit{Definiteness}: \( \norm{\mathbf{v}} = 0 \iff \mathbf{v} = \zero \)
    \item \textit{Homogeneity}: \( \norm{a \mathbf{v}} = \abs{a} \, \norm{\mathbf{v}} \)
    \item \textit{Triangle inequality}: \( \norm{\mathbf{u} + \mathbf{v}} \leq \norm{\mathbf{u}} + \norm{\mathbf{v}} \)
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}

\begin{equation*}
    \norm{\x}_p
    =
    {\left( \sum_{i = 1}^{d} \abs{x_i}^p \right)}^{\frac{1}{p}},
    \quad p \geq 1
\end{equation*}

\subsubsection{\( L_1 \) norm (Manhattan norm)}

\begin{equation*}
    \norm{\x}_1
    =
    \abs{x_1} + \abs{x_2} + \cdots + \abs{x_d}
    =
    \sum_{i = 1}^{d} \abs{x_i}
\end{equation*}

\subsubsection{\( L_2 \) norm (Euclidean norm)}

\begin{equation*}
    \norm{\x}_2
    =
    \sqrt{x_1^2 + x_2^2 + \cdots + x_d^2}
    =
    \sqrt{\dotp{\x}{\x}}
    \triangleq
    \norm{\x}
\end{equation*}

\subsubsection{\( L_{\infty} \) norm (Maximum norm)}

\begin{equation*}
    \norm{\x}_{\infty}
    =
    \max \{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_d} \}
    =
    \max_{1 \leq i \leq d} \abs{x_i}
\end{equation*}

\subsection{Energy norm}

For \( \Q \in \PD \), the \textbf{energy norm} (or \( \Q \)-norm) is defined as
\begin{equation*}
    \norm{\x}_{\Q}
    =
    \sqrt{\qf{\x}{\Q}}
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item \( \norm{\x}_{\I} = \norm{\x}_2 \)

    \item \( \norm{\x}_{\Q} = \norm{\Q^{\frac{1}{2}} \x}_2 \)

    \item \( \norm{\x}_{\Q} = \norm{\x}_{\Q^{\frac{1}{2}}} \)

    \item \( \lambda_{\min}(\Q) \; \norm{\x}^2 \leq \norm{\x}_{\Q}^2 \leq \lambda_{\max}(\Q) \; \norm{\x}^2 \)

    \item \( \norm{\x}_{\Q} = \norm{\Q \x}_{\Q^{-1}} \)

    \item \( \norm{\x}_{\Q} = \norm{\Q^{-1} \x}_{\Q} \)
\end{itemize}

\section{Matrix norm}

A \textbf{matrix norm} is a norm defined on a space of matrices.

\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\mathbf{A}} \geq 0 \)

    \item \textit{Definiteness}: \( \norm{\mathbf{A}} = 0 \iff \mathbf{A} = \zero \)

    \item \textit{Homogeneity}: \( \norm{a \mathbf{A}} = \abs{a} \, \norm{\mathbf{A}} \)

    \item \textit{Triangle inequality}: \( \norm{\mathbf{A} + \mathbf{B}} \leq \norm{\mathbf{A}} + \norm{\mathbf{B}} \)
\end{itemize}

\begin{itemize}
    \item \textit{Sub-multiplicativity}: \( \norm{\mathbf{A} \mathbf{B}} \leq \norm{\mathbf{A}} \, \norm{\mathbf{B}} \)

    \item \textit{Consistency}: \( \norm{\mathbf{A} \x} \leq \norm{\mathbf{A}} \, \norm{\x}, \quad \forall \x \in \R^d \)

    \item \textit{Transpose invariance}: \( \norm{\mathbf{A}} = \norm{\mathbf{A}^\top} \)

    \item \textit{Continuity}: \( \norm{\mathbf{A}} \) is continuous with respect to the entries of \( \mathbf{A} \)
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}

\begin{equation*}
    \norm{\mathbf{A}}_p
    =
    \sup_{\x \neq \zero} \frac{\norm{\mathbf{A} \x}_p}{\norm{\x}_p}
    =
    \max_{\norm{\x}_p = 1} \norm{\mathbf{A} \x}_p,
    \quad p \geq 1
\end{equation*}

\subsubsection{\( L_1 \) norm (Maximum absolute column sum norm)}

\begin{equation*}
    \norm{\mathbf{A}}_1
    =
    \max_{1 \leq j \leq d} \sum_{i = 1}^{d} \abs{a_{ij}}
    =
    \max_{1 \leq j \leq d} \norm{\mathbf{A}_{\ast j}}_1
\end{equation*}

\subsubsection{\( L_2 \) norm (Spectral norm)}

\begin{equation*}
    \norm{\mathbf{A}}_2
    =
    \sqrt{\lambda_{\max}(\mathbf{A}^\top \mathbf{A})}
    =
    \sigma_{\max}(\mathbf{A})
\end{equation*}

For \( \mathbf{A} \in \SD, \norm{\mathbf{A}}_2 = \rho(\mathbf{A}) = \abs{\lambda_{\max}(\mathbf{A})} \).

\subsubsection{\( L_{\infty} \) norm (Maximum absolute row sum norm)}

\begin{equation*}
    \norm{\mathbf{A}}_{\infty}
    =
    \max_{1 \leq i \leq d} \sum_{j = 1}^{d} \abs{a_{ij}}
    =
    \max_{1 \leq i \leq d} \norm{\mathbf{A}_{i \ast}}_1
\end{equation*}

\subsection{Frobenius norm (Euclidean norm)}

\begin{equation*}
    \norm{\mathbf{A}}_F
    =
    \sqrt{\sum_{i = 1}^{d} \sum_{j = 1}^{d} a_{ij}^2}
    =
    \sqrt{\operatorname{trace}(\mathbf{A}^\top \mathbf{A})}
    =
    \sqrt{\sum_{i = 1}^{d} \sigma_i^2(\mathbf{A})}
    =
    \norm{\operatorname{vec}(\mathbf{A})}_2
\end{equation*}

The vector \( \operatorname{vec}(\mathbf{A}) \) is the \textbf{vectorization} of matrix \( \mathbf{A} \), obtained by stacking its columns into a single column vector, i.e.,
\begin{equation*}
    \mathbf{A}
    =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1d} \\
        a_{21} & a_{22} & \cdots & a_{2d} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{d1} & a_{d2} & \cdots & a_{dd}
    \end{bmatrix}
    \in \R^{d \times d}
    \implies
    \operatorname{vec}(\mathbf{A})
    =
    \begin{bmatrix}
        a_{11} \\
        a_{21} \\
        \vdots \\
        a_{d1} \\
        a_{12} \\
        a_{22} \\
        \vdots \\
        a_{d2} \\
        \vdots \\
        a_{1d} \\
        a_{2d} \\
        \vdots \\
        a_{dd}
    \end{bmatrix}
    \in \R^{d^2}
\end{equation*}

\subsection{Spectral radius}

The \textbf{spectral radius} of a matrix \( \mathbf{A} \in \R^{d \times d} \) with eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \) is defined as
\begin{equation*}
    \rho(\mathbf{A})
    =
    \max \set{ \abs{\lambda_1}, \abs{\lambda_2}, \ldots, \abs{\lambda_d} }
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item \( \rho(\mathbf{A}) = \rho(\mathbf{A}^\top) \)

    \item \( \rho(\mathbf{A}^k) = {\left[ \rho(\mathbf{A}) \right]}^k, \quad k \in \N \)

    \item \( \rho(\inv{\mathbf{A}}) = \frac{1}{\rho(\mathbf{A})}, \quad \text{if } \mathbf{A} \text{ is invertible} \)

    \item \( \rho(\mathbf{A}) \leq \norm{\mathbf{A}}, \quad \forall \text{ matrix norms } \norm{\,\cdot\,} \)

    \item (Gelfand's formula) \( \rho(\mathbf{A}) = \lim_{k \to \infty} {\left( \norm{\mathbf{A}^k} \right)}^{\frac{1}{k}}, \quad \forall \text{ matrix norms } \norm{\,\cdot\,} \)

    \item \( \rho(\mathbf{A}) \leq \norm{\mathbf{A}^k}^{\frac{1}{k}}, \quad \forall k \in \N, \; \forall \text{ sub-multiplicative matrix norms } \norm{\,\cdot\,} \)

    \item \( \rho(\mathbf{A} \mathbf{B}) \leq \rho(\mathbf{A}) \, \rho(\mathbf{B}) \), if \( \mathbf{A} \) and \( \mathbf{B} \) commute, i.e., \( \mathbf{A} \mathbf{B} = \mathbf{B} \mathbf{A} \)

    \item \( \mathbf{A} \in \R^{d \times d} \implies \rho(\mathbf{A}) \leq \norm{\mathbf{A}}_2 \)

    \item \( \mathbf{A} \in \SD \implies \rho(\mathbf{A}) = \norm{\mathbf{A}}_2 \)

    \item \( \mathbf{A} \in \SD \implies \norm{\mathbf{A} \mathbf{x}}_2 \leq \rho(\mathbf{A}) \, \norm{\x}_2, \quad \forall \x \in \R^d \)
\end{itemize}

\section{Neighborhood}\label{sec:neighborhood}

A \textbf{neighborhood} of a point \( \x \in \R^d \) is an open ball centered at \( \x \) with radius \( r > 0 \), defined as
\begin{equation*}
    B_{r}(\x) = \left\{ \z \in \R^d \;\big|\; \normbf{z - x} < r \right\}, \quad r > 0, \ \x \in \R^d
\end{equation*}

\section{Interior point}

A point \( \x \in S \) is an \textbf{interior point} of set \( S \subseteq \R^d \) if there exists a neighborhood of \( \x \) that is entirely contained within \( S \), i.e., there exists an \( r > 0 \) such that \( B_{r}(\x) \subseteq S \).

\section{Limit point}

A point \( \x \in \R^d \) is a \textbf{limit point} of set \( S \subseteq \R^d \) if every neighborhood of \( \x \) contains at least one point of \( S \) different from \( \x \) itself, i.e., for every \( r > 0 \), there exists a point \( \y \in S \setminus \{ \x \} \) such that \( \y \in B_{r}(\x) \).
\begin{equation*}
    \forall r > 0, \ B_{r}(\x) \cap (S \setminus \{ \x \}) \neq \emptyset
\end{equation*}

Trivially, every interior point of \( S \) is also a limit point of \( S \).

\section{Open set}

A set \( S \subseteq \R^d \) is \textbf{open} if every point in \( S \) is an interior point of \( S \).

\section{Closed set}

A set \( S \subseteq \R^d \) is \textbf{closed} if it contains all its limit points, i.e., if \( \x \) is a limit point of \( S \), then \( \x \in S \).

\section{Bounded set}

A set \( S \subseteq \R^d \) is \textbf{bounded} if there exists \( \z \in \R^d, M \in \R \) such that \( \normbf{z - x} < M, \ \forall \x \in S \).

\section{Infimum}

The \textbf{infimum} (greatest lower bound) of a set \( S \subseteq \R \) is the largest real number \( m \) such that \( m \leq x, \ \forall x \in S \).

\section{Supremum}

The \textbf{supremum} (least upper bound) of a set \( S \subseteq \R \) is the smallest real number \( M \) such that \( M \geq x, \ \forall x \in S \).

\section{Closure}

The \textbf{closure} of a set \( S \subseteq \R^d \) is the smallest closed set containing \( S \), which can be obtained by adding all limit points of \( S \) to \( S \) itself.
\begin{equation*}
    \operatorname{closure}{(S)} = S \cup \{ \text{all limit points of } S \}
\end{equation*}

\section{Convergent sequence}

A sequence of real numbers \( {\{ a_n \}}_{n = 1}^{\infty} \) is said to \textbf{converge} to a limit \( L \in \R \) if
\begin{equation*}
    a_n \to L
    \quad \iff \quad
    \lim_{n \to \infty} a_n = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall n > N \implies \abs{a_n - L} < \epsilon
\end{equation*}

\section{Cauchy sequence}

A sequence of real numbers \( {\{ a_k \}}_{k = 1}^{\infty} \) is a \textbf{Cauchy sequence} if
\begin{equation*}
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall m, n > N \implies \abs{a_n - a_m} < \epsilon
\end{equation*}

In \( \R \), every Cauchy sequence is convergent, and every convergent sequence is Cauchy, since \( \R \) is complete.

\chapter{Calculus}

\section{Limits}

\begin{definition}{\( \epsilon \)--\( \delta \) definition of limit of a function}{}
    The \textbf{limit} of a function \( f: D \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( D \):
    \begin{equation*}
        \lim_{\x \to \x_0} \func{f}{\x} = L
        \quad \iff \quad
        \forall \epsilon > 0, \
        \exists \delta > 0, \
        \text{s.t.} \
        0 < \norm{\x - \x_0} < \delta, \,
        \x \in D
        \implies
        \abs{\func{f}{\x} - L} < \epsilon
    \end{equation*}
    i.e., \( \func{f}{\x} \) can be made arbitrarily close to \( L \) by making \( \x \) sufficiently close to \( \x_0 \).
\end{definition}

Alternate notation~\psecref{sec:neighborhood}:
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \x \in D \cap B_{\delta}(\x_0) \setminus \{ \x_0 \}
    \implies
    \func{f}{\x} \in B_{\epsilon}(L)
\end{equation*}

\begin{definition}{Sequence definition of limit of a function}{}
    Sequence definition of \textbf{limit} of a function \( f: D \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( D \):
    \begin{equation*}
        \lim_{\x \to \x_0} \func{f}{\x} = L
        \,\iff\,
        \lim_{n \to \infty} \abs{\func{f}{\x_n} - L} = 0
        \,\iff\,
        \forall {\{ \x_n \}}_{n = 1}^{\infty} \subseteq D \setminus \{ \x_0 \},
        \; \x_n \to \x_0
        \implies
        \func{f}{\x_n} \to L
    \end{equation*}
\end{definition}

\section{Continuity}

For a function \( f: D \subseteq \R^d \to \R \),
\begin{equation*}
    \lim_{\x \to \mathbf{c}} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \abs{\func{f}{\x} - L} < \epsilon, \
    \forall \x \in D \cap B_{\delta}(\mathbf{c}) \setminus \{ \mathbf{c} \}
\end{equation*}

\section{Continuous functions}

A function \( f: D \subseteq \R^d \to \R \) is \textbf{continuous} at a point \( \x_0 \in D \) iff
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = \func{f}{\x_0}
\end{equation*}

\section{Derivative}

The \textbf{derivative} of a function \( f: \R \to \R \) at a point \( x \in \R \) is defined as
\begin{equation*}
    f'(x) = \lim_{h \to 0} \frac{\func{f}{x + h} - \func{f}{x}}{h}
\end{equation*}

The \textbf{gradient} of a function \( f: \R^d \to \R \) at a point \( \x = {\left(x_1, x_2, \ldots, x_d\right)}^\top \in \R^d \) is defined as
\begin{equation*}
    \grad{f}{\x} = {\left( \frac{\partial f}{\partial x_1}(\x), \frac{\partial f}{\partial x_2}(\x), \ldots, \frac{\partial f}{\partial x_d}(\x) \right)}^\top
    \in \R^d
\end{equation*}

The \textbf{Hessian} of a function \( f: \R^d \to \R \) at a point \( \x = {\left(x_1, x_2, \ldots, x_d\right)}^\top \in \R^d \) is defined as
\begin{equation*}
    \hess{f}{\x} =
    \begin{bmatrix}
        \cfrac{\partial^2 f}{\partial x_1^2}(\x) & \cfrac{\partial^2 f}{\partial x_1 \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_1 \partial x_d}(\x) \\
        \cfrac{\partial^2 f}{\partial x_2 \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_2^2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_2 \partial x_d}(\x) \\
        \vdots & \vdots & \ddots & \vdots \\
        \cfrac{\partial^2 f}{\partial x_d \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_d \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_d^2}(\x)
    \end{bmatrix}
    \in \R^{d \times d}
\end{equation*}

\section{Fundamental theorem of calculus}\label{sec:fundamental-theorem-of-calculus}

\begin{theorem}{First fundamental theorem of calculus}{}
    Let \( f: [a, b] \to \R \) be a continuous function.
    Then the function \( F: [a, b] \to \R \) defined by
    \begin{equation*}
        \func{F}{x} = \int_{a}^{x} \func{f}{t} \, dt
    \end{equation*}
    is continuous on \( [a, b] \), differentiable on \( (a, b) \), and
    \begin{equation*}
        \func{F'}{x} = \func{f}{x},
        \quad \forall x \in (a, b)
    \end{equation*}
\end{theorem}

This means that the integral of \( f \) defines an antiderivative of \( f \), i.e., an antiderivative of \( f \) can be constructed by integrating \( f \).

\begin{theorem}{Second fundamental theorem of calculus / Newton-Leibniz theorem}{}
    Let \( f: [a, b] \to \R \) be a continuous function.

    Let \( F \) be any antiderivative of \( f \) on \( [a, b] \), i.e., \( \func{F'}{x} = \func{f}{x}, \; \forall x \in (a, b) \).
    Then
    \begin{equation*}
        \int_{a}^{b} \func{f}{x} \, dx = \func{F}{b} - \func{F}{a}
    \end{equation*}
\end{theorem}

This tells us that we can evaluate definite integrals using antiderivatives.

\section{Intermediate value theorem}

\begin{theorem}{Intermediate value theorem}{}
    Let \( f: D \subseteq \R \to \R \) be continuous on the closed interval \( [a, b] \subseteq D \), then \( f \) takes on any given value between \( \func{f}{a} \) and \( \func{f}{b} \) at some point within the interval, i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \min \{ \func{f}{a}, \func{f}{b} \} \leq N \leq \max \{ \func{f}{a}, \func{f}{b} \}
        \implies
        \exists c \in [a, b] \text{ such that } \func{f}{c} = N
    \end{equation*}
\end{theorem}

\section{Wierstrass extreme value theorem}

If a function \( f: S \to \R \) is continuous on a set \( S \subseteq \R^d \) that is closed and bounded, then \( f \) attains its maximum and minimum values on \( S \), i.e., there exist points \( \x_{\min}, \x_{\max} \in S \) such that
\begin{equation*}
    \func{f}{\x_{\min}} \leq \func{f}{\x} \leq \func{f}{\x_{\max}}, \quad \forall \x \in S
\end{equation*}

\section{Taylor's theorem}

\subsection{Univariate Taylor's theorem}

Let \( f: \R \to \R \) be \( (n+1) \) times continuously differentiable on an interval containing \( a \) and \( x \).
Then, for some \( \xi \) between \( a \) and \( x \),
\begin{align*}
    \func{f}{x}
    & =
    \func{f}{a} + f'(a)(x - a) + \frac{f''(a)}{2!}{(x - a)}^2 + \cdots + \frac{f^{(n)}(a)}{n!}{(x - a)}^n + R_{n+1}(x)
    \\ & =
    \sum_{k = 0}^{n} \frac{f^{(k)}(a)}{k!} {(x - a)}^k + R_{n+1}(x)
\end{align*}
where the remainder term \( R_{n+1}(x) \) is given by the Lagrange form:
\begin{equation*}
    R_{n+1}(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} {(x - a)}^{n+1}
\end{equation*}

\subsection{Multivariate Taylor's theorem}

\section{Levels of smoothness}

A function \( f: \R^d \to \R \) is said to be of \textbf{class} \( \C^k \) if all partial derivatives of \( f \) up to and including order \( k \in \N \) exist and are continuous.

If a function is of class \( \C^k \ \forall k \in \N \), it is said to be of class \( \C^{\infty} \) or \textbf{smooth}.

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \C^0 \), then \( f \) is continuous.

    \item If \( f \in \C^k \), then \( f \in \C^{k-1} \), for \( k \geq 1 \).

    \item (Schwarz's theorem) If \( f \in \C^2 \), then \( \hess{f}{\x} \in \mathbf{S}_d\), i.e., \( \hess{f}{\x} \) is symmetric.
        \begin{equation*}
            \frac{\partial^2 f}{\partial x_i \partial x_j}
            =
            \frac{\partial^2 f}{\partial x_j \partial x_i},
            \quad \forall i, j = 1, 2, \ldots, d
        \end{equation*}

    \item If \( f, g \in \C^k \), then \( f + g, f \cdot g \in \C^k \).

    \item If \( f \in \C^k \), then \( \frac{\partial f}{\partial x_i} \in \C^{k-1}, \ \forall i = 1, 2, \ldots, d \).
\end{itemize}

\section{Lipschitz}

A function \( f: \R^d \to \R \) is said to be \( L \)-\textbf{smooth} if it is differentiable and there exists a constant \( L > 0 \) such that
\begin{equation*}
    \norm{\grad{f}{\x} - \grad{f}{\y}}_2
    \leq
    L \; \normbf{x - y}_2,
    \quad \forall \x, \y \in \R^d
\end{equation*}

The constant \( L \) is called the \textbf{Lipschitz constant} of the gradient \( \nabla f \).

Here, we denote \( f \in \C^{1}_L \) to mean that \( f \) is \( L \)-smooth and of class \( \C^1 \).

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \C^{2} \), then \( f \in \C^{1}_L \) iff \( \hess{f}{\x} \preceq L \I, \ \forall \x \in \R^d \).

    \item If \( f \in \C^{1}_L \), then
        \begin{equation*}
            \abs{ \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x) }
            \leq
            \frac{L}{2} \normbf[\big]{y - x}^2,
            \quad \forall \x, \y \in \R^d
        \end{equation*}

        \begin{proof}
            Follows from \( \func{f}{\x} \pm \frac{L}{2} \norm{\x}^2 \) being convex functions, given \( f \in \C^{1}_L \).
        \end{proof}
\end{itemize}

\chapter{Linear Algebra}

\section{Outer product}

The \textbf{outer product} of two vectors \( \x \in \R^m \) and \( \y \in \R^n \) is defined as
\begin{equation*}
    \x \y^\top =
    \begin{bmatrix}
        x_1 y_1 & x_1 y_2 & \cdots & x_1 y_n \\
        x_2 y_1 & x_2 y_2 & \cdots & x_2 y_n \\
        \vdots & \vdots & \ddots & \vdots \\
        x_m y_1 & x_m y_2 & \cdots & x_m y_n
    \end{bmatrix}
    \in \R^{m \times n}
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \operatorname{rank} (\x \y^\top) = 1, \; \forall \x \in \R^m \setminus \{ \zero \}, \; \forall \y \in \R^n \setminus \{ \zero \} \).

    \item \( \x \x^\top \in \mathbf{S}_d^+, \; \forall \x \in \R^d \).

    \item \( \operatorname{spec}(\x \x^\top) = \{ \norm{\x}_2^2, 0^{(d-1)} \} \).
        Eigenvectors: \( \{ \x \} \cup \underbrace{ \{ \mathbf{v} \mid \dotpbf{v}{x} = 0 \} }_{\x^\perp} \).

        Eigenvalues of \( \x \x^\top \) are \( \norm{\x}_2^2 \) and \( 0 \) (with multiplicity \( d-1 \)).
        The eigenvector corresponding to \( \norm{\x}_2^2 \) is \( \x \); the eigenspace corresponding to \( 0 \) is the orthogonal complement \( \x^\perp \).
\end{itemize}

\section{Matrices}

\subsection{Symmetric Matrices}

A matrix \( \mathbf{A} \in \R^{d \times d} \) is symmetric if \( \mathbf{A} = \mathbf{A}^\top \), i.e., \( a_{ij} = a_{ji}, \ \forall i, j \).

\paragraph{Properties}

\begin{itemize}
    \item \( \mathbf{A}, \mathbf{B} \in \SD \implies \mathbf{A} + \mathbf{B} \in \SD \).
        \quad \( \because {(\mathbf{A} + \mathbf{B})}^\top = \mathbf{A}^\top + \mathbf{B}^\top = \mathbf{A} + \mathbf{B} \).

    \item \( \mathbf{A} \in \SD, c \in \R \implies c \mathbf{A} \in \SD \).
        \quad \( \because {(c \mathbf{A})}^\top = c \mathbf{A}^\top = c \mathbf{A} \).

    \item \( \mathbf{A} \in \SD \implies \mathbf{A}^k \in \SD, \; k \in \N \).
        \quad \( \because {(\mathbf{A}^k)}^\top = {(\mathbf{A}^\top)}^k = \mathbf{A}^k \).

    \item \( \mathbf{A} \in \SD \implies \mathbf{A}^{-1} \in \SD, \text{ if } \mathbf{A} \text{ is invertible} \).
        \quad \( \because {(\inv{\mathbf{A}})}^\top = \inv{(\mathbf{A}^\top)} = \inv{\mathbf{A}} \).
\end{itemize}

\subsection{Symmetric Positive Semi-definite Matrices}

A symmetric matrix \( \mathbf{A} \in \SD \) is \textbf{positive semi-definite} (PSD), denoted as \( \mathbf{A} \succeq \zero \), if
\begin{equation*}
    \x^\top \mathbf{A} \x \geq 0,
    \quad \forall \x \in \R^d
\end{equation*}

\subsection{Symmetric Positive Definite Matrices}

A symmetric matrix \( \mathbf{A} \in \SD \) is \textbf{positive definite} (PD), denoted as \( \mathbf{A} \succ \zero \), if
\begin{equation*}
    \x^\top \mathbf{A} \x > 0,
    \quad \forall \x \in \R^d \setminus \{ \zero \}
\end{equation*}

\section{Eigenvalues and Eigenvectors}

\begin{equation*}
    \mathbf{A}\mathbf{v} = \lambda \mathbf{v}
\end{equation*}
where
\( \mathbf{A} \in \R^{d \times d} \), \quad
\( \lambda \in \R \longrightarrow \) Eigenvalue, \quad
\( \mathbf{v} \in \R^d \setminus \{ \zero \} \longrightarrow \) corresponding Eigenvector.

The set of all eigenvalues of \( \mathbf{A} \) is called the \textbf{spectrum} of \( \mathbf{A} \).

\subsection{Properties}

\begin{itemize}
    \item \( \operatorname{trace}(\mathbf{A}) = \sum_{i = 1}^{d} a_{ii} = \sum_{i = 1}^{d} \lambda_i \).

    \item \( \det(\mathbf{A}) = \prod_{i = 1}^{d} \lambda_i \).

    \item \( \mathbf{A} \in \SD \implies \mathbf{A} \) has an orthonormal basis of eigenvectors in \( \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \mathbf{A} \in \SD \implies \operatorname{spec}(\mathbf{A}) \subseteq \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \mathbf{A} \in \SD \implies \mathbf{v}_i^\top \mathbf{v}_j = 0, \; \text{if } \lambda_i \neq \lambda_j \) \, \psecref{sec:spectral-decomposition-theorem}
\end{itemize}

\section{Spectral Decomposition Theorem}\label{sec:spectral-decomposition-theorem}

\begin{theorem}{Spectral Decomposition Theorem}{}
    For any \( \mathbf{A} \in \SD \), there exists an orthonormal basis of eigenvectors \( \{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d \} \), \; \( \mathbf{v}_i \in \R^d \setminus \{ \zero \} \), with corresponding eigenvalues \( \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d, \; \lambda_i \in \R \), such that
    \begin{equation*}
        \mathbf{A} = \sum_{i = 1}^{d} \lambda_i \mathbf{v}_i \mathbf{v}_i^\top
        = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top
    \end{equation*}
    where \( \mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d] \in \R^{d \times d} \) is an orthogonal matrix (i.e., \( \dotpbf{V}{V} = \I \)) and \( \boldsymbol{\Lambda} = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d) \in \R^{d \times d} \) is a diagonal matrix containing the eigenvalues of \( \mathbf{A} \).
\end{theorem}

\begin{proof}
    The eigenvalues of a real symmetric matrix are all real
    (Can be shown considering Hermitian matrices, Fundamental theorem of algebra, and properties of complex conjugates).

    The eigenvectors corresponding to distinct eigenvalues are orthogonal.
    This can be shown using the fact that for any two eigenvectors \( \mathbf{v}_i \) and \( \mathbf{v}_j \) corresponding to distinct eigenvalues \( \lambda_i \) and \( \lambda_j \), we have
    \begin{align*}
        \mathbf{v}_i^\top \mathbf{A} \mathbf{v}_j
        & =
        \mathbf{v}_i^\top (\mathbf{A} \mathbf{v}_j)
        =
        \lambda_j \mathbf{v}_i^\top \mathbf{v}_j,
        \qquad \text{Similarly, } \;
        \mathbf{v}_j^\top \mathbf{A} \mathbf{v}_i
        =
        \lambda_i \mathbf{v}_j^\top \mathbf{v}_i
        \\
        \text{Now, } \;
        \mathbf{v}_i^\top \mathbf{A} \mathbf{v}_j
        & =
        \mathbf{v}_i^\top \mathbf{A}^\top \mathbf{v}_j
        =
        {(\mathbf{A} \mathbf{v}_i)}^\top \mathbf{v}_j
        =
        {(\lambda_i \mathbf{v}_i)}^\top \mathbf{v}_j
        =
        \lambda_i \mathbf{v}_i^\top \mathbf{v}_j
        =
        \lambda_i \mathbf{v}_j^\top \mathbf{v}_i
        =
        \mathbf{v}_j^\top \mathbf{A} \mathbf{v}_i
        \\
        \implies
        \lambda_j \mathbf{v}_i^\top \mathbf{v}_j
        & =
        \lambda_i \mathbf{v}_i^\top \mathbf{v}_j
        \implies
        (\lambda_j - \lambda_i) \mathbf{v}_i^\top \mathbf{v}_j
        = 0
        \implies
        \mathbf{v}_i^\top \mathbf{v}_j = 0,
        \; \because
        \lambda_i \neq \lambda_j,
        \;\;
        \therefore
        \mathbf{v}_i, \mathbf{v}_j \text{ are orthogonal.}
    \end{align*}

    For repeated eigenvalues, if any, we can always find a set of orthonormal eigenvectors using the Gram-Schmidt process.
    Thereby, we can always find \( d \) orthonormal eigenvectors of \( \mathbf{A} \), thus forming an orthonormal basis for \( \R^d \).
    This gives that \( \mathbf{V} \) is an orthogonal matrix \( \implies \dotpbf{V}{V} = \I \implies \mathbf{V}^{-1} = \mathbf{V}^\top \).

    Now, with \( \boldsymbol{\Lambda} = \operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_d) \) and \( \mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d] \), we have
    \begin{equation*}
        \mathbf{A} \mathbf{V}
        =
        [\mathbf{A} \mathbf{v}_1, \mathbf{A} \mathbf{v}_2, \ldots, \mathbf{A} \mathbf{v}_d]
        =
        [\lambda_1 \mathbf{v}_1, \lambda_2 \mathbf{v}_2, \ldots, \lambda_d \mathbf{v}_d]
        =
        \mathbf{V} \boldsymbol{\Lambda}
        \implies
        \mathbf{A}
        =
        \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^{-1}
        =
        \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top
    \end{equation*}
\end{proof}

\begin{itemize}
    \item The identity matrix \( \I \in \R^{d \times d} \) has eigenvalues \( \lambda_i = 1, \; \forall i \) and the corresponding eigenvectors can be chosen as any orthonormal basis of \( \R^d \), commonly taken as the standard basis vectors \( \mathbf{e}_i \in \R^d \).

    \item \( \mathbf{A} \in \PSD \iff \mathbf{A} \in \SD, \text{ and } \lambda_i \geq 0, \; \forall i \).

    \item \( \mathbf{A} \in \PD \iff \mathbf{A} \in \SD, \text{ and } \lambda_i > 0, \; \forall i \).

    \item \( \mathbf{A} \in \PD, \; \kappa(\mathbf{A}) = 1 \iff \mathbf{A} = c \I, \; c > 0 \).
\end{itemize}

\section{Condition number}

The \textbf{condition number} of a matrix \( \mathbf{A} \in \SD \) is given by
\begin{equation*}
    \kappa(\mathbf{A}) = \frac{\abs{\lambda_{\max}(\mathbf{A})}}{\abs{\lambda_{\min}(\mathbf{A})}}
\end{equation*}
where \( \lambda_{\max}(\mathbf{A}) \) and \( \lambda_{\min}(\mathbf{A}) \) are the largest and smallest eigenvalues of \( \mathbf{A} \) respectively.

\chapter{Inequalities}

\section{AM-GM inequality}

\begin{theorem}{AM-GM inequality}{}
    For any non-negative real numbers \( a_1, a_2, \ldots, a_n \geq 0 \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \frac{1}{n} \sum_{i = 1}^{n} a_i
        \geq
        {\left( \prod_{i = 1}^{n} a_i \right)}^{\frac{1}{n}}
        \qquad \text{i.e.,} \quad
        \overbrace{
            \left( \frac{a_1 + a_2 + \cdots + a_n}{n} \right)
        }^{\text{Arithmetic Mean (AM)}}
        \geq
        \overbrace{
            \sqrt[n]{a_1 a_2 \ldots a_n}
        }^{\text{Geometric Mean (GM)}}
    \end{equation*}
    with equality iff \( a_1 = a_2 = \cdots = a_n \).
\end{theorem}

\begin{proof}
    Can be shown by mathematical induction.
\end{proof}

\section{Cauchy-Schwarz inequality}\label{sec:cauchy-schwarz-inequality}

\begin{theorem}{Cauchy-Schwarz inequality}{}
    For any vectors \( \x, \y \in \R^d \), we have
    \vspace{-1em}
    \begin{equation*}
        \abs{\dotp{\x}{\y}} \leq \norm{\x}_2 \; \norm{\y}_2
    \end{equation*}
    with equality iff \( \x \) and \( \y \) are linearly dependent, i.e., \( \exists c \in \R \) such that \( \x = c \y \) or \( \y = c \x \).
\end{theorem}

\begin{proof}
    Follows in the \hyperlink{line-point-minimisation}{problem} of minimising the distance between a point and a line in \( \R^d \).
\end{proof}

\section{H{\"o}lder's inequality}

\begin{theorem}{H{\"o}lder's inequality}{}
    For any vectors \( \x, \y \in \R^d \) and for any H{\"o}lder conjugates \( p, q > 1 \) such that \( \frac{1}{p} + \frac{1}{q} = 1 \), we have
    \vspace{-1em}
    \begin{equation*}
        \abs{\dotp{\x}{\y}} \leq \norm{\x}_p \; \norm{\y}_q
    \end{equation*}
    with equality iff \( \exists c > 0 \) such that \( \abs{x_i}^p = c \abs{y_i}^q, \; \forall i = 1, 2, \ldots, d \).
\end{theorem}

\section{Kantorovich inequality}\label{sec:kantorovich-inequality}

\begin{theorem}{Kantorovich inequality}{}
    Let \( p_i \geq 0, \ 0 < x_{\min} \leq x_i \leq x_{\max}, \ \forall i \in \{1, 2, \ldots, n\} \), then we have
    \vspace{-1em}
    \begin{equation*}
        \left( \sum_{i = 1}^{n} p_i x_i \right) \left( \sum_{i = 1}^{n} \frac{p_i}{x_i} \right)
        \leq
        \frac{{(x_{\max} + x_{\min})}^2}{4 x_{\max} x_{\min}}
        {\left( \sum_{i = 1}^{n} p_i \right)}^2
    \end{equation*}
    with equality iff \( x_i \in \{ x_{\min}, x_{\max} \}, \ \forall i \in \{1, 2, \ldots, n\} \).
\end{theorem}

\begin{corollary}{Kantorovich inequality for real symmetric positive definite matrices~\citep{Luenberger1984}}{}
    For any \( \Q \in \PD \), with smallest eigenvalue \( \lambda_{\min} \) and largest eigenvalue \( \lambda_{\max} \), and for any \( \x \in \R^d \setminus \{ \zero \} \), we have
    \begin{equation*}
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
    with equality iff \( \x \) is an eigenvector of \( \Q \) corresponding to either \( \lambda_{\min} \) or \( \lambda_{\max} \).
\end{corollary}

\begin{proof}
    Let the matrix \( \Q \) have eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \), with corresponding orthonormal eigenvectors \( \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_d \).
    Since the eigenvectors form an orthonormal basis for \( \R^d \), any vector \( \x \in \R^d \) can be expressed as a linear combination of these eigenvectors as \( \x = \sum_{i = 1}^{d} c_i \mathbf{v}_i \), where each coefficient \( c_i \) is the projection of \( \x \) onto the eigenvector \( \mathbf{v}_i \), given by \( c_i = \mathbf{v}_i^\top \x \).
    Now,
    \begin{align*}
        \qf{\x}{\Q}
        & =
        {\left( \sum_{i = 1}^{d} c_i \mathbf{v}_i \right)}^\top \Q \left( \sum_{j = 1}^{d} c_j \mathbf{v}_j \right)
        =
        \sum_{i = 1}^{d} \sum_{j = 1}^{d} c_i c_j \qf{\mathbf{v}_i}{\Q}[\mathbf{v}_j]
        \\ & =
        \sum_{i = 1}^{d} c_i^2 \qf{\mathbf{v}_i}{\Q}
        +
        \sum_{i \neq j} c_i c_j \qf{\mathbf{v}_i}{\Q}[\mathbf{v}_j]
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i \underbrace{ \dotp{\mathbf{v}_i}{\mathbf{v}_i} }_{= 1}
        +
        \cancel{ \sum_{i \neq j} c_i c_j \lambda_j \underbrace{ \dotp{\mathbf{v}_i}{\mathbf{v}_j} }_{= 0} }
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i
        \\ &
        \text{Similarly, }
        \quad
        \qf{\x}{\Qinv}
        =
        \sum_{i = 1}^{d} c_i^2 \frac{1}{\lambda_i},
        \qquad
        \dotp{\x}{\x}
        =
        \qf{\x}{\I}
        =
        \sum_{i = 1}^{d} c_i^2 (1)
        =
        \sum_{i = 1}^{d} c_i^2
    \end{align*}
    Now, let \( p_i = c_i^2 \) and \( x_i = \lambda_i \) in Kantorovich inequality, we have
    \begin{equation*}
        \underbrace{ \left( \sum_{i = 1}^{d} c_i^2 \lambda_i \right) }_{\qf{\x}{\Q}}
        \underbrace{ \left( \sum_{i = 1}^{d} \frac{c_i^2}{\lambda_i} \right) }_{\qf{\x}{\Qinv}}
        \leq
        \frac{{(\lambda_{\max} + \lambda_{\min})}^2}{4 \lambda_{\max} \lambda_{\min}} \underbrace{ {\left( \sum_{i = 1}^{d} c_i^2 \right)}^2 }_{{(\dotp{\x}{\x})}^2}
        \implies
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
\end{proof}

\section{Polyak-\L{}ojasiewicz (PL) inequality}

\begin{definition}{Polyak-\L{}ojasiewicz (PL) inequality}{}
    A function \( f: D \subseteq \R^n \to \R \) on a convex domain \( D \) satisfies the \textbf{Polyak-\L{}ojasiewicz (PL) inequality} with parameter \( \mu > 0 \) if
    \begin{equation*}
        \half \norm[\Big]{\grad{f}{\x}}^2 \geq \mu \left( \func{f}{\x} - f^\ast \right)
        , \quad \forall \x \in D
    \end{equation*}
    where \( f^\ast = \inf_{\x \in D} \func{f}{\x} \).
    Such an \( f \) is said to be \( \mu \)-\textbf{PL} on \( D \).
\end{definition}

\section{Jensen's inequality}

\begin{theorem}{Jensen's inequality}{}
    For any convex function \( f: \R^d \to \R \), and for any set of points \( {\{ \x_i \}}_{i = 1}^n, \; \x_i \in \R^d \) and weights \( {\{ \alpha_i \}}_{i = 1}^n, \; \alpha_i \geq 0 \) such that \( \sum_{i = 1}^{n} \alpha_i = 1 \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\sum_{i = 1}^{n} \alpha_i \x_i}
        \leq
        \sum_{i = 1}^{n} \alpha_i \func{f}{\x_i}
    \end{equation*}
    with equality iff \( \x_1 = \x_2 = \cdots = \x_n \) or \( f \) is linear on the convex hull of \( {\{ \x_i \}}_{i = 1}^n \).
\end{theorem}

\subsection{Convex hull}

The \textbf{convex hull} of a set of points \( {\{ \x_i \}}_{i = 1}^n, \; \x_i \in \R^d \) is the smallest convex set that contains all the points.
It can be defined as the set of all convex combinations of the points, i.e.,
\begin{equation*}
    \func{\operatorname{conv}}{{\{ \x_i \}}_{i = 1}^n}
    =
    \set{
        \sum_{i = 1}^{n} \alpha_i \x_i
        \;\middle|\;
        \alpha_i \geq 0, \; \sum_{i = 1}^{n} \alpha_i = 1
    }
\end{equation*}
