
\chapter{Real analysis}

\section{Metric space}

A \textbf{metric space} \( (\calS, d) \) is a set \( \calS \) together with a \textit{metric/distance function} \( d: \calS \times \calS \to \R_{\geq 0} \) that satisfies the following axioms for all \( x, y, z \in \calS \):
\begin{itemize}
    \item \textit{Definiteness}: \( \func{d}{x, y} = 0 \iff x = y \)
    \item \textit{Symmetry}: \( \func{d}{x, y} = \func{d}{y, x} \)
    \item \textit{Triangle inequality}: \( d(x, z) \leq \func{d}{x, y} + \func{d}{y, z} \)
\end{itemize}

\section{Norm}

A \textbf{norm} is a real-valued function \( \eta: V \to \R_{\geq 0} \) that satisfies
\begin{itemize}
    \item \textit{Non-negativity}: \( \func{\eta}{x} \geq 0, \quad \forall x \in V \)
    \item \textit{Definiteness}: \( \func{\eta}{x} = 0 \iff x = 0 \)
    \item \textit{Homogeneity}: \( \func{\eta}{s x} = \abs{s} \, \func{\eta}{x}, \quad \forall x \in V, s \in F \)
    \item \textit{Triangle inequality}: \( \func{\eta}{x + y} \leq \func{\eta}{x} + \func{\eta}{y}, \quad \forall x, y \in V \)
\end{itemize}

\section{Vector norm}

A \textbf{vector norm} is a norm defined on a vector space.

A \textbf{normed vector space} \( (\V, \norm{\,\cdot\,}) \) is a vector space \( \V \) over a field \( \mathbb{F} \) (usually \( \R \) or \( \CC \)) equipped with a function \( \norm{\,\cdot\,}: \V \to \R_{\geq 0} \) that satisfies the following properties for all \( \u, \v \in \V \) and \( a \in \mathbb{F} \):
\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\v} \geq 0 \)
    \item \textit{Definiteness}: \( \norm{\v} = 0 \iff \v = \zero \)
    \item \textit{Homogeneity}: \( \norm{a \v} = \abs{a} \, \norm{\v} \)
    \item \textit{Triangle inequality}: \( \norm{\u + \v} \leq \norm{\u} + \norm{\v} \)
\end{itemize}

\paragraph{Properties}

\begin{itemize}
    \item (Reverse triangle inequality) \( \abs{\norm{\u} - \norm{\v}} \leq \norm{\u - \v} \)

    \item (Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality}) \( \abs{\dotp{\u}{\v}} \leq \norm{\u} \, \norm{\v} \)

    \item (Parallelogram law) \( \norm{\u + \v}^2 + \norm{\u - \v}^2 = 2 \, (\norm{\u}^2 + \norm{\v}^2) \)

    \item \( \norm{\u - \v}^2 = \norm{\u}^2 + \norm{\v}^2 - 2 \, \dotp{\u}{\v} \)

    \item \( \norm{\u + \v}^2 = \norm{\u}^2 + \norm{\v}^2 + 2 \, \dotp{\u}{\v} \)

    \item (Polarisation identity)
        \begin{equation*}
            \dotp{\u}{\v}
            =
            \frac{1}{4} \pbrac{\norm{\u + \v}^2 - \norm{\u - \v}^2}
            =
            \frac{1}{2} \pbrac{\norm{\u}^2 + \norm{\v}^2 - \norm{\u - \v}^2}
            =
            \frac{1}{2} \pbrac{\norm{\u + \v}^2 - \norm{\u}^2 - \norm{\v}^2}
        \end{equation*}
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}

\begin{equation*}
    \norm{\x}_p
    =
    \pbrac{\sum_{i = 1}^{d} \abs{x_i}^p}^{\frac{1}{p}},
    \quad p \geq 1
\end{equation*}

\subsubsection{\( L_1 \) norm (Manhattan norm)}

\begin{equation*}
    \norm{\x}_1
    =
    \abs{x_1} + \abs{x_2} + \cdots + \abs{x_d}
    =
    \sum_{i = 1}^{d} \abs{x_i}
\end{equation*}

\subsubsection{\( L_2 \) norm (Euclidean norm)}

\begin{equation*}
    \norm{\x}_2
    =
    \sqrt{x_1^2 + x_2^2 + \cdots + x_d^2}
    =
    \sqrt{\dotp{\x}{\x}}
    \triangleq
    \norm{\x}
\end{equation*}

\subsubsection{\( L_{\infty} \) norm (Maximum norm)}

\begin{equation*}
    \norm{\x}_{\infty}
    =
    \max \set{\abs{x_1}, \abs{x_2}, \ldots, \abs{x_d}}
    =
    \max_{1 \leq i \leq d} \abs{x_i}
\end{equation*}

\subsection{Energy norm}

For \( \Q \in \PD \), the \textbf{energy norm} (or \( \Q \)-norm) is defined as
\begin{equation*}
    \norm{\x}_{\Q}
    =
    \sqrt{\qf{\x}{\Q}}
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item \( \norm{\x}_{\I} = \norm{\x}_2 \)

    \item \( \norm{\x}_{\Q} = \norm{\Q^\half \x}_2 \)

    \item \( \norm{\x}_{\Q} = \norm{\x}_{\Q^\half} \)

    \item \( \lambda_{\min}(\Q) \; \norm{\x}^2 \leq \norm{\x}_{\Q}^2 \leq \lambda_{\max}(\Q) \; \norm{\x}^2 \)

    \item \( \norm{\x}_{\Q} = \norm{\Q \x}_{\Qinv} \)

    \item \( \norm{\x}_{\Q} = \norm{\Qinv \x}_{\Q} \)
\end{itemize}

\section{Matrix norm}

A \textbf{matrix norm} is a norm defined on a space of matrices.

\begin{itemize}
    \item \textit{Non-negativity}: \( \norm{\A} \geq 0 \)

    \item \textit{Definiteness}: \( \norm{\A} = 0 \iff \A = \zero \)

    \item \textit{Homogeneity}: \( \norm{a \A} = \abs{a} \, \norm{\A} \)

    \item \textit{Triangle inequality}: \( \norm{\A + \B} \leq \norm{\A} + \norm{\B} \)
\end{itemize}

\begin{itemize}
    \item \textit{Sub-multiplicativity}: \( \norm{\A \B} \leq \norm{\A} \, \norm{\B} \)

    \item \textit{Consistency}: \( \norm{\A \x} \leq \norm{\A} \, \norm{\x}, \quad \forall \x \in \R^d \)

    \item \textit{Transpose invariance}: \( \norm{\A} = \norm{\A^\top} \)

    \item \textit{Continuity}: \( \norm{\A} \) is continuous with respect to the entries of \( \A \)
\end{itemize}

\subsection{\texorpdfstring{\( L_p \) norm}{Lp norm}}

\begin{equation*}
    \norm{\A}_p
    =
    \sup_{\x \neq \zero} \frac{\norm{\A \x}_p}{\norm{\x}_p}
    =
    \max_{\norm{\x}_p = 1} \norm{\A \x}_p,
    \quad p \geq 1
\end{equation*}

\subsubsection{\( L_1 \) norm (Maximum absolute column sum norm)}

\begin{equation*}
    \norm{\A}_1
    =
    \max_{1 \leq j \leq d} \sum_{i = 1}^{d} \abs{a_{ij}}
    =
    \max_{1 \leq j \leq d} \norm{\A_{\ast j}}_1
\end{equation*}

\subsubsection{\( L_2 \) norm (Spectral norm)}

\begin{equation*}
    \norm{\A}_2
    =
    \sqrt{\lambda_{\max}(\A^\top \A)}
    =
    \sigma_{\max}(\A)
\end{equation*}

For \( \A \in \SD, \norm{\A}_2 = \rho(\A) = \abs{\lambda_{\max}(\A)} \).

\subsubsection{\( L_{\infty} \) norm (Maximum absolute row sum norm)}

\begin{equation*}
    \norm{\A}_{\infty}
    =
    \max_{1 \leq i \leq d} \sum_{j = 1}^{d} \abs{a_{ij}}
    =
    \max_{1 \leq i \leq d} \norm{\A_{i \ast}}_1
\end{equation*}

\subsection{Frobenius norm (Euclidean norm)}

\begin{equation*}
    \norm{\A}_F
    =
    \sqrt{\sum_{i = 1}^{d} \sum_{j = 1}^{d} a_{ij}^2}
    =
    \sqrt{\trace{\A^\top \A}}
    =
    \sqrt{\sum_{i = 1}^{d} \sigma_i^2(\A)}
    =
    \norm{\operatorname{vec}(\A)}_2
\end{equation*}

The vector \( \operatorname{vec}(\A) \) is the \textbf{vectorisation} of matrix \( \A \), obtained by stacking its columns into a single column vector, i.e.,
\begin{equation*}
    \A
    =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1d} \\
        a_{21} & a_{22} & \cdots & a_{2d} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{d1} & a_{d2} & \cdots & a_{dd}
    \end{bmatrix}
    \in \R^{d \times d}
    \implies
    \operatorname{vec}(\A)
    =
    \begin{bmatrix}
        a_{11} \\
        a_{21} \\
        \vdots \\
        a_{d1} \\
        a_{12} \\
        a_{22} \\
        \vdots \\
        a_{d2} \\
        \vdots \\
        a_{1d} \\
        a_{2d} \\
        \vdots \\
        a_{dd}
    \end{bmatrix}
    \in \R^{d^2}
\end{equation*}

\subsection{Spectral radius}

The \textbf{spectral radius} of a matrix \( \A \in \R^{d \times d} \) with eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \) is defined as
\begin{equation*}
    \rho(\A)
    =
    \max \set{ \abs{\lambda_1}, \abs{\lambda_2}, \ldots, \abs{\lambda_d} }
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item \( \rho(\A) = \rho(\A^\top) \)

    \item \( \rho(\A^k) = {\bbrac{\rho(\A)}}^k, \quad k \in \N \)

    \item \( \rho(\inv{\A}) = \frac{1}{\rho(\A)}, \quad \text{if } \A \text{ is invertible} \)

    \item \( \rho(\A) \leq \norm{\A}, \quad \forall \text{ matrix norms } \norm{\,\cdot\,} \)

    \item (Gelfand's formula) \( \rho(\A) = \lim_{k \to \infty} \pbrac{\norm{\A^k}}^{\frac{1}{k}}, \quad \forall \text{ matrix norms } \norm{\,\cdot\,} \)

    \item \( \rho(\A) \leq \norm{\A^k}^{\frac{1}{k}}, \quad \forall k \in \N, \; \forall \text{ sub-multiplicative matrix norms } \norm{\,\cdot\,} \)

    \item \( \rho(\A \B) \leq \rho(\A) \, \rho(\B) \), if \( \A \) and \( \B \) commute, i.e., \( \A \B = \B \A \)

    \item \( \A \in \R^{d \times d} \implies \rho(\A) \leq \norm{\A}_2 \)

    \item \( \A \in \SD \implies \rho(\A) = \norm{\A}_2 \)

    \item \( \A \in \SD \implies \norm{\A \mathbf{x}}_2 \leq \rho(\A) \, \norm{\x}_2, \quad \forall \x \in \R^d \)
\end{itemize}

\section{Neighborhood}\label{sec:neighborhood}

A \textbf{neighborhood} of a point \( \x \in \R^d \) is an open ball centered at \( \x \) with radius \( r > 0 \), defined as
\begin{equation*}
    B_{r}(\x)
    =
    \set{\z \in \R^d \given \norm{\z - \x} < r},
    \quad r > 0,
    \; \x \in \R^d
\end{equation*}

\section{Interior point}

A point \( \x \in \calS \) is an \textbf{interior point} of set \( \calS \subseteq \R^d \) if there exists a neighborhood of \( \x \) that is entirely contained within \( \calS \), i.e., there exists an \( r > 0 \) such that \( B_{r}(\x) \subseteq \calS \).

\section{Limit point}

A point \( \x \in \R^d \) is a \textbf{limit point} of set \( \calS \subseteq \R^d \) if every neighborhood of \( \x \) contains at least one point of \( \calS \) different from \( \x \) itself, i.e., for every \( r > 0 \), there exists a point \( \y \in \calS \setminus \set{\x} \) such that \( \y \in B_{r}(\x) \).
\begin{equation*}
    \forall r > 0, \; B_{r}(\x) \cap (\calS \setminus \set{\x}) \neq \emptyset
\end{equation*}

Trivially, every interior point of \( \calS \) is also a limit point of \( \calS \).

\section{Open set}

A set \( \calS \subseteq \R^d \) is \textbf{open} if every point in \( \calS \) is an interior point of \( \calS \).

\section{Closed set}

A set \( \calS \subseteq \R^d \) is \textbf{closed} if it contains all its limit points, i.e., if \( \x \) is a limit point of \( \calS \), then \( \x \in \calS \).

\section{Bounded set}

A set \( \calS \subseteq \R^d \) is \textbf{bounded} if there exists \( \z \in \R^d, M \in \R \) such that \( \norm{\z - \x} < M, \; \forall \x \in \calS \).

\section{Infimum}

The \textbf{infimum} (greatest lower bound) of a set \( \calS \subseteq \R \) is the largest real number \( m \) such that \( m \leq x, \; \forall x \in \calS \).

\section{Supremum}

The \textbf{supremum} (least upper bound) of a set \( \calS \subseteq \R \) is the smallest real number \( M \) such that \( M \geq x, \; \forall x \in \calS \).

\section{Closure}

The \textbf{closure} of a set \( \calS \subseteq \R^d \) is the smallest closed set containing \( \calS \), which can be obtained by adding all limit points of \( \calS \) to \( \calS \) itself.
\begin{equation*}
    \operatorname{closure}{(\calS)} = \calS \cup \set{\text{all limit points of } \calS}
\end{equation*}

\section{Convergent sequence}

A sequence of real numbers \( \set{a_n}_{n = 1}^{\infty} \) is said to \textbf{converge} to a limit \( L \in \R \) if
\begin{equation*}
    a_n \to L
    \quad \iff \quad
    \lim_{n \to \infty} a_n = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall n > N \implies \abs{a_n - L} < \epsilon
\end{equation*}

\section{Cauchy sequence}

A sequence of real numbers \( \set{a_k}_{k = 1}^{\infty} \) is a \textbf{Cauchy sequence} if
\begin{equation*}
    \forall \epsilon > 0, \
    \exists N \in \N, \
    \text{s.t.} \
    \forall m, n > N \implies \abs{a_n - a_m} < \epsilon
\end{equation*}

In \( \R \), every Cauchy sequence is convergent, and every convergent sequence is Cauchy, since \( \R \) is complete.

\chapter{Calculus}

\section{Limits}

\begin{definition}{\( \epsilon \)--\( \delta \) definition of limit of a function}{}
    The \textbf{limit} of a function \( f: \calD \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( \calD \):
    \vspace{-0.5em}
    \begin{equation*}
        \lim_{\x \to \x_0} \func{f}{\x} = L
        \quad \iff \quad
        \forall \epsilon > 0, \
        \exists \delta > 0, \
        \text{s.t.} \
        0 < \norm{\x - \x_0} < \delta, \,
        \x \in \calD
        \implies
        \abs{\func{f}{\x} - L} < \epsilon
    \end{equation*}

    \vspace{-0.5em}
    i.e., \( \func{f}{\x} \) can be made arbitrarily close to \( L \) by making \( \x \) sufficiently close to \( \x_0 \).
\end{definition}

Alternate notation~\psecref{sec:neighborhood}:
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \x \in \calD \cap B_{\delta}(\x_0) \setminus \set{\x_0}
    \implies
    \func{f}{\x} \in B_{\epsilon}(L)
\end{equation*}

\begin{definition}{Sequence definition of limit of a function}{}
    Sequence definition of \textbf{limit} of a function \( f: \calD \subseteq \R^d \to \R \) at a limit point \( \x_0 \) of \( \calD \):
    \vspace{-0.5em}
    \begin{equation*}
        \lim_{\x \to \x_0} \func{f}{\x} = L
        \,\iff\,
        \lim_{n \to \infty} \abs{\func{f}{\x_n} - L} = 0
        \,\iff\,
        \forall \set{\x_n}_{n = 1}^{\infty} \subseteq \calD \setminus \set{\x_0},
        \; \x_n \to \x_0
        \implies
        \func{f}{\x_n} \to L
    \end{equation*}
\end{definition}

\section{Continuity}

For a function \( f: \calD \subseteq \R^d \to \R \),
\begin{equation*}
    \lim_{\x \to \mathbf{c}} \func{f}{\x} = L
    \quad \iff \quad
    \forall \epsilon > 0, \
    \exists \delta > 0, \
    \text{s.t.} \
    \abs{\func{f}{\x} - L} < \epsilon, \
    \forall \x \in \calD \cap B_{\delta}(\mathbf{c}) \setminus \set{\mathbf{c}}
\end{equation*}

\section{Continuous functions}

A function \( f: \calD \subseteq \R^d \to \R \) is \textbf{continuous} at a point \( \x_0 \in \calD \) iff
\begin{equation*}
    \lim_{\x \to \x_0} \func{f}{\x} = \func{f}{\x_0}
\end{equation*}

\section{Derivative}

The \textbf{derivative} of a function \( f: \R \to \R \) at a point \( x \in \R \) is defined as
\begin{equation*}
    \func{\frac{df}{dx}}{x}
    =
    \func{f'}{x}
    \triangleq
    \lim_{h \to 0} \frac{\func{f}{x + h} - \func{f}{x}}{h}
\end{equation*}

The \textbf{partial derivative} of a function \( f: \R^d \to \R \) with respect to the \( i^{\text{th}} \) dimension at a point \( \x = \bbrac{x_1, x_2, \ldots, x_d}^\top \in \R^d \) is defined as
\begin{equation*}
    \func{\frac{\partial f}{\partial x_i}}{\x}
    \triangleq
    =
    \lim_{h \to 0} \frac{\func{f}{x_1, x_2, \ldots, x_i + h, \ldots, x_d} - \func{f}{x_1, x_2, \ldots, x_i, \ldots, x_d}}{h}
    =
    \lim_{h \to 0} \frac{\func{f}{\x + h \e_i} - \func{f}{\x}}{h}
\end{equation*}
where \( \e_i \) is the \( i^{\text{th}} \) standard basis vector in \( \R^d \).

The \textbf{gradient} \( \nabla\!f: \R^d \to \R^d \) of a function \( f: \R^d \to \R \) at a point \( \x = \bbrac{x_1, x_2, \ldots, x_d}^\top \in \R^d \) where \( f \) is differentiable is given by
\begin{equation*}
    \grad{f}{\x}
    =
    \begin{bmatrix}
        \func{\cfrac{\partial f}{\partial x_1}}{\x}
        &
        \func{\cfrac{\partial f}{\partial x_2}}{\x}
        &
        \cdots
        &
        \func{\cfrac{\partial f}{\partial x_d}}{\x}
    \end{bmatrix}^\top
    \in \R^d
\end{equation*}

The \textbf{directional derivative} \( \nabla_{\v} f: \R^d \to \R \) of a function \( f: \R^d \to \R \) along the vector \( \v \in \R^d \) is defined as
\begin{equation*}
    \func{\nabla_{\v} f}{\x}
    \triangleq
    \lim_{h \to 0} \frac{\func{f}{\x + h \v} - \func{f}{\x}}{h \norm{\v}}
    =
    \frac{1}{\norm{\v}}
    \frac{d}{dt} \func{f}{\x + t \v} \Big|_{t = 0}
    =
    \frac{\dotp{\grad{f}{\x}}{\v}}{\norm{\v}}
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item The gradient \( \grad{f}{\x} \) points in the direction of the steepest ascent of \( f \) at \( \x \), and the negative gradient \( -\grad{f}{\x} \) points in the direction of the steepest descent of \( f \) at \( \x \).
        \begin{equation*}
            \argmax_{\v \in \R^d} \func{\nabla_{\v} f}{\x}
            =
            \grad{f}{\x},
            \qquad \qquad
            \argmin_{\v \in \R^d} \func{\nabla_{\v} f}{\x}
            =
            -\grad{f}{\x}
        \end{equation*}
\end{itemize}

The \textbf{Hessian} \( \nabla^2\!f: \R^d \to \R^{d \times d} \) of a function \( f: \R^d \to \R \) at a point \( \x = \bbrac{x_1, x_2, \ldots, x_d}^\top \in \R^d \) where \( f \) is twice differentiable is given by
\begin{equation*}
    \hess{f}{\x} =
    \begin{bmatrix}
        \cfrac{\partial^2 f}{\partial x_1^2}(\x) & \cfrac{\partial^2 f}{\partial x_1 \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_1 \partial x_d}(\x) \\
        \cfrac{\partial^2 f}{\partial x_2 \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_2^2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_2 \partial x_d}(\x) \\
        \vdots & \vdots & \ddots & \vdots \\
        \cfrac{\partial^2 f}{\partial x_d \partial x_1}(\x) & \cfrac{\partial^2 f}{\partial x_d \partial x_2}(\x) & \cdots & \cfrac{\partial^2 f}{\partial x_d^2}(\x)
    \end{bmatrix}
    \in \R^{d \times d}
\end{equation*}

\paragraph{Properties}

\begin{itemize}
    \item If \( f \) is differentiable at \( x \), then \( f \) is continuous at \( x \).

    \item \( \displaystyle \func{f}{\x} = \half \qf{\x}{\A}, \; \A = \outp{\u}{\v}, \; \u, \v \in \R^d, \; \u \neq \v \implies \hess{f}{\x} = \half (\A + \A^\top) = \half (\outp{\u}{\v} + \outp{\v}{\u}) \).
\end{itemize}

\section{Levels of smoothness}

A function \( f: \R^d \to \R \) is said to be of \textbf{class} \( \classC^k \) if all partial derivatives of \( f \) up to and including order \( k \in \N \) exist and are continuous.

If a function is of class \( \classC^k \; \forall k \in \N \), it is said to be of class \( \classC^{\infty} \) or \textbf{smooth}.

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \classC^0 \), then \( f \) is continuous.

    \item If \( f \in \classC^k \), then \( f \in \classC^{k-1} \), for \( k \geq 1 \).

    \item (Schwarz's theorem) If \( f \in \classC^2 \), then \( \hess{f}{\x} \in \SD \), i.e., \( \hess{f}{\x} \) is symmetric.
        \begin{equation*}
            \frac{\partial^2 f}{\partial x_i \partial x_j}
            =
            \frac{\partial^2 f}{\partial x_j \partial x_i},
            \quad \forall i, j = 1, 2, \ldots, d
        \end{equation*}

    \item If \( f, g \in \classC^k \), then \( f + g, f \cdot g \in \classC^k \).

    \item If \( f \in \classC^k \), then \( \frac{\partial f}{\partial x_i} \in \classC^{k-1}, \; \forall i = 1, 2, \ldots, d \).
\end{itemize}

\section{Lipschitz}

A function \( f: \R^d \to \R \) is said to be \( L \)-\textbf{smooth} if it is differentiable and there exists a constant \( L > 0 \) such that
\begin{equation*}
    \norm{\grad{f}{\x} - \grad{f}{\y}}_2
    \leq
    L \; \norm{\x - \y}_2,
    \quad \forall \x, \y \in \R^d
\end{equation*}

The constant \( L \) is called the \textbf{Lipschitz constant} of the gradient \( \nabla f \).

Here, we denote \( f \in \classC^{1}_L \) to mean that \( f \) is \( L \)-smooth and of class \( \classC^1 \).

\subsection{Properties}

\begin{itemize}
    \item If \( f \in \classC^{2} \), then \( f \in \classC^{1}_L \) iff \( \hess{f}{\x} \preceq L \I, \; \forall \x \in \R^d \).

    \item If \( f \in \classC^{1}_L \), then
        \begin{equation*}
            \abs{ \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x) }
            \leq
            \frac{L}{2} \norm[\big]{\y - \x}^2,
            \quad \forall \x, \y \in \R^d
        \end{equation*}

        \begin{proof}
            Follows from \( \func{f}{\x} \pm \frac{L}{2} \norm{\x}^2 \) being convex functions, given \( f \in \classC^{1}_L \).
        \end{proof}
\end{itemize}

\section{Fundamental theorem of calculus}\label{sec:fundamental-theorem-of-calculus}

\begin{theorem}{First fundamental theorem of calculus~\citep{wiki}}{}
    Let \( f: [a, b] \to \R \) be a continuous function.
    Then the function \( F: [a, b] \to \R \) defined by
    \vspace{-0.5em}
    \begin{equation*}
        \func{F}{x} = \int_{a}^{x} \func{f}{t} \, dt
    \end{equation*}

    \vspace{-0.5em}
    is continuous on \( [a, b] \), differentiable on \( (a, b) \), and
    \vspace{-0.5em}
    \begin{equation*}
        \func{F'}{x} = \func{f}{x},
        \quad \forall x \in (a, b)
    \end{equation*}
\end{theorem}

This means that the integral of \( f \) defines an antiderivative of \( f \), i.e., an antiderivative of \( f \) can be constructed by integrating \( f \).

\begin{theorem}{Second fundamental theorem of calculus / Newton-Leibniz theorem~\citep{wiki}}{}
    Let \( f: [a, b] \to \R \) be a continuous function.

    Let \( F \) be any antiderivative of \( f \) on \( [a, b] \), i.e., \( \func{F'}{x} = \func{f}{x}, \; \forall x \in (a, b) \).
    Then
    \vspace{-0.5em}
    \begin{equation*}
        \int_{a}^{b} \func{f}{x} \, dx = \func{F}{b} - \func{F}{a}
    \end{equation*}
\end{theorem}

This tells us that we can evaluate definite integrals using antiderivatives.

\section{Intermediate value theorem}

\begin{theorem}{Intermediate value theorem~\citep{wiki}}{}
    Let \( f: \calD \subseteq \R \to \R \) be continuous on the closed interval \( [a, b] \subseteq \calD \), then \( f \) takes on any given value between \( \func{f}{a} \) and \( \func{f}{b} \) at some point within the interval, i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \min \set{\func{f}{a}, \func{f}{b}} \leq N \leq \max \set{\func{f}{a}, \func{f}{b}}
        \implies
        \exists c \in [a, b] \text{ such that } \func{f}{c} = N
    \end{equation*}
\end{theorem}

\section{Wierstrass extreme value theorem}

\begin{theorem}{Wierstrass extreme value theorem}{}
    If a function \( f: \calS \to \R \) is continuous on a set \( \calS \subseteq \R^d \) that is closed and bounded, then \( f \) attains its maximum and minimum values on \( \calS \), i.e., there exist points \( \x_{\min}, \x_{\max} \in \calS \) such that
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\x_{\min}} \leq \func{f}{\x} \leq \func{f}{\x_{\max}}, \quad \forall \x \in \calS
    \end{equation*}
\end{theorem}

\section{Taylor's theorem}\label{sec:taylor-theorem}

\begin{theorem}{(Univariate) Taylor's theorem~\cite{wiki}}{}
    Let \( f: \R \to \R \) be \( k \) times differentiable at the point \( a \in \R, \; k \in \N_{\geq 1} \).
    Then we have
    \vspace{-0.5em}
    \begin{align*}
        \func{f}{x}
        & =
        \func{P_k}{x}
        +
        \func{R_k}{x},
        \\
        \func{P_k}{x}
        & \triangleq
        \sum_{i = 0}^{k}
        \frac{\func{f^{(i)}}{a}}{i!} \pbrac{x - a}^i
        \tag{\textbf{\( k \)-th order Taylor polynomial of \( f \) at point \( a \)}}
        \\
        \implies
        \func{R_k}{x}
        & \in
        \smalloh{\abs{x - a}^k},
        \quad x \to a
    \end{align*}
\end{theorem}

\begin{theorem}{(Multivariate) Taylor's theorem~\cite{wiki}}{}
    Let \( f: \R^d \to \R \) be \( k \) times continuously differentiable at the point \( \a \in \R^d, \; k \in \N_{\geq 1} \).
    Then there exist functions \( h_{\alpha}: \R^d \to \R, \; \abs{\alpha} = k \) such that
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\x}
        =
        \sum_{\abs{\alpha} \leq k}
        \frac
        {\partial^{\alpha} \func{f}{\a}}
        {\alpha!}
        \pbrac{\x - \a}^{\alpha}
        +
        \sum_{\abs{\alpha} = k}
        \func{h_{\alpha}}{\x}
        \pbrac{\x - \a}^{\alpha},
        \quad
        \lim_{\x \to \a} \func{h_{\alpha}}{\x} = 0
    \end{equation*}

    \vspace{-0.5em}
    See multi-index notation~\psecref{sec:multi-index-notation}.
\end{theorem}

\subsection{Forms of the remainder}

\subsubsection{Peano form of the remainder}

\begin{equation*}
    \exists
    h_k: \R \to \R
    \text{ such that }
    \func{f}{x}
    =
    \func{P_k}{x}
    +
    \func{h_k}{x} \pbrac{x - a}^k,
    \quad
    \lim_{x \to a} \func{h_k}{x} = 0
\end{equation*}

\subsubsection{Mean-value forms of the remainder}

Assumes \( f \) is \( (k+1) \) times differentiable on the open interval between \( a \) and \( x \), with \( f^{(k)} \) continuous on the closed interval between \( a \) and \( x \).
\begin{align*}
    \func{R_k}{x}
    & =
    \frac
    {\func{f^{(k+1)}}{\xi_L}}
    {(k+1)!}
    \pbrac{x - a}^{k+1},
    \quad \xi_L \in \R
    \text{ between } a \text{ and } x
    \tag{\textbf{Lagrange form}}
    \\
    \func{R_k}{x}
    & =
    \frac
    {\func{f^{(k+1)}}{\xi_C}}
    {k!}
    \pbrac{x - \xi_C}^k
    \pbrac{x - a},
    \quad \xi_C \in \R
    \text{ between } a \text{ and } x
    \tag{\textbf{Cauchy form}}
    \\
    \func{R_k}{x}
    & =
    \frac
    {\func{f^{(k+1)}}{\xi_S}}
    {k!}
    \pbrac{x - \xi_S}^{k + 1 - p}
    \frac{\pbrac{x - a}^p}{p},
    \quad p > 0,
    \; \xi_S \in \R
    \text{ between } a \text{ and } x
    \tag{\textbf{Schl\"omilch form}}
\end{align*}
Lagrange and Cauchy form are special cases of Schl\"omilch form with \( p = k + 1 \) and \( p = 1 \), respectively.

\subsubsection{Integral form of the remainder}

Assumes \( f^{(k)} \) is absolutely continuous on the closed interval between \( a \) and \( x \), which implies that \( f^{(k+1)} \) exists almost everywhere on the interval and is integrable.
\begin{equation*}
    \func{R_k}{x}
    =
    \frac{1}{k!}
    \int_{a}^{x}
    \func{f^{(k+1)}}{t}
    \pbrac{x - t}^k
    \, dt
\end{equation*}

\chapter{Linear algebra}

\section{Vectors}

A \textbf{vector} \( \x \in \R^d \) is an ordered tuple of \( d \) real numbers, i.e.,
\begin{equation*}
    \x
    =
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_d
    \end{bmatrix}^\top,
    \quad x_i \in \R,
    \; \forall i = 1, 2, \ldots, d
\end{equation*}

\subsection{Linear combinations}

A \textbf{linear combination} of a set of vectors \( \set{\v_i}_{i = 1}^{n} \subseteq \R^d \) is defined as
\begin{equation*}
    \sum_{i = 1}^{n} \alpha_i \v_i,
    \quad \alpha_i \in \R,
    \; \forall i \in \set{1, 2, \ldots, n}
\end{equation*}

\subsection{Linear independence}

A set of vectors \( \set{\v_i}_{i = 1}^{n} \subseteq \R^d \) are said to be \textbf{linearly independent} if
\begin{equation*}
    \sum_{i = 1}^{n} \alpha_i \v_i = \zero,
    \quad \alpha_i \in \R,
    \; \forall i \in \set{1, 2, \ldots, n}
    \quad \implies \quad
    \alpha_i = 0,
    \; \forall i \in \set{1, 2, \ldots, n}
\end{equation*}

Otherwise, they are said to be \textbf{linearly dependent}.

\section{Linear span}

The \textbf{linear span} / \textbf{linear hull} of a set of vectors \( \set{\v_i}_{i = 1}^{n}, \v_i \in \R^d \) is defined as the set of all possible linear combinations of the vectors, i.e.,
\begin{equation*}
    \spanset{\v_i}_{i = 1}^{n}
    =
    \set{
        \sum_{i = 1}^{n} \alpha_i \v_i
        \given
        \alpha_i \in \R,
        \; \forall i \in \set{1, 2, \ldots, n}
    }
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \spanset{\v_i}_{i = 1}^{n} \) is a subspace of \( \R^d \).

    \item If \( \set{\v_i}_{i = 1}^{n} \) are linearly independent, then \( \dim{\spanset{\v_i}_{i = 1}^{n}} = n \).

    \item If \( n < d \), then \( \spanset{\v_i}_{i = 1}^{n} \subsetneq \R^d \), i.e., \( \spanset{\v_i}_{i = 1}^{n} \) is a proper subspace of \( \R^d \).

    \item If \( n = d \) and \( \set{\v_i}_{i = 1}^{n} \) are linearly independent, then \( \spanset{\v_i}_{i = 1}^{n} = \R^d \).

    \item If \( n > d \), then \( \set{\v_i}_{i = 1}^{n} \) are linearly dependent.

    \item \( \spanset{\e_i}_{i = 1}^{d} = \R^d  \).
\end{itemize}

\section{Basis}

A \textbf{basis} of a subspace \( \V \subseteq \R^d \) is a set of linearly independent vectors \( \set{\v_i}_{i = 1}^{k} \subseteq \V \) such that
\begin{equation*}
    \V
    =
    \spanset{\v_i}_{i = 1}^{k}
\end{equation*}
i.e., the basis is a linearly independent spanning set of \( \V \).

The vectors \( \set{\v_i}_{i = 1}^{k} \) are called the \textbf{basis vectors} of \( \V \).

The number of vectors \( k \) in the basis is called the \textbf{dimension} of the subspace \( \V \), denoted as \( \dim{\V} = k \).

\subsection{Properties}

\begin{itemize}
    \item Any vector \( \x \in \V \) can be expressed (uniquely) as a linear combination of the basis vectors.
        \begin{equation*}
            \x
            =
            \sum_{i = 1}^{k} c_i \v_i,
            \quad c_i \in \R,
            \; \forall i \in \set{1, 2, \ldots, k}
        \end{equation*}
        The coefficients \( c_i \) are called the \textbf{coordinates}/\textbf{components} of \( \x \) with respect to the basis \( \set{\v_i}_{i = 1}^{k} \).

    \item Every vector space \( \V \subseteq \R^d \) has a basis.

    \item A vector space can have several bases; however, all bases of a vector space have the same dimension.\\
        This is known as the \textbf{dimension theorem} for (finite-dimensional) vector spaces.\\
        (Can be shown by Steinitz exchange lemma).

    \item The orthogonal complement of \( \V \) is given by
        \begin{equation*}
            \V^\perp
            =
            \set{
                \x \in \R^d \given \dotp{\x}{\v_i} = 0,
                \; \forall i = 1, 2, \ldots, k
            }
        \end{equation*}
\end{itemize}

\section{Orthonormal basis}

A set of vectors \( \set{\v_i}_{i = 1}^{k} \subseteq \R^d \) is said to form an \textbf{orthonormal basis} of a subspace \( \V \subseteq \R^d \) if
\begin{itemize}
    \item \textit{Basis}: \( \V = \spanset{\v_i}_{i = 1}^{k} \)
    \item \textit{Orthogonality}: \( \dotp{\v_i}{\v_j} = 0, \; \forall i \neq j \)
    \item \textit{Normality}: \( \norm{\v_i}_2 = 1, \; \forall i \)
\end{itemize}

\section{Dot product}

The \textbf{dot product} (or inner product) of two vectors \( \x, \y \in \R^d \) is defined as
\begin{equation*}
    \dotp{\x}{\y}
    =
    \sum_{i = 1}^{d} x_i y_i
    \quad \in \R
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \textit{Commutative}: \( \dotp{\x}{\y} = \dotp{\y}{\x}, \; \forall \x, \y \in \R^d \).

    \item \textit{Distributive}: \( \dotp{\x}{(\y + \z)} = \dotp{\x}{\y} + \dotp{\x}{\z}, \; \forall \x, \y, \z \in \R^d \).

    \item \textit{Associative}: \( \dotp{(a \x)}{\y} = \dotp{\x}{(a \y)} = a \pbrac{\dotp{\x}{\y}}, \; \forall \x, \y \in \R^d, \; a \in \R \).

    \item \textit{Definiteness}: \( \dotp{\x}{\x} = \norm{\x}_2^2 \geq 0, \; \forall \x \in \R^d, \; \text{and } \dotp{\x}{\x} = 0 \iff \x = \zero \).

    \item \textit{Orthogonality}: \( \x \perp \y \iff \dotp{\x}{\y} = 0 \).

    \item Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality}: \( \abs{\dotp{\x}{\y}} \leq \norm{\x}_2 \, \norm{\y}_2, \; \forall \x, \y \in \R^d \).

    \item \( \dotp{\x}{\y} = \norm{\x}_2 \, \norm{\y}_2 \cos \theta \), where \( \theta \) is the angle between \( \x \) and \( \y \).

    \item \( \dotp{\x}{\y} = \trace{\outp{\y}{\x}}, \; \forall \x, \y \in \R^d \).

    \item \( \dotp{\x}{\y} = \norm{\x}_2^2, \iff \x = \y \).
\end{itemize}

\section{Outer product}

The \textbf{outer product} of two vectors \( \x \in \R^m \) and \( \y \in \R^n \) is defined as
\begin{equation*}
    \outp{\x}{\y} =
    \begin{bmatrix}
        x_1 y_1 & x_1 y_2 & \cdots & x_1 y_n \\
        x_2 y_1 & x_2 y_2 & \cdots & x_2 y_n \\
        \vdots & \vdots & \ddots & \vdots \\
        x_m y_1 & x_m y_2 & \cdots & x_m y_n
    \end{bmatrix}
    \in \R^{m \times n}
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \rank{\outp{\x}{\y}} = 1, \; \forall \x \in \R^m \setminus \set{\zero}, \; \forall \y \in \R^n \setminus \set{\zero} \).

    \item \( \outp{\x}{\x} \in \PSD, \; \forall \x \in \R^d \).

    \item \( \operatorname{spec}(\outp{\x}{\x}) = \set{\norm{\x}_2^2, 0^{(d-1)}} \).
        Eigenvectors: \( \set{\x} \cup \underbrace{ \set{\v \given \dotp{\v}{\x} = 0} }_{\x^\perp} \).

        Eigenvalues of \( \outp{\x}{\x} \) are \( \norm{\x}_2^2 \) and \( 0 \) (with multiplicity \( d-1 \)).
        The eigenvector corresponding to \( \norm{\x}_2^2 \) is \( \x \); the eigenspace corresponding to \( 0 \) is the orthogonal complement \( \x^\perp \).
\end{itemize}

\section{Matrices}

A \textbf{matrix} \( \A \in \R^{m \times n} \) is a rectangular array of real numbers with \( m \) rows and \( n \) columns, i.e.,
\begin{equation*}
    \A
    =
    \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix},
    \quad a_{ij} \in \R,
    \; \forall i \in \set{1, 2, \ldots, m},
    \; \forall j \in \set{1, 2, \ldots, n}
\end{equation*}

\subsection{Quadratic form}

The \textbf{quadratic form} associated with a matrix \( \A \in \R^{d \times d} \) is defined as
\begin{equation*}
    \func{q_\A}{x_1, x_2, \ldots, x_d}
    \triangleq
    \sum_{i = 1}^{d} \sum_{j = 1}^{d} a_{ij} x_i x_j
    =
    \qf{\x}{\A},
    \qquad
    \A
    =
    \bbrac{a_{ij}}_{i, j = 1}^{d}
    \in \R^{d \times d},
    \quad
    \x
    =
    \bbrac{x_i}_{i = 1}^{d}
    \in \R^d
\end{equation*}

Without loss of generality, we can assume \( \A \in \SD \) since
\begin{equation*}
    \qf{\x}{\A}
    =
    \qf{\x}{\bbrac{\half \pbrac{\A + \A^\top}}},
    \quad \forall \A \in \R^{d \times d}, \x \in \R^d
\end{equation*}

\subsubsection{Properties}

\begin{itemize}
    \item There may be multiple matrices associated with a given quadratic form; however, every quadratic form \( q_\A \) has a unique symmetric matrix associated with it, given by \( \half \pbrac{\A + \A^\top} \).
\end{itemize}

\subsection{Symmetric matrices}\label{sec:symmetric-matrices}

A matrix \( \A \in \R^{d \times d} \) is symmetric if \( \A = \A^\top \), i.e., \( a_{ij} = a_{ji}, \; \forall i, j \).

\paragraph{Properties}

\begin{itemize}
    \item \( \A, \B \in \SD \implies \A + \B \in \SD \).
        \quad \( \because {(\A + \B)}^\top = \A^\top + \B^\top = \A + \B \).

    \item \( \A \in \SD, c \in \R \implies c \A \in \SD \).
        \quad \( \because {(c \A)}^\top = c \A^\top = c \A \).

    \item \( \A \in \SD \implies \A^k \in \SD, \; k \in \N \).
        \quad \( \because {(\A^k)}^\top = {(\A^\top)}^k = \A^k \).

    \item \( \A \in \SD \implies \inv{\A} \in \SD, \text{ if } \A \text{ is invertible} \).
        \quad \( \because {(\inv{\A})}^\top = \inv{(\A^\top)} = \inv{\A} \).
\end{itemize}

\subsection{Symmetric positive semi-definite matrices}\label{sec:symmetric-positive-semi-definite-matrices}

A symmetric matrix \( \A \in \SD \) is \textbf{positive semi-definite} (PSD), denoted as \( \A \succeq \zero \), if
\begin{equation*}
    \qf{\x}{\A} \geq 0,
    \quad \forall \x \in \R^d
\end{equation*}

\subsection{Symmetric positive definite matrices}\label{sec:symmetric-positive-definite-matrices}

A symmetric matrix \( \A \in \SD \) is \textbf{positive definite} (PD), denoted as \( \A \succ \zero \), if
\begin{equation*}
    \qf{\x}{\A} > 0,
    \quad \forall \x \in \R^d \setminus \set{\zero}
\end{equation*}

\section{Eigenvalues and Eigenvectors}

\begin{equation*}
    \A \v = \lambda \v
\end{equation*}
where
\( \A \in \R^{d \times d} \), \quad
\( \lambda \in \R \longrightarrow \) Eigenvalue, \quad
\( \v \in \R^d \setminus \set{\zero} \longrightarrow \) corresponding Eigenvector.

The set of all eigenvalues of \( \A \) is called the \textbf{spectrum} of \( \A \).

\subsection{Properties}

\begin{itemize}
    \item \( \trace{\A} = \sum_{i = 1}^{d} a_{ii} = \sum_{i = 1}^{d} \lambda_i \).

    \item \( \determinant{\A} = \prod_{i = 1}^{d} \lambda_i \).

    \item \( \A \in \SD \implies \A \) has an orthonormal basis of eigenvectors in \( \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \A \in \SD \implies \operatorname{spec}(\A) \subseteq \R^d \) \, \psecref{sec:spectral-decomposition-theorem}

    \item \( \A \in \SD \implies \dotp{\v_i}{\v_j} = 0, \; \text{if } \lambda_i \neq \lambda_j \) \, \psecref{sec:spectral-decomposition-theorem}
\end{itemize}

\section{Spectral decomposition theorem}\label{sec:spectral-decomposition-theorem}

\begin{theorem}{Spectral decomposition theorem}{spectral-decomposition-theorem}
    For any \( \A \in \SD \), there exists an orthonormal basis of eigenvectors \( \set{\v_i}_{i = 1}^{d}, \v_i \in \R^d \setminus \set{\zero} \), with corresponding eigenvalues \( \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_d, \; \lambda_i \in \R \), such that
    \vspace{-0.5em}
    \begin{equation*}
        \A = \sum_{i = 1}^{d} \lambda_i \outp{\v_i}{\v_i}
        = \V \bfLambda \V^\top
        = \V \bfLambda \inv{\V}
    \end{equation*}

    \vspace{-0.5em}
    where \( \V =
        \begin{bmatrix}
            \v_1 & \v_2 & \cdots & \v_d
        \end{bmatrix}
    \in \R^{d \times d} \) is an orthogonal matrix (i.e., \( \dotp{\V}{\V} = \outp{\V}{\V} = \I \)) and \( \bfLambda = \diag{\lambda_1, \lambda_2, \ldots, \lambda_d} \in \R^{d \times d} \) is a diagonal matrix containing the eigenvalues of \( \A \).
\end{theorem}

\begin{proof}
    The eigenvalues of a real symmetric matrix are all real
    (Can be shown considering Hermitian matrices, Fundamental theorem of algebra, and properties of complex conjugates).

    The eigenvectors corresponding to distinct eigenvalues are orthogonal.
    This can be shown using the fact that for any two eigenvectors \( \v_i \) and \( \v_j \) corresponding to distinct eigenvalues \( \lambda_i \) and \( \lambda_j \), we have
    \begin{align*}
        \qf{\v_i}{\A}[\v_j]
        & =
        \dotp{\v_i}{\pbrac{\A \v_j}}
        =
        \lambda_j \dotp{\v_i}{\v_j},
        \qquad \text{Similarly, } \;
        \qf{\v_j}{\A}[\v_i]
        =
        \lambda_i \dotp{\v_j}{\v_i}
        \\
        \text{Now, }
        \qf{\v_i}{\A}[\v_j]
        & =
        \qf{\v_i}{\A^\top}[\v_j]
        =
        \dotp{\pbrac{\A \v_i}}{\v_j}
        =
        \dotp{\pbrac{\lambda_i \v_i}}{\v_j}
        =
        \lambda_i \dotp{\v_i}{\v_j}
        =
        \lambda_i \dotp{\v_j}{\v_i}
        =
        \qf{\v_j}{\A}[\v_i]
        \\
        \implies
        \lambda_j \dotp{\v_i}{\v_j}
        & =
        \lambda_i \dotp{\v_i}{\v_j}
        \implies
        (\lambda_j - \lambda_i) \dotp{\v_i}{\v_j}
        = 0
        \implies
        \dotp{\v_i}{\v_j} = 0,
        \; \because
        \lambda_i \neq \lambda_j,
        \;\;
        \therefore
        \v_i, \v_j \text{ are orthogonal.}
    \end{align*}

    For repeated eigenvalues, if any, we can always find a set of orthonormal eigenvectors using the Gram-Schmidt process.
    Thereby, we can always find \( d \) orthonormal eigenvectors of \( \A \), thus forming an orthonormal basis for \( \R^d \).
    This gives that \( \V \) is an orthogonal matrix \( \implies \dotp{\V}{\V} = \I \implies \inv{\V} = \V^\top \).

    Now, with \( \bfLambda = \diag{\lambda_1, \lambda_2, \ldots, \lambda_d} \) and \( \V = [\v_1, \v_2, \ldots, \v_d] \), we have
    \begin{equation*}
        \A \V
        =
        [\A \v_1, \A \v_2, \ldots, \A \v_d]
        =
        [\lambda_1 \v_1, \lambda_2 \v_2, \ldots, \lambda_d \v_d]
        =
        \V \bfLambda
        \implies
        \A
        =
        \V \bfLambda \inv{\V}
        =
        \V \bfLambda \V^\top
    \end{equation*}
\end{proof}

\begin{itemize}
    \item The identity matrix \( \I \in \R^{d \times d} \) has eigenvalues \( \lambda_i = 1, \; \forall i \) and the corresponding eigenvectors can be chosen as any orthonormal basis of \( \R^d \), commonly taken as the standard basis vectors \( \mathbf{e}_i \in \R^d \).

    \item \( \A \in \PSD \iff \A \in \SD, \text{ and } \lambda_i \geq 0, \; \forall i \).

    \item \( \A \in \PD \iff \A \in \SD, \text{ and } \lambda_i > 0, \; \forall i \).

    \item \( \A \in \PD, \; \kappa(\A) = 1 \iff \A = c \I, \; c > 0 \).
\end{itemize}

\section{Condition number}

The \textbf{condition number} of a matrix \( \A \in \SD \) is given by
\begin{equation*}
    \kappa(\A) = \frac{\abs{\lambda_{\max}(\A)}}{\abs{\lambda_{\min}(\A)}}
\end{equation*}
where \( \lambda_{\max}(\A) \) and \( \lambda_{\min}(\A) \) are the largest and smallest eigenvalues of \( \A \) respectively.

\section{Characteristic polynomial}

The \textbf{characteristic polynomial} of a matrix \( \A \in \CC^{d \times d} \) is defined as
\begin{equation*}
    p_{\A}(\lambda)
    =
    \determinant{\lambda \I - \A}
    =
    \sum_{i = 0}^{d} c_i \, \lambda^i,
    \quad c_i \in \CC,
    \; \forall i \in \set{0, 1, \ldots, d}
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( p_{\A}(\lambda) \) is a monic polynomial of degree \( d \) in \( \lambda \), i.e., \( c_d = 1 \).

    \item The roots of \( p_{\A}(\lambda) \) are the eigenvalues of \( \A \), \( \set{\lambda_i}_{i = 1}^{d} \).\\
        \( p_{\A}(\lambda) = 0 \iff \lambda \) is an eigenvalue of \( \A \).

    \item \( p_{\A}(\lambda) = \prod_{i = 1}^{d} (\lambda - \lambda_i) \), if \( \A \) is diagonalisable over \( \CC \).

    \item \( c_0 = {(-1)}^{d} \determinant{\A} = {(-1)}^{d} \prod_{i = 1}^{d} \lambda_i \).

    \item \( c_{d-1} = -\trace{\A} = -\sum_{i = 1}^{d} \lambda_i \).

    \item Two similar matrices have the same characteristic polynomial.\\
        \(
            \B \sim \A
            \iff
            \B = P \A \inv{P}
            \implies
            p_{\B}(\lambda) = p_{\A}(\lambda)
        \).

    \item (Cayley-Hamilton theorem~\psecref{sec:cayley-hamilton-theorem})
        Every square matrix satisfies its own characteristic equation.\\
        \(
            p_{\A}(\lambda)
            =
            \sum_{i = 0}^{d} c_i \, \lambda^i
            \implies
            p_{\A}(\A)
            =
            \sum_{i = 0}^{d} c_i \, \A^i
            =
            \zero
        \).

    \item \( \lambda, \, \v \) is an eigenpair of \( \A \implies \lambda^k, \, \v \) is an eigenpair of \( \A^k, \; k \in \N \).\\
        \(
            \A^k \, \v
            =
            \A^{k - 1} \pbrac{\A \v}
            =
            \A^{k - 1} \pbrac{\lambda \v}
            =
            \lambda \, \pbrac{\A^{k - 1} \, \v}
            =
            \lambda^2 \, \pbrac{\A^{k - 2} \, \v}
            =
            \cdots
            =
            \lambda^{k - 1} \pbrac{\A \v}
            =
            \lambda^k \, \v
        \)

    \item
        \(
            p_{f(\A)}(\lambda)
            =
            \prod_{i = 1}^{d} \pbrac{\lambda - f(\lambda_i)}
        \),
        where \( f \) is a polynomial function, i.e.,\\
        The eigenvalues of \( f(\A) \) are given by \( f(\lambda_i), \; i \in \set{1, 2, \ldots, d} \).

    \item
        \(
            p_{\A}(\lambda)
            =
            \determinant{\lambda \I - \A}
            =
            {(-1)}^{d} \determinant{\A - \lambda \I}
            =
            {(-1)}^{d} p_{-\A}(-\lambda)
        \)

    \item
        \(
            \displaystyle
            p_{\inv{\A}}(\lambda)
            =
            {(-1)}^{d} \frac{1}{\determinant{\A}} \, p_{\A}(\inv{\lambda})
        \),
        if \( \A \) is invertible.
\end{itemize}

\section{Minimal polynomial}

The \textbf{minimal polynomial} of a matrix \( \A \in \CC^{d \times d} \) is defined as the monic polynomial \( \mu_{\A}(\lambda) \) of least degree such that
\begin{equation*}
    \mu_{\A}(\A)
    =
    \zero,
    \qquad \text{where }
    \mu_{\A}(\lambda)
    =
    \sum_{i = 0}^{k} b_i \, \lambda^i,
    \quad b_i \in \CC,
    \; \forall i \in \set{0, 1, \ldots, k},
    \; b_k = 1
\end{equation*}

\subsection{Properties}

\begin{itemize}
    \item \( \mu_{\A}(\lambda) \) is unique for a given square matrix \( \A \).

    \item The roots of \( \mu_{\A}(\lambda) \) are exactly the distinct eigenvalues of \( \A \).
        The multiplicity of each root \( \lambda \) in \( \mu_{\A}(\lambda) \) equals the size of the largest Jordan block corresponding to \( \lambda \).
        In contrast, the characteristic polynomial \( p_{\A}(\lambda) \) includes all eigenvalues with their full algebraic multiplicity.

    \item Two similar matrices have the same minimal polynomial.\\
        \(
            \B \sim \A
            \iff
            \B = P \A \inv{P}
            \implies
            \mu_{\B}(\lambda) = \mu_{\A}(\lambda)
        \).

    \item Any \textbf{annihilating polynomial} of \( \A \) must be a multiple of \( \mu_{\A}(\lambda) \), i.e.,\\
        if \( f(\A) = \zero \), then \( f(\lambda) \) is a multiple of \( \mu_{\A}(\lambda) \).

    \item (Cayley-Hamilton theorem~\psecref{sec:cayley-hamilton-theorem})
        \( \mu_{\A}(\lambda) \) always divides \( p_{\A}(\lambda) \).
\end{itemize}

\section{Cayley-Hamilton theorem}\label{sec:cayley-hamilton-theorem}

\begin{theorem}{Cayley-Hamilton theorem}{}
    Every square matrix \( \A \in \CC^{d \times d} \) satisfies its own characteristic equation, i.e., \( p_{\A}(\A) = \zero \).
\end{theorem}

\section{Krylov subspace}

The \textbf{Krylov subspace} of order \( k \) generated by a matrix \( \A \in \R^{d \times d} \) and a vector \( \b \in \R^d \) is defined as
\begin{align*}
    \func{\calK_k}{\A, \b}
    & \triangleq
    \spanset{\A^i \, \b}_{i = 0}^{k-1}
    =
    \spanset{\b, \A \b, \A^2 \, \b, \ldots, \A^{k-1} \, \b}
    \\ & =
    \set{
        \sum_{i = 0}^{k-1} \alpha_i \, \A^i \, \b
        \given
        \alpha_i \in \R,
        \; \forall i \in \set{0, 1, \ldots, k-1}
    }
\end{align*}

\section{Gram-Schmidt process}

\subsection{Gram-Schmidt orthogonalisation}

The \textbf{Gram-Schmidt orthogonalisation} process takes a finite, linearly independent set of vectors \( \set{\v_i}_{i = 0}^{n - 1} \) and generates an orthogonal set of vectors \( \set{\u_i}_{i = 0}^{n - 1} \) that spans the same subspace.
\begin{equation*}
    \spanset{\v_i}_{i = 0}^{n - 1}
    =
    \spanset{\u_i}_{i = 0}^{n - 1}
    , \qquad \qquad
    \dotp{\u_i}{\u_j}
    = 0,
    \quad \forall i, j \in \set{0, 1, \ldots, n - 1},
    \; i \neq j
\end{equation*}

\begin{algorithm}[H]
    \caption{
        Gram-Schmidt orthogonalisation process
    }\label{alg:gram-schmidt-orthogonalisation}
    \SetAlgoLined{}
    \KwIn{
        A set of linearly independent vectors \( \set{\v_i}_{i = 0}^{n - 1} \subseteq \R^d, \; n \leq d \)
    }
    \KwOut{
        An orthogonal set of vectors \( \set{\u_i}_{i = 0}^{n - 1} \subseteq \R^d \) that spans the same subspace as \( \set{\v_i}_{i = 0}^{n - 1} \)
    }

    \( \displaystyle \u_0 \leftarrow \v_0 \)\;

    \( k \leftarrow 1 \)\;

    \While{\( k < n \)}{
        \( \displaystyle \u_k \leftarrow \v_k - \sum_{j = 0}^{k - 1} \frac{\dotp{\v_k}{\u_j}}{\dotp{\u_j}{\u_j}} \, \u_j \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \set{\u_i}_{i = 0}^{n - 1} \)}
\end{algorithm}

\begin{theorem}{}{}
    The vectors \( \set{\u_i}_{i = 0}^{n - 1} \) generated by the Gram-Schmidt orthogonalisation process
    \vspace{-0.5em}
    \begin{equation*}
        \u_0 = \v_0,
        \qquad
        \u_k
        =
        \v_k - \sum_{j = 0}^{k - 1} \frac{\dotp{\v_k}{\u_j}}{\dotp{\u_j}{\u_j}} \, \u_j,
        \quad k = 1, 2, \ldots, n - 1
    \end{equation*}

    \vspace{-0.5em}
    form an orthogonal set that spans the same subspace as \( \set{\v_i}_{i = 0}^{n - 1} \).
\end{theorem}

\begin{proof}
    By mathematical induction.

    \paragraph{Base case:}
    For \( k = 0 \), the statement holds trivially.
    \begin{equation*}
        \spanset{\u_0}
        =
        \spanset{\v_0}
    \end{equation*}

    \paragraph{Inductive step:}
    Assume the statement holds for some \( k = m - 1, \; m \leq n - 1 \), i.e.,
    \begin{equation*}
        \spanset{\u_i}_{i = 0}^{m - 1}
        =
        \spanset{\v_i}_{i = 0}^{m - 1},
        \qquad
        \dotp{\u_i}{\u_j}
        = 0,
        \; \forall i, j \in \set{0, 1, \ldots, m - 1},
        \; i \neq j
    \end{equation*}
    We need to show that the statement holds for \( k = m \), i.e.,
    \begin{equation*}
        \spanset{\u_i}_{i = 0}^{m}
        =
        \spanset{\v_i}_{i = 0}^{m},
        \qquad
        \dotp{\u_m}{\u_i}
        = 0,
        \; \forall i \in \set{0, 1, \ldots, m - 1}
    \end{equation*}

    \begin{align*}
        \implies
        \dotp{\u_m}{\u_i}
        & =
        \dotp{\bbrac{\v_m - \sum_{j = 0}^{m - 1} \frac{\dotp{\v_m}{\u_j}}{\dotp{\u_j}{\u_j}} \, \u_j}}{\u_i}
        =
        \dotp{\v_m}{\u_i} - \sum_{j = 0}^{m - 1} \frac{\dotp{\v_m}{\u_j}}{\dotp{\u_j}{\u_j}} \dotp{\u_j}{\u_i}
        \\ & =
        \dotp{\v_m}{\u_i} - \frac{\dotp{\v_m}{\u_i}}{\cancel{\dotp{\u_i}{\u_i}}} \cancel{\dotp{\u_i}{\u_i}}
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{align*}

    \begin{align*}
        \text{Now, }
        \spanset{\u_i}_{i = 0}^{m}
        & =
        \spanset{\u_i}_{i = 0}^{m - 1} \cup \spanset{\u_m}
        \\ & =
        \spanset{\v_i}_{i = 0}^{m - 1} \cup \bbrac{\spanset{\u_i}_{i = 0}^{m - 1} \cup \spanset{\v_m}}
        \\ & =
        \spanset{\v_i}_{i = 0}^{m - 1} \cup \spanset{\v_m}
        =
        \spanset{\v_i}_{i = 0}^{m}
    \end{align*}

    Hence, the statement holds for all \( k \in \set{0, 1, \ldots, n - 1} \).
\end{proof}

\subsection{Gram-Schmidt orthonormalisation}

The \textbf{Gram-Schmidt orthonormalisation} process, in addition, normalises each of the vectors generated, and thereby generates an orthonormal set of vectors \( \set{\e_i}_{i = 0}^{n - 1} \) that spans the same subspace.
\begin{equation*}
    \spanset{\v_i}_{i = 0}^{n - 1}
    =
    \spanset{\e_i}_{i = 0}^{n - 1}
    , \qquad \qquad
    \dotp{\e_i}{\e_j}
    =
    \begin{cases}
        1, & i = j \\
        0, & i \neq j
    \end{cases},
    \quad \forall i, j \in \set{0, 1, \ldots, n - 1}
\end{equation*}

\begin{algorithm}[H]
    \caption{
        Gram-Schmidt orthonormalisation process
    }\label{alg:gram-schmidt-orthonormalisation}
    \SetAlgoLined{}
    \KwIn{
        A set of linearly independent vectors \( \set{\v_i}_{i = 0}^{n - 1} \subseteq \R^d, \; n \leq d \)
    }
    \KwOut{
        An orthonormal set of vectors \( \set{\e_i}_{i = 0}^{n - 1} \subseteq \R^d \) that spans the same subspace as \( \set{\v_i}_{i = 0}^{n - 1} \)
    }

    \( \displaystyle \e_0 \leftarrow \frac{\v_0}{\norm{\v_0}_2} \)\;

    \( k \leftarrow 1 \)\;

    \While{\( k < n \)}{
        \( \displaystyle \e_k \leftarrow \v_k - \sum_{j = 0}^{k - 1} \dotp{\v_k}{\e_j} \, \e_j \)\;

        \( \displaystyle \e_k \leftarrow \frac{\e_k}{\norm{\e_k}_2} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \set{\e_i}_{i = 0}^{n - 1} \)}
\end{algorithm}

\begin{theorem}{}{}
    The vectors \( \set{\e_i}_{i = 0}^{n - 1} \) generated by the Gram-Schmidt orthonormalisation process
    \vspace{-0.5em}
    \begin{equation*}
        \e_0
        =
        \frac{\v_0}{\norm{\v_0}_2},
        \qquad
        \e_k
        =
        \frac
        {\v_k - \sum_{j = 0}^{k - 1} \dotp{\v_k}{\e_j} \, \e_j}
        {\norm{
                \v_k - \sum_{j = 0}^{k - 1} \dotp{\v_k}{\e_j} \, \e_j
        }_2},
        \quad k = 1, 2, \ldots, n - 1
    \end{equation*}

    \vspace{-0.5em}
    form an orthonormal set that spans the same subspace as \( \set{\v_i}_{i = 0}^{n - 1} \).
\end{theorem}

\begin{proof}
    By mathematical induction.

    \paragraph{Base case:}
    For \( k = 0 \), the statement holds trivially.
    \begin{equation*}
        \spanset{\e_0}
        =
        \spanset{\v_0},
        \qquad
        \norm{\e_0}_2
        = 1
    \end{equation*}

    \paragraph{Inductive step:}
    Assume the statement holds for some \( k = m - 1, \; m \leq n - 1 \), i.e.,
    \begin{equation*}
        \spanset{\e_i}_{i = 0}^{m - 1}
        =
        \spanset{\v_i}_{i = 0}^{m - 1},
        \qquad
        \dotp{\e_i}{\e_j}
        =
        \begin{cases}
            1, & i = j \\
            0, & i \neq j
        \end{cases},
        \quad \forall i, j \in \set{0, 1, \ldots, m - 1}
    \end{equation*}
    We need to show that the statement holds for \( k = m \), i.e.,
    \begin{equation*}
        \spanset{\e_i}_{i = 0}^{m}
        =
        \spanset{\v_i}_{i = 0}^{m},
        \qquad
        \dotp{\e_m}{\e_i}
        =
        \begin{cases}
            1, & i = m \\
            0, & i \neq m
        \end{cases},
        \quad \forall i \in \set{0, 1, \ldots, m}
    \end{equation*}

    \begin{align*}
        \because
        \dotp{\bbrac{\v_m - \sum_{j = 0}^{m - 1} \dotp{\v_m}{\e_j} \, \e_j}}{\e_i}
        & =
        \dotp{\v_m}{\e_i} - \sum_{j = 0}^{m - 1} \dotp{\v_m}{\e_j} \dotp{\e_j}{\e_i}
        \\ & =
        \dotp{\v_m}{\e_i} - \dotp{\v_m}{\e_i}
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
        \\
        \implies
        \dotp{\e_m}{\e_i}
        & = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{align*}
    Also, \( \dotp{\e_m}{\e_m} = \norm{\e_m}_2^2 = 1 \) by construction.

    \begin{align*}
        \text{Now, }
        \spanset{\e_i}_{i = 0}^{m}
        & =
        \spanset{\e_i}_{i = 0}^{m - 1} \cup \spanset{\e_m}
        \\ & =
        \spanset{\v_i}_{i = 0}^{m - 1} \cup \bbrac{\spanset{\e_i}_{i = 0}^{m - 1} \cup \spanset{\v_m}}
        \\ & =
        \spanset{\v_i}_{i = 0}^{m - 1} \cup \spanset{\v_m}
        =
        \spanset{\v_i}_{i = 0}^{m}
    \end{align*}

    Hence, the statement holds for all \( k \in \set{0, 1, \ldots, n - 1} \).
\end{proof}

\section{Matrix inversion lemma}\label{sec:matrix-inversion-lemma}

\begin{theorem}{Matrix inversion lemma / Sherman-Morrison-Woodbury formula / Woodbury matrix identity~\citep{wiki}}{matrix-inversion-lemma}
    For any invertible matrices \( \A \in \R^{m \times m}, \C \in \R^{n \times n} \) and matrices \( \B \in \R^{m \times n}, \D \in \R^{n \times m} \), such that \( \A + \B \C \D \) is invertible, we have
    \vspace{-0.5em}
    \begin{equation}\label{eq:matrix-inversion-lemma}
        \inv{\bbrac{\A + \B \C \D}}
        =
        \inv{\A}
        -
        \inv{\A} \B
        \inv{\bbrac{\inv{\C} + \D \inv{\A} \B}} \D
        \inv{\A}
    \end{equation}
\end{theorem}

\begin{proof}
    Put \( \U = \inv{\A} \B, \; \V = \C \D \) in the reduced case~\pcorref{cor:reduced-matrix-inversion-lemma} to recover~\peqref{eq:matrix-inversion-lemma}.
    \begin{align*}
        \implies
        \inv{\bbrac{\I + \inv{\A} \B \C \D}}
        & =
        \I
        -
        \inv{\A} \B \inv{\bbrac{\I + \C \D \inv{\A} \B}} \C \D
        \\
        \implies
        \inv{\bbrac{\I + \inv{\A} \B \C \D}} \inv{\A}
        & =
        \I \inv{\A}
        -
        \inv{\A} \B \inv{\bbrac{\I + \C \D \inv{\A} \B}} \C \D \inv{\A}
        \\
        \implies
        \inv{\bbrac{\A \pbrac{\I + \inv{\A} \B \C \D}}}
        & =
        \inv{\A}
        -
        \inv{\A} \B \inv{\bbrac{\inv{\C} \pbrac{\I + \C \D \inv{\A} \B}}} \D \inv{\A}
        \\
        \implies
        \inv{\bbrac{\A + \B \C \D}}
        & =
        \inv{\A}
        -
        \inv{\A} \B \inv{\bbrac{\inv{\C} + \D \inv{\A} \B}} \D \inv{\A}
    \end{align*}
\end{proof}

\begin{corollary}{Reduced matrix inversion lemma~\cite{wiki}}{reduced-matrix-inversion-lemma}
    For any matrices \( \U \in \R^{m \times n}, \V \in \R^{n \times m} \), such that \( \I + \U \V \) is invertible, we have
    \vspace{-0.5em}
    \begin{equation*}
        \inv{\bbrac{\I + \U \V}}
        =
        \I
        -
        \U \inv{\bbrac{\I + \V \U}} \V
    \end{equation*}
\end{corollary}

\begin{proof}
    We can see that this is a special case of the matrix inversion lemma~\psecref{sec:matrix-inversion-lemma} by setting\\
    \( \A = \I, \; \B = \U, \; \C = \I, \; \D = \V \) in~\peqref{eq:matrix-inversion-lemma}.
    We prove this as follows.
    \begin{align*}
        \because
        \I
        & =
        \inv{\bbrac{\I + \E}} \bbrac{\I + \E}
        =
        \inv{\bbrac{\I + \E}} + \inv{\bbrac{\I + \E}} \E
        =
        \inv{\bbrac{\I + \E}} + \E \inv{\bbrac{\I + \E}}
        \\
        \implies
        &
        \inv{\bbrac{\I + \E}}
        =
        \I
        -
        \inv{\bbrac{\I + \E}} \E
        =
        \I
        -
        \E \inv{\bbrac{\I + \E}}
    \end{align*}
    \begin{align*}
        \because
        \V + \V \U \V
        & =
        \bbrac{\I + \V \U} \V
        =
        \V \bbrac{\I + \U \V}
        \\
        \implies
        \V \inv{\bbrac{\I + \U \V}}
        & =
        \inv{\bbrac{\I + \V \U}} \V
        \tag{push-through identity}
    \end{align*}
    \begin{equation*}
        \implies
        \inv{\bbrac{\I + \U \V}}
        =
        \I
        -
        \U \V \inv{\bbrac{\I + \U \V}}
        =
        \I
        -
        \U \inv{\bbrac{\I + \V \U}} \V
    \end{equation*}
\end{proof}

\begin{corollary}{Inverse of matrix sum~\citep{wiki}}{}
    If \( \A, \B \in \R^{d \times d} \) are invertible matrices such that \( \A + \B \) is invertible, then
    \vspace{-0.5em}
    \begin{align*}
        \inv{\bbrac{\A + \B}}
        & =
        \inv{\A}
        -
        \inv{\A}
        \inv{\bbrac{\inv{\B} + \inv{\A}}}
        \inv{\A}
        \\ & =
        \inv{\A}
        -
        \inv{\A}
        \inv{\bbrac{\A \inv{\B} + \I}}
        \\ & =
        \inv{\A}
        -
        \inv{\bbrac{\A + \A \inv{\B} \A}}
        \tag{Hua's identity}
    \end{align*}
\end{corollary}

\begin{proof}
    Set \( m = n \) and \( \B = \I, \; \D = \I \) in the matrix inversion lemma~\psecref{sec:matrix-inversion-lemma}~\peqref{eq:matrix-inversion-lemma}.
\end{proof}

\begin{corollary}{Inverse of matrix difference~\citep{wiki}}{}
    If \( \A, \; \A - \B \in \R^{d \times d} \) are invertible matrices, then
    \vspace{-0.5em}
    \begin{equation}\label{eq:inverse-of-matrix-difference}
        \inv{\bbrac{\A - \B}}
        =
        \inv{\A}
        +
        \inv{\A} \B
        \inv{\bbrac{\A - \B}}
    \end{equation}

    Further, if the spectral radius \( \rho(\inv{\A} \B) < 1 \), then
    \vspace{-0.5em}
    \begin{equation*}
        \inv{\bbrac{\A - \B}}
        =
        \sum_{i = 0}^{\infty} \pbrac{\inv{\A} \B}^i \inv{\A}
    \end{equation*}
\end{corollary}

\begin{proof}
    We can see this holds by setting \( m = n \) and \( \B = -\I, \; \D = \I \) in the matrix inversion lemma~\psecref{sec:matrix-inversion-lemma}~\peqref{eq:matrix-inversion-lemma}, but that requires invertibility of \( \B \) in~\peqref{eq:inverse-of-matrix-difference} as well.
    \begin{align*}
        \because
        \bbrac{\I - \inv{\A} \B}
        & =
        \inv{\A} \bbrac{\A - \B}
        \implies
        \bbrac{\I - \inv{\A} \B} \inv{\bbrac{\A - \B}}
        =
        \inv{\A}
        \\
        \implies
        &
        \inv{\bbrac{\A - \B}}
        -
        \inv{\A} \B
        \inv{\bbrac{\A - \B}}
        =
        \inv{\A}
    \end{align*}

    Also, if \( \rho(\inv{\A} \B) < 1 \), then we have
    \begin{equation*}
        \inv{\bbrac{\A - \B}}
        =
        \inv{\bbrac{\I - \inv{\A} \B}} \inv{\A},
        \qquad
        \inv{\bbrac{\I - \inv{\A} \B}}
        =
        \sum_{i = 0}^{\infty} \pbrac{\inv{\A} \B}^i
    \end{equation*}
\end{proof}

\subsection{Sherman-Morrison formula}\label{sec:sherman-morrison-formula}

\begin{theorem}{Sherman-Morrison formula~\citep{wiki}}{sherman-morrison-formula}
    For any invertible matrix \( \A \in \R^{d \times d} \) and vectors \( \u, \v \in \R^d \), \( \A + \outp{\u}{\v} \) is invertible iff \( 1 + \qf{\v}{\inv{\A}}[\u] \neq 0 \), and we have
    \vspace{-0.5em}
    \begin{equation*}
        \inv{\bbrac{\A + \outp{\u}{\v}}}
        =
        \inv{\A}
        -
        \frac
        {\inv{\A} \outp{\u}{\v} \inv{\A}}
        {1 + \qf{\v}{\inv{\A}}[\u]}
    \end{equation*}
\end{theorem}

\begin{proof}
    We can see that this is a special case of the matrix inversion lemma~\psecref{sec:matrix-inversion-lemma} for \( n = 1 \), i.e., when \( \B, \D \) are vectors, by setting \( \B = \u, \; \C = 1, \; \D = {\v}^\top \) in~\peqref{eq:matrix-inversion-lemma}.
\end{proof}

\begin{corollary}{Scalar case of Sherman-Morrison formula~\citep{wiki}}{}
    \begin{equation*}
        \frac{1}{1 + u v}
        =
        1
        -
        \frac{u v}{1 + v u},
        \quad \forall u, v \in \R, \; u v \neq -1
    \end{equation*}
\end{corollary}

\begin{proof}
    We can see that this is a special case of the Sherman-Morrison formula~\psecref{sec:sherman-morrison-formula} for \( d = 1 \), i.e., by setting \( \A = 1, \; \u = u, \; \v = v \).
\end{proof}

\section{Matrix determinant lemma}\label{sec:matrix-determinant-lemma}

\begin{theorem}{Matrix determinant lemma~\citep{wiki}}{matrix-determinant-lemma}
    For any invertible matrix \( \A \in \R^{m \times m} \) and matrices \( \B \in \R^{m \times n}, \C \in \R^{n \times m} \), we have
    \vspace{-0.5em}
    \begin{equation}\label{eq:matrix-determinant-lemma}
        \determinant{\A + \B \C}
        =
        \determinant{\I + \C \inv{\A} \B}
        \; \determinant{\A}
    \end{equation}
\end{theorem}

\begin{corollary}{{}~\citep{wiki}}{}
    For any invertible matrices \( \A \in \R^{m \times m}, \C \in \R^{n \times n} \) and matrices \( \B \in \R^{m \times n}, \D \in \R^{n \times m} \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \determinant{\A + \B \C \D}
        =
        \determinant{\A}
        \; \determinant{\C}
        \; \determinant{\inv{\C} + \D \inv{\A} \B}
    \end{equation*}
\end{corollary}

\begin{corollary}{Sylvester's determinant theorem / Weinstein-Aronszajn identity~\citep{wiki}}{}
    For any matrices \( \A \in \R^{m \times n}, \B \in \R^{n \times m} \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \determinant{\I + \A \B}
        =
        \determinant{\I + \B \A}
    \end{equation*}
\end{corollary}

\begin{proof}
    Set \( \A = \I \) in the matrix determinant lemma~\psecref{sec:matrix-determinant-lemma}~\peqref{eq:matrix-determinant-lemma}.
\end{proof}

\chapter{Inequalities}

\section{AM-GM inequality}

\begin{theorem}{AM-GM inequality}{}
    For any non-negative real numbers \( a_1, a_2, \ldots, a_n \geq 0 \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \frac{1}{n} \sum_{i = 1}^{n} a_i
        \geq
        \pbrac{\prod_{i = 1}^{n} a_i}^{\frac{1}{n}}
        \qquad \text{i.e.,} \quad
        \overbrace{
            \pbrac{\frac{a_1 + a_2 + \cdots + a_n}{n}}
        }^{\text{Arithmetic Mean (AM)}}
        \geq
        \overbrace{
            \sqrt[n]{a_1 a_2 \ldots a_n}
        }^{\text{Geometric Mean (GM)}}
    \end{equation*}

    \vspace{-0.5em}
    with equality iff \( a_1 = a_2 = \cdots = a_n \).
\end{theorem}

\begin{proof}
    Can be shown by mathematical induction.
    \( n = 1 \) holds trivially.

    For \( n = 2 \), we have
    \begin{equation*}
        \frac{a_1 + a_2}{2}
        \geq
        \sqrt{a_1 a_2}
        \iff
        \pbrac{a_1 - a_2}^2 \geq 0
    \end{equation*}
    The equality holds iff \( a_1 = a_2 \).

    \paragraph{P\'olya's proof:}
    Consider \( f: \R \to \R, \; f(x) = e^{x - 1} - x \). Then we have \( f'(x) = e^{x - 1} - 1 \) and \( f''(x) = e^{x - 1} > 0, \; \forall x \in \R \).
    Also, \( f(1) = 0, \; f'(1) = 0, \; f''(x) > 0, \; \forall x \in \R \), i.e., \( f(x) \) is strictly convex and has a global minimum at \( x = 1 \implies f(x) \geq 0, \; \forall x \in \R \implies \boxed{x \leq e^{x - 1}, \; \forall x \in \R} \), with equality iff \( x = 1 \).
    Let \( \bar{a} \triangleq \frac{1}{n} \sum_{i = 1}^{n} a_i \) denote the arithmetic mean of \( \set{a_i}_{i = 1}^{n} \).
    Then, we have
    \begin{align*}
        \implies
        \frac{1}{\bar{a}^n} \prod_{i = 1}^{n} a_i
        & =
        \prod_{i = 1}^{n} \pbrac{\frac{a_i}{\bar{a}}}
        \leq
        \prod_{i = 1}^{n} \exp{\pbrac{\frac{a_i}{\bar{a}} - 1}}
        =
        \exp{\pbrac{\sum_{i = 1}^{n} \frac{a_i}{\bar{a}} - n}}
        =
        \exp{\pbrac{\frac{n \cancel{\bar{a}}}{\cancel{\bar{a}}} - n}}
        =
        \exp{(0)}
        =
        1
        \\ &
        \implies
        \prod_{i = 1}^{n} a_i
        \leq
        \bar{a}^n
        \implies
        \pbrac{\prod_{i = 1}^{n} a_i}^{\frac{1}{n}}
        \leq
        \bar{a}
        =
        \frac{1}{n} \sum_{i = 1}^{n} a_i
    \end{align*}
    with equality iff \( \displaystyle \pbrac{\frac{a_i}{\bar{a}}} = 1, \; \forall i \in \set{1, 2, \ldots, n} \), i.e., \( a_1 = a_2 = \cdots = a_n \).
\end{proof}

\section{Cauchy-Schwarz inequality}\label{sec:cauchy-schwarz-inequality}

\begin{theorem}{Cauchy-Schwarz inequality / Cauchy-Bunyakovsky-Schwarz inequality}{}
    For any vectors \( \x, \y \in \R^d \), we have
    \vspace{-1em}
    \begin{equation*}
        \abs{\dotp{\x}{\y}} \leq \norm{\x}_2 \; \norm{\y}_2
    \end{equation*}

    \vspace{-0.5em}
    with equality iff \( \x \) and \( \y \) are linearly dependent, i.e., \( \exists c \in \R \) such that \( \x = c \y \) or \( \y = c \x \).
\end{theorem}

\begin{proof}
    Follows in the \hyperlink{line-point-minimisation}{problem} of minimising the distance between a point and a line in \( \R^d \).
\end{proof}

\begin{theorem}{Lagrange's identity~\citep{wiki}}{}
    For any vectors \( \x = \bbrac{x_1, x_2, \ldots, x_d}^\top, \; \y = \bbrac{y_1, y_2, \ldots, y_d}^\top \in \R^d \), we have
    \vspace{-0.5em}
    \begin{equation}\label{eq:lagranges-identity}
        \underbrace{
            \pbrac{\sum_{i = 1}^{d} x_i^2}
        }_{\norm{\x}_2^2}
        \underbrace{
            \pbrac{\sum_{i = 1}^{d} y_i^2}
        }_{\norm{\y}_2^2}
        -
        \underbrace{
            \pbrac{\sum_{i = 1}^{d} x_i y_i}^2
        }_{\pbrac{\dotp{\x}{\y}}^2}
        =
        \half
        \sum_{i = 1}^{d}
        \sum_{j = 1}^{d}
        \pbrac{x_i y_j - x_j y_i}^2
    \end{equation}

    \vspace{-1.5em}
\end{theorem}

\begin{proof}
    \begin{align*}
        &
        \half
        \sum_{i = 1}^{d}
        \sum_{j = 1}^{d}
        \pbrac{x_i y_j - x_j y_i}^2
        =
        \half
        \sum_{i = 1}^{d}
        \sum_{j = 1}^{d}
        \pbrac{x_i^2 y_j^2 + x_j^2 y_i^2 - 2 x_i y_i x_j y_j}
        \\ & =
        \half
        \bbrac{
            \sum_{i = 1}^{d} x_i^2
            \sum_{j = 1}^{d} y_j^2
            +
            \sum_{j = 1}^{d} x_j^2
            \sum_{i = 1}^{d} y_i^2
            -
            2
            \pbrac{\sum_{i = 1}^{d} x_i y_i}
            \pbrac{\sum_{j = 1}^{d} x_j y_j}
        }
        =
        \pbrac{\sum_{i = 1}^{d} x_i^2}
        \pbrac{\sum_{i = 1}^{d} y_i^2}
        -
        \pbrac{\sum_{i = 1}^{d} x_i y_i}^2
    \end{align*}
\end{proof}

Note that the right-hand side of Lagrange's identity~\peqref{eq:lagranges-identity} is always non-negative, which provides an alternative proof of the Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality}.

\section{H{\"o}lder's inequality}

\begin{theorem}{H{\"o}lder's inequality~\citep{wiki}}{}
    For any vectors \( \x, \y \in \R^d \) and for any H{\"o}lder conjugates \( p, q > 1 \) such that \( \displaystyle \frac{1}{p} + \frac{1}{q} = 1 \), we have
    \vspace{-1em}
    \begin{equation*}
        \abs{\dotp{\x}{\y}} \leq \norm{\x}_p \; \norm{\y}_q
    \end{equation*}

    \vspace{-0.5em}
    with equality iff \( \exists c > 0 \) such that \( \abs{x_i}^p = c \abs{y_i}^q, \; \forall i = 1, 2, \ldots, d \).
\end{theorem}

Note that the Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality} is a special case of H{\"o}lder's inequality for \( p = q = 2 \).

\section{Kantorovich inequality}\label{sec:kantorovich-inequality}

\begin{theorem}{Kantorovich inequality~\citep{mathworld}}{}
    Let \( p_i \geq 0, \; 0 < x_{\min} \leq x_i \leq x_{\max}, \; \forall i \in \set{1, 2, \ldots, n} \), then we have
    \vspace{-0.5em}
    \begin{equation*}
        \pbrac{\sum_{i = 1}^{n} p_i x_i} \pbrac{\sum_{i = 1}^{n} \frac{p_i}{x_i}}
        \leq
        \frac{{(x_{\max} + x_{\min})}^2}{4 x_{\max} x_{\min}}
        \pbrac{\sum_{i = 1}^{n} p_i}^2
    \end{equation*}

    \vspace{-0.5em}
    with equality iff \( x_i \in \set{x_{\min}, x_{\max}}, \; \forall i \in \set{1, 2, \ldots, n} \).
\end{theorem}

\begin{corollary}{Kantorovich inequality for real symmetric positive definite matrices~\citep{Luenberger1984}}{}
    For any real symmetric positive-definite matrix \( \Q \in \PD \), with smallest eigenvalue \( \lambda_{\min} \) and largest eigenvalue \( \lambda_{\max} \), and for any \( \x \in \R^d \setminus \set{\zero} \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}

    \vspace{-0.5em}
    with equality iff \( \x \) is an eigenvector of \( \Q \) corresponding to either \( \lambda_{\min} \) or \( \lambda_{\max} \).
\end{corollary}

\begin{proof}
    Let the matrix \( \Q \) have eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_d \), with corresponding orthonormal eigenvectors \( \v_1, \v_2, \ldots, \v_d \).
    Since the eigenvectors form an orthonormal basis for \( \R^d \), any vector \( \x \in \R^d \) can be expressed as a linear combination of these eigenvectors as \( \x = \sum_{i = 1}^{d} c_i \v_i \), where each coefficient \( c_i \) is the projection of \( \x \) onto the eigenvector \( \v_i \), given by \( c_i = \dotp{\v_i}{\x} \).
    Now,
    \begin{align*}
        \qf{\x}{\Q}
        & =
        \pbrac{\sum_{i = 1}^{d} c_i \v_i}^\top \Q \pbrac{\sum_{j = 1}^{d} c_j \v_j}
        =
        \sum_{i = 1}^{d} \sum_{j = 1}^{d} c_i c_j \qf{\v_i}{\Q}[\v_j]
        \\ & =
        \sum_{i = 1}^{d} c_i^2 \qf{\v_i}{\Q}
        +
        \sum_{i \neq j} c_i c_j \qf{\v_i}{\Q}[\v_j]
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i \underbrace{ \dotp{\v_i}{\v_i} }_{= 1}
        +
        \cancel{ \sum_{i \neq j} c_i c_j \lambda_j \underbrace{ \dotp{\v_i}{\v_j} }_{= 0} }
        =
        \sum_{i = 1}^{d} c_i^2 \lambda_i
        \\ &
        \text{Similarly, }
        \quad
        \qf{\x}{\Qinv}
        =
        \sum_{i = 1}^{d} c_i^2 \frac{1}{\lambda_i},
        \qquad
        \dotp{\x}{\x}
        =
        \qf{\x}{\I}
        =
        \sum_{i = 1}^{d} c_i^2 (1)
        =
        \sum_{i = 1}^{d} c_i^2
    \end{align*}
    Now, let \( p_i = c_i^2 \) and \( x_i = \lambda_i \) in Kantorovich inequality, we have
    \begin{equation*}
        \underbrace{ \pbrac{\sum_{i = 1}^{d} c_i^2 \lambda_i} }_{\qf{\x}{\Q}}
        \underbrace{ \pbrac{\sum_{i = 1}^{d} \frac{c_i^2}{\lambda_i}} }_{\qf{\x}{\Qinv}}
        \leq
        \frac{{(\lambda_{\max} + \lambda_{\min})}^2}{4 \lambda_{\max} \lambda_{\min}} \underbrace{ \pbrac{\sum_{i = 1}^{d} c_i^2}^2 }_{{(\dotp{\x}{\x})}^2}
        \implies
        \frac{{(\dotp{\x}{\x})}^2}{(\qf{\x}{\Q})(\qf{\x}{\Qinv})}
        \geq
        \frac{4 \lambda_{\max} \lambda_{\min}}{{(\lambda_{\max} + \lambda_{\min})}^2}
    \end{equation*}
\end{proof}

\section{Polyak-\L{}ojasiewicz (PL) inequality}

\begin{definition}{Polyak-\L{}ojasiewicz (PL) inequality~\citep{wiki}}{}
    A function \( f: \calD \subseteq \R^n \to \R \) on a convex domain \( \calD \) satisfies the \textbf{Polyak-\L{}ojasiewicz (PL) inequality} with parameter \( \mu > 0 \) if
    \vspace{-0.5em}
    \begin{equation*}
        \half \norm[\Big]{\grad{f}{\x}}^2 \geq \mu \pbrac{\func{f}{\x} - f^\ast}
        , \quad \forall \x \in \calD
    \end{equation*}

    \vspace{-0.5em}
    where \( f^\ast = \inf_{\x \in \calD} \func{f}{\x} \).
    Such an \( f \) is said to be \( \mu \)-\textbf{PL} on \( \calD \).
\end{definition}

\section{Jensen's inequality}

\begin{theorem}{Jensen's inequality~\citep{wiki}}{}
    For any convex function \( f: \R^d \to \R \), and for any set of points \( \set{\x_i}_{i = 1}^n, \; \x_i \in \R^d \) and weights \( \set{\alpha_i}_{i = 1}^n, \; \alpha_i \geq 0 \) such that \( \sum_{i = 1}^{n} \alpha_i = 1 \), we have
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\sum_{i = 1}^{n} \alpha_i \x_i}
        \leq
        \sum_{i = 1}^{n} \alpha_i \func{f}{\x_i}
    \end{equation*}

    \vspace{-0.5em}
    with equality iff \( \x_1 = \x_2 = \cdots = \x_n \) or \( f \) is linear on the convex hull of \( \set{\x_i}_{i = 1}^n \).
\end{theorem}

\subsection{Convex hull}

The \textbf{convex hull} of a set of points \( \set{\x_i}_{i = 1}^n, \; \x_i \in \R^d \) is the smallest convex set that contains all the points.
It can be defined as the set of all convex combinations of the points, i.e.,
\begin{equation*}
    \func{\operatorname{conv}}{\set{\x_i}_{i = 1}^n}
    =
    \set{
        \sum_{i = 1}^{n} \alpha_i \x_i
        \given
        \alpha_i \geq 0,
        \; \sum_{i = 1}^{n} \alpha_i = 1
    }
\end{equation*}
