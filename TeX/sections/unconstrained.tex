\chapter{Unconstrained optimisation}

\section{Optimisation methods}

\subsection{First-order methods}

Assume \( f \in \mathcal{C}^{1} \), and use \( f(\mathbf{x} + \mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^{\top} \mathbf{p} + o(\| \mathbf{p} \|) \).

\subsection{Second-order methods}

Assume \( f \in \mathcal{C}^{2} \), and use \( f(\mathbf{x} + \mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^{\top} \mathbf{p} + \frac{1}{2} \mathbf{p}^{\top} \nabla^{2} f(\mathbf{x}) \mathbf{p} + o(\| \mathbf{p} \|^2) \).

\subsection{Global minimum}

The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{global minimum} of the function \( f: \mathbb{R}^d \to \mathbb{R} \) if
\begin{equation*}
    f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in \mathbb{R}^d
\end{equation*}

\subsection{Local minimum}

The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{local minimum} of the function \( f: \mathbb{R}^d \to \mathbb{R} \) if there exists a \( \delta > 0 \) such that for all \( \mathbf{x} \) in the \( \delta \)-neighborhood of \( \mathbf{x}^* \), we have \( f(\mathbf{x}^*) \leq f(\mathbf{x}) \), i.e.,
\begin{equation*}
    f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*)
\end{equation*}

\section{Necessary and sufficient conditions}

\subsection{First-order necessary condition for a local minimum}

If \( f \in \mathcal{C}^{1} \) and \( \mathbf{x}^* \) is a local minimum of \( f \), then \( \nabla f(\mathbf{x}^*) = \mathbf{0} \).

\subsection{Second-order necessary condition for a local minimum}

If \( f \in \mathcal{C}^{2} \) and \( \mathbf{x}^* \) is a local minimum of \( f \), then \( \nabla f(\mathbf{x}^*) = \mathbf{0}, \; \nabla^2 f(\mathbf{x}^*) \succeq 0 \).

\subsection{Second-order sufficient condition for a strict local minimum}

If \( f \in \mathcal{C}^{2} \), \( \nabla f(\mathbf{x}^*) = \mathbf{0} \) and \( \nabla^2 f(\mathbf{x}^*) \succ 0 \), then \( \mathbf{x}^* \) is a strict local minimum of \( f \).

\subsection{First-order sufficient condition for a local minimum under convexity}

If \( f \in \mathcal{C}^1 \) is a convex function and \( \nabla f(\mathbf{x}^*) = \mathbf{0} \), then \( \mathbf{x}^* \) is a global minimum of \( f \).

\subsection{Convex functions}

A function $f: \mathbb{R}^n \to \mathbb{R}$ is \textbf{convex} if
\begin{equation*}
    f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y})
    \leq
    \lambda f(\mathbf{x}) + (1 - \lambda) f(\mathbf{y}),
    \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n, \; \lambda \in [0, 1]
\end{equation*}

\paragraph{First-order condition for convexity:}
If \( f \in \mathcal{C}^1 \), then \( f \) is convex iff
\begin{equation*}
    f(\mathbf{y})
    \geq
    f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}),
    \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n
\end{equation*}

\paragraph{Second-order condition for convexity:}
If \( f \in \mathcal{C}^2 \), then \( f \) is convex if
\begin{equation*}
    \nabla^2 f(\mathbf{x}) \succeq 0,
    \quad \forall \mathbf{x} \in \mathbb{R}^n
\end{equation*}

\section{Algorithmic design}

\paragraph{Problem:}
\begin{align*}
    \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}),
    \quad \text{where }
    f(\mathbf{x}) = \left( \frac{1}{2} \mathbf{x}^{\top} \mathbf{Q} \mathbf{x} + \mathbf{h}^{\top} \mathbf{x} + c \right),
    \quad \mathbf{Q} \succ \mathbf{0}, \text{i.e.}, \mathbf{Q} \in \mathbf{S}_d^{++},
    \quad \mathbf{h} \in \mathbb{R}^d,
    \quad c \in \mathbb{R}
\end{align*}

\paragraph{Solution:}

The gradient of \( f \) can be computed as
\begin{equation*}
    \nabla f(\mathbf{x}) = \mathbf{Q} \mathbf{x} + \mathbf{h}
\end{equation*}
and thereby from the first-order necessary condition for local minimum, the local minimum \( \mathbf{x}^* \) must necessarily satisfy
\begin{equation*}
    \nabla f(\mathbf{x}^*) = \mathbf{Q} \mathbf{x}^* + \mathbf{h} = \mathbf{0}
    \quad \iff \quad
    \boxed{
        \mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{h}
    }
\end{equation*}

\paragraph{Challenge:}
Not allowed to compute \( \mathbf{Q}^{-1} \) explicitly.
Matrix-vector products with \( \mathbf{Q} \) are allowed.

\subsection{Algorithm template}

\begin{algorithm}[H]
    \caption{
        Iterative generic descent algorithm template for unconstrained minimisation
    }
    \SetAlgoLined
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Choose a descent direction \( \mathbf{p}^{(k)} \) from the set of descent directions
        \begin{equation*}
            \mathcal{DS} \left( \mathbf{x}^{(k)} \right) = \left\{ \mathbf{p} \in \mathbb{R}^d \;\big|\; {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p} < 0 \right\}
        \end{equation*}

        Choose a step size \( \alpha_k \geq 0 \)\;

        Update the current point: \( \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return \( \mathbf{x}^{(k)} \)\;
\end{algorithm}

\paragraph{First-order oracle:}
Given \( \mathbf{x} \in \mathbb{R}^d \), returns a tuple \( \big( f(\mathbf{x}), \nabla f(\mathbf{x}) \big) \).
Typically, assumes \( f \in \mathcal{C}^1 \).
