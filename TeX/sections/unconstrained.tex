\chapter{Unconstrained optimisation}

\section{Optimisation methods}

\subsection{First-order methods}

Assume \( f \in \mathcal{C}^{1} \), and use \( f(\mathbf{x} + \mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^{\top} \mathbf{p} + o(\| \mathbf{p} \|) \).

\subsection{Second-order methods}

Assume \( f \in \mathcal{C}^{2} \), and use \( f(\mathbf{x} + \mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^{\top} \mathbf{p} + \frac{1}{2} \mathbf{p}^{\top} \nabla^{2} f(\mathbf{x}) \mathbf{p} + o(\| \mathbf{p} \|^2) \).

\subsection{Global minimum}

The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{global minimum} of the function \( f: \mathbb{R}^d \to \mathbb{R} \) if
\begin{equation*}
    f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in \mathbb{R}^d
\end{equation*}

\subsection{Local minimum}

The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{local minimum} of the function \( f: \mathbb{R}^d \to \mathbb{R} \) if there exists a \( \delta > 0 \) such that for all \( \mathbf{x} \) in the \( \delta \)-neighborhood of \( \mathbf{x}^* \), we have \( f(\mathbf{x}^*) \leq f(\mathbf{x}) \), i.e.,
\begin{equation*}
    f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*)
\end{equation*}

\section{Necessary and sufficient conditions}

\subsection{First-order necessary condition for a local minimum}

If \( f \in \mathcal{C}^{1} \) and \( \mathbf{x}^* \) is a local minimum of \( f \), then \( \nabla f(\mathbf{x}^*) = \mathbf{0} \).

\subsection{Second-order necessary condition for a local minimum}

If \( f \in \mathcal{C}^{2} \) and \( \mathbf{x}^* \) is a local minimum of \( f \), then \( \nabla f(\mathbf{x}^*) = \mathbf{0}, \; \nabla^2 f(\mathbf{x}^*) \succeq 0 \).

\subsection{Second-order sufficient condition for a strict local minimum}

If \( f \in \mathcal{C}^{2} \), \( \nabla f(\mathbf{x}^*) = \mathbf{0} \) and \( \nabla^2 f(\mathbf{x}^*) \succ 0 \), then \( \mathbf{x}^* \) is a strict local minimum of \( f \).

\subsection{First-order sufficient condition for a local minimum under convexity}

If \( f \in \mathcal{C}^1 \) is a convex function and \( \nabla f(\mathbf{x}^*) = \mathbf{0} \), then \( \mathbf{x}^* \) is a global minimum of \( f \).

\subsection{Convex functions}

A function $f: \mathbb{R}^n \to \mathbb{R}$ is \textbf{convex} if
\begin{equation*}
    f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y})
    \leq
    \lambda f(\mathbf{x}) + (1 - \lambda) f(\mathbf{y}),
    \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n, \; \lambda \in [0, 1]
\end{equation*}

\paragraph{First-order condition for convexity:}
If \( f \in \mathcal{C}^1 \), then \( f \) is convex iff
\begin{equation*}
    f(\mathbf{y})
    \geq
    f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}),
    \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n
\end{equation*}

\paragraph{Second-order condition for convexity:}
If \( f \in \mathcal{C}^2 \), then \( f \) is convex if
\begin{equation*}
    \nabla^2 f(\mathbf{x}) \succeq 0,
    \quad \forall \mathbf{x} \in \mathbb{R}^n
\end{equation*}

\section{Algorithmic design}

\subsection{Iterative algorithm template}

\begin{algorithm}[H]
    \caption{
        Iterative algorithm template for unconstrained minimisation
    }
    \SetAlgoLined
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Update the current point: \( \mathbf{x}^{(k+1)} = \operatorname{ALGO}(\mathbf{x^{(k)}}) \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return \( \mathbf{x}^{(k)} \)\;
\end{algorithm}

\paragraph{First-order oracle:}
Given \( \mathbf{x} \in \mathbb{R}^d \), returns a tuple \( \big( f(\mathbf{x}), \nabla f(\mathbf{x}) \big) \).

\subsection{Line search methods}

\begin{algorithm}[H]
    \caption{
        Algorithm template for unconstrained minimisation using line search
    }
    \SetAlgoLined
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Choose a descent direction \( \mathbf{p}^{(k)} \in \mathbb{R}^d \)\;

        Choose a step size \( \alpha_k \geq 0 \)\;

        Update the current point: \( \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return \( \mathbf{x}^{(k)} \)\;
\end{algorithm}

\subsection{Gradient descent with exact line search}

\begin{algorithm}[H]
    \caption{
        Gradient descent with exact line search
    }
    \SetAlgoLined
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Choose a descent direction \( \mathbf{p}^{(k)} \) from the set of descent directions
        \begin{equation*}
            \mathcal{DS} \left( \mathbf{x}^{(k)} \right) = \left\{ \mathbf{p} \in \mathbb{R}^d \;\big|\; {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p} < 0 \right\}
        \end{equation*}
        One such choice is the negative gradient direction: \( \mathbf{p}^{(k)} = -\nabla f(\mathbf{x}^{(k)}) \)\;

        Choose step size \( \alpha_k \) by solving the one-dimensional optimisation problem
        \begin{equation*}
            \alpha_k = \argmin_{\alpha \geq 0} f\left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
        \end{equation*}

        Update the current point: \( \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return \( \mathbf{x}^{(k)} \)\;
\end{algorithm}

\paragraph{Problem:}
\begin{align*}
    \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}),
    \quad \text{where }
    f(\mathbf{x}) = \left( \frac{1}{2} \mathbf{x}^{\top} \mathbf{Q} \mathbf{x} + \mathbf{h}^{\top} \mathbf{x} + c \right),
    \quad \mathbf{Q} \succ \mathbf{0}, \text{i.e.}, \mathbf{Q} \in \mathbf{S}_d^{++},
    \quad \mathbf{h} \in \mathbb{R}^d,
    \quad c \in \mathbb{R}
\end{align*}

\paragraph{Solution:}

The gradient of \( f \) can be computed as
\begin{equation*}
    \nabla f(\mathbf{x}) = \mathbf{Q} \mathbf{x} + \mathbf{h}
\end{equation*}
and thereby from the first-order necessary condition for local minimum, the local minimum \( \mathbf{x}^* \) must necessarily satisfy
\begin{equation*}
    \nabla f(\mathbf{x}^*) = \mathbf{Q} \mathbf{x}^* + \mathbf{h} = \mathbf{0}
    \quad \iff \quad
    \boxed{
        \mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{h}
    }
\end{equation*}

\paragraph{Challenge:}
Not allowed to compute \( \mathbf{Q}^{-1} \) explicitly.
Matrix-vector products with \( \mathbf{Q} \) are allowed.

\paragraph{Algorithm design:}

To find the optimal step size \( \alpha_k \), consider
\begin{align*}
    g_k(\alpha)
    & \triangleq
    f \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
    \\ & =
    \frac{1}{2} \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)^{\top} \mathbf{Q} \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
    + \mathbf{h}^{\top} \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
    + c
    \\ & =
    \frac{1}{2} {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{x}^{(k)}
    + \alpha {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}
    + \frac{\alpha^2}{2} {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}
    + \mathbf{h}^{\top} \mathbf{x}^{(k)}
    + \alpha \mathbf{h}^{\top} \mathbf{p}^{(k)}
    + c
    \\ & =
    \alpha^2 \left( \frac{1}{2} {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)} \right)
    + \alpha \left( {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)} + \mathbf{h}^{\top} \mathbf{p}^{(k)} \right)
    + \left( \frac{1}{2} {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{x}^{(k)} + \mathbf{h}^{\top} \mathbf{x}^{(k)} + c \right)
    \\ & =
    \alpha^2 \underbrace{ \left( \frac{1}{2} {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)} \right) }_{p_k}
    + \alpha \underbrace{ \left( \nabla f(\mathbf{x}^{(k)})^{\top} \mathbf{p}^{(k)} \right) }_{q_k}
    + \underbrace{ \left( f(\mathbf{x}^{(k)}) \right) }_{r_k}
\end{align*}
which is a quadratic function in \( \alpha \) with \( p_k > 0 \; \left( \because \mathbf{Q} \succ \mathbf{0} \right) \) and \( q_k < 0 \; \left( \because \mathbf{p}^{(k)} \in \mathcal{DS}(\mathbf{x}^{(k)}) \right) \).

For the \hyperlink{1D-quadratic}{one-dimensional quadratic} case, we can then compute the minimum point and minimum value as
\begin{equation*}
    \alpha_k
    = -\frac{q_k}{2 p_k}
    \implies
    \boxed{
        \alpha_k
        = -\frac{\nabla f(\mathbf{x}^{(k)})^{\top} \mathbf{p}^{(k)}}{{\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
    }
    > 0
\end{equation*}
\begin{equation*}
    f \left( \mathbf{x}^{(k+1)} \right)
    =
    g_k(\alpha_k)
    =
    r_k - \frac{q_k^2}{4 p_k}
    \implies
    \boxed{
        f(\mathbf{x}^{(k+1)})
        =
        f(\mathbf{x}^{(k)}) - \frac{\left( \nabla f(\mathbf{x}^{(k)})^{\top} \mathbf{p}^{(k)} \right)^2}{2 {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
    }
\end{equation*}

For the choice \( \mathbf{p}^{(k)} = -\nabla f(\mathbf{x}^{(k)}) \), we have
\begin{equation*}
    \alpha_k
    =
    \frac{{\Vert \nabla f(\mathbf{x}^{(k)}) \Vert}^2} {\nabla f(\mathbf{x}^{(k)})^{\top} \mathbf{Q} \nabla f(\mathbf{x}^{(k)})},
    \qquad
    f(\mathbf{x}^{(k+1)})
    =
    f(\mathbf{x}^{(k)}) - \frac{{\Vert \nabla f(\mathbf{x}^{(k)}) \Vert}^4}{2 \nabla f(\mathbf{x}^{(k)})^{\top} \mathbf{Q} \nabla f(\mathbf{x}^{(k)})}
\end{equation*}

\paragraph{Analysis:}

For a local minimum \( \mathbf{x}^* \), and any \( \mathbf{p} \in \mathbb{R}^d \), we have
\begin{align*}
    f(\mathbf{x}^* + \mathbf{p})
    & =
    \frac{1}{2} (\mathbf{x}^* + \mathbf{p})^{\top} \mathbf{Q} (\mathbf{x}^* + \mathbf{p}) + \mathbf{h}^{\top} (\mathbf{x}^* + \mathbf{p}) + c
    \\ & =
    \frac{1}{2} {\mathbf{x}^*}^{\top} \mathbf{Q} \mathbf{x}^* + {\mathbf{x}^*}^{\top} \mathbf{Q} \mathbf{p} + \frac{1}{2} \mathbf{p}^{\top} \mathbf{Q} \mathbf{p} + \mathbf{h}^{\top} \mathbf{x}^* + \mathbf{h}^{\top} \mathbf{p} + c
    \\ & =
    \left( \frac{1}{2} {\mathbf{x}^*}^{\top} \mathbf{Q} \mathbf{x}^* + \mathbf{h}^{\top} \mathbf{x}^* + c \right)
    + {\cancel{\left( \mathbf{Q} \mathbf{x}^* + \mathbf{h} \right)}}^{\top} \mathbf{p}
    + \frac{1}{2} \mathbf{p}^{\top} \mathbf{Q} \mathbf{p}
    \\ & =
    f(\mathbf{x}^*) + \frac{1}{2} \mathbf{p}^{\top} \mathbf{Q} \mathbf{p}
\end{align*}
