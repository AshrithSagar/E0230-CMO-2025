
\part{Unconstrained optimisation}

\chapter{Introduction}

\section{Unconstrained optimisation problem}

An \textbf{unconstrained minimisation problem} aims to find a point \( \xstar \in \R^d \) that minimises a function \( f: \R^d \to \R \), i.e.,
\vspace{-1.5em}
\begin{align*}
    &
    \minimize_{\x \in \R^d} \func{f}{\x}
    \\
    \func{f}{\xstar}
    =
    \min_{\x \in \R^d} \func{f}{\x}
    \quad \iff \quad
    \xstar
    & =
    \argmin_{\x \in \R^d} \func{f}{\x}
    \quad \iff \quad
    \func{f}{\xstar}
    \leq
    \func{f}{\x},
    \quad \forall \x \in \R^d
\end{align*}

The function \( f \) is called the \textbf{objective function}, the point \( \xstar \) is called the \textbf{minimum point}, and the value \( \func{f}{\xstar} \) is called the \textbf{minimum value}.

\subsection{Examples}

% chktex-file 44
\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Objective function \( f(x) \)}
        & \textbf{Minimum point \( x^\ast \)}
        & \textbf{Minimum value \( f(x^\ast) \)} \\
        \hline

        \( f(x) = \half x^2, \quad x \in \R \)
        & \( x^\ast = 0 \)
        & \( f(x^\ast) = 0 \) \\
        \hline

        \( f(x) = \half {(x - x_0)}^2, \quad x, x_0 \in \R \)
        & \( x^\ast = x_0 \)
        & \( f(x^\ast) = 0 \) \\
        \hline

        \( f(x) = \half {(x - x_0)}^2 + c, \quad x, x_0, c \in \R \)
        & \( x^\ast = x_0 \)
        & \( f(x^\ast) = c \) \\
        \hline

        \hypertarget{1D-quadratic}
        {\( f(x) = ax^2 + bx + c, \quad x, a, b, c \in \R, \; a > 0 \)}
        & \( x^\ast = -\cfrac{b}{2a} \)
        & \( f(x^\ast) = c - \cfrac{b^2}{4a} \)
        \rule[-13pt]{0pt}{36pt} \\
        \hline

        \hypertarget{line-point-minimisation}
        {\( f(t) = \cfrac{1}{2} \norm[\big]{\u - t \v}^2, \quad t \in \R, \; \u, \v \in \R^d, \; \v \neq \zero \)}
        & \( t^\ast = \cfrac{\dotp{\u}{\v}}{\dotp{\v}{\v}} \)
        & \( f(t^\ast) = \cfrac{1}{2} \pbrac{\norm{\u}^2 - \cfrac{{(\dotp{\u}{\v})}^2}{\dotp{\v}{\v}}} \)
        \rule[-20pt]{0pt}{48pt} \\
        \hline
    \end{tabular}
\end{table}

Note that \( \because f(t) \geq 0, \; \forall t \in \R \implies f(t^\ast) \geq 0 \), which gives the Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality}.

\section{Optimisation methods}

\subsection{First-order methods}

Assume \( f \in \classC^1 \), and use \( \func{f}{\x + \p} = \func{f}{\x} + \dotp{\grad{f}{\x}}{\p} + \smalloh{\norm{\p}} \).

\subsection{Second-order methods}

Assume \( f \in \classC^2 \), and use \( \func{f}{\x + \p} = \func{f}{\x} + \dotp{\grad{f}{\x}}{\p} + \half \qf{\p}{\hess{f}{\x}} + \smalloh{\norm{\p}^2} \).

\section{Types of minimum}

\subsection{Global minimum}

\begin{definition}{Global minimum}{global-minimum-unconstrained}
    The point \( \xstar \in \R^d \) is a \textbf{global minimum} of the function \( f: \R^d \to \R \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x},
        \quad \forall \x \in \R^d
    \end{equation*}
\end{definition}

\subsection{Strict global minimum}

\begin{definition}{Strict global minimum}{}
    The point \( \xstar \in \R^d \) is a \textbf{strict global minimum} of the function \( f: \R^d \to \R \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x},
        \quad \forall \x \in \R^d \setminus \set{\xstar}
    \end{equation*}
\end{definition}

\subsection{Local minimum}

\begin{definition}{Local minimum}{local-minimum-unconstrained}
    The point \( \xstar \in \R^d \) is a \textbf{local minimum} of the function \( f: \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \in \R^d \) in the \( \delta \)-neighborhood of \( \xstar \), we have \( \func{f}{\xstar} \leq \func{f}{\x} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x},
        \quad \forall \x \in B_{\delta}(\xstar)
    \end{equation*}
\end{definition}

\begin{corollary}{First-order variational inequality condition}{first-order-variational-inequality-condition}
    For a function \( f \in \classC^1 \), if \( \xstar \) is a local minimum of \( f \), then there exists a \( \delta > 0 \) such that
    \vspace{-0.5em}
    \begin{equation*}
        \dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}} \geq 0,
        \quad \forall \x \in B_{\delta}(\xstar)
    \end{equation*}
\end{corollary}

\begin{proof}
    Since \( f \in \classC^1 \), from Taylor's theorem~\psecref{sec:taylor-theorem}, there exists a \( \delta_1 > 0 \) such that
    \begin{equation*}
        \func{f}{\x}
        =
        \func{f}{\xstar}
        + \dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}},
        \quad \forall \x \in B_{\delta_1}(\xstar)
    \end{equation*}
    Since \( \xstar \) is a local minimum, there exists a \( \delta_2 > 0 \) by~\pdefref{def:local-minimum-unconstrained} such that
    \begin{equation*}
        \func{f}{\xstar}
        \leq
        \func{f}{\x}, \quad \forall \x \in B_{\delta_2}(\xstar)
    \end{equation*}
    Setting \( \delta = \min(\delta_1, \delta_2) > 0 \), we have
    \begin{align*}
        \func{f}{\xstar}
        & \leq
        \func{f}{\x}
        =
        \func{f}{\xstar}
        + \dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}},
        \quad \forall \x \in B_{\delta}(\xstar)
        \\
        \implies
        0
        & \leq
        \dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}},
        \quad \forall \x \in B_{\delta}(\xstar)
    \end{align*}
\end{proof}

\subsection{Strict local minimum}

\begin{definition}{Strict local minimum}{strict-local-minimum-unconstrained}
    The point \( \xstar \in \R^d \) is a \textbf{strict local minimum} of the function \( f: \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \in \R^d \) in the \( \delta \)-neighborhood of \( \xstar \) except \( \xstar \) itself, we have \( \func{f}{\xstar} < \func{f}{\x} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x},
        \quad \forall \x \in B_{\delta}(\xstar) \setminus \set{\xstar}
    \end{equation*}
\end{definition}

\section{Optimality conditions}

\subsection{First-order necessary condition for a local minimum}

\begin{theorem}{First-order necessary condition for a local minimum}{first-order-necessary-condition-for-a-local-minimum}
    For a function \( f \in \classC^1 \), if \( \xstar \) is a local minimum of \( f \), then \( \grad{f}{\xstar} = \zero \).
\end{theorem}

\begin{proof}
    From the first-order variational inequality condition~\pcorref{cor:first-order-variational-inequality-condition}, there exists a \( \delta > 0 \) such that
    \begin{equation*}
        \dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}}
        \geq 0,
        \quad \forall \x \in B_{\delta}(\xstar)
    \end{equation*}
    Setting \( \x = \xstar + t \e_i, \; t \in (-\delta, \delta) \), we can see that \( \x \in B_{\delta}(\xstar) \) for all \( i \in \set{1, 2, \ldots, d} \).
    \begin{equation*}
        \implies
        \dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}}
        =
        \dotp{\grad{f}{\xstar}}{\pbrac{t \e_i}}
        =
        t \pbrac{\dotp{\grad{f}{\xstar}}{\e_i}}
        \geq 0,
        \quad \forall t \in (-\delta, \delta),
        \; i \in \set{1, 2, \ldots, d}
    \end{equation*}
    This is possible only if \( \dotp{\grad{f}{\xstar}}{\e_i} = 0 = \bbrac{\grad{f}{\xstar}}_i, \; \forall i \in \set{1, 2, \ldots, d} \implies \grad{f}{\xstar} = \zero \).
\end{proof}

\subsection{Second-order necessary condition for a local minimum}

\begin{theorem}{Second-order necessary condition for a local minimum}{}
    For a function \( f \in \classC^2 \), if \( \xstar \) is a local minimum of \( f \), then \( \grad{f}{\xstar} = \zero, \; \hess{f}{\xstar} \succeq \zero \).
\end{theorem}

\begin{proof}
    \( \because \classC^2 \subseteq \classC^1 \), by the first-order necessary condition for optimality~\pthmref{thm:first-order-necessary-condition-for-a-local-minimum}, \( \grad{f}{\xstar} = \zero \).

    Since \( f \in \classC^2 \), from Taylor's theorem~\psecref{sec:taylor-theorem}, there exists a \( \delta_1 > 0 \) such that
    \begin{equation*}
        \func{f}{\x}
        =
        \func{f}{\xstar}
        + \cancel{\dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}}}
        + \half \qf{\pbrac{\x - \xstar}}{\hess{f}{\xstar}},
        \quad \forall \x \in B_{\delta_1}(\xstar)
    \end{equation*}
    Since \( \xstar \) is a local minimum, there exists a \( \delta_2 > 0 \) by~\pdefref{def:local-minimum-unconstrained} such that
    \begin{equation*}
        \func{f}{\xstar}
        \leq
        \func{f}{\x},
        \quad \forall \x \in B_{\delta_2}(\xstar)
    \end{equation*}
    Setting \( \delta = \min(\delta_1, \delta_2) > 0 \), we have
    \begin{align*}
        \func{f}{\xstar}
        & \leq
        \func{f}{\x}
        =
        \func{f}{\xstar}
        + \half \qf{\pbrac{\x - \xstar}}{\hess{f}{\xstar}},
        \quad \forall \x \in B_{\delta}(\xstar)
        \\
        \implies
        0
        & \leq
        \qf{\pbrac{\x - \xstar}}{\hess{f}{\xstar}},
        \quad \forall \x \in B_{\delta}(\xstar)
    \end{align*}
    Setting \( \x = \xstar + t \p, \; t \in (0, \delta / \norm{\p}) \), we can see that \( \x \in B_{\delta}(\xstar) \) for all \( \p \in \R^d \setminus \set{\zero} \).
    \begin{align*}
        \implies
        0
        \leq
        \qf{(t \p)}{\hess{f}{\xstar}}
        & =
        t^2 \qf{\p}{\hess{f}{\xstar}},
        \quad \forall t \in (0, \delta / \norm{\p}),
        \; \p \in \R^d \setminus \set{\zero}
        \\
        \implies
        \qf{\p}{\hess{f}{\xstar}}
        & \geq 0,
        \quad \forall \p \in \R^d \setminus \set{\zero}
        \implies
        \hess{f}{\xstar} \succeq \zero
    \end{align*}
\end{proof}

\subsection{Second-order sufficient condition for a strict local minimum}

\begin{theorem}{Second-order sufficient condition for a strict local minimum}{}
    For a function \( f \in \classC^2 \) such that \( \grad{f}{\xstar} = \zero, \; \hess{f}{\xstar} \succ \zero \), then \( \xstar \) is a strict local minimum of \( f \).
\end{theorem}

\begin{proof}
    Since \( f \in \classC^2 \), from Taylor's theorem~\psecref{sec:taylor-theorem}, there exists a \( \delta_1 > 0 \) and \( \xi \in (0, 1) \) such that
    \begin{equation*}
        \func{f}{\x}
        =
        \func{f}{\xstar}
        + \cancel{\dotp{\grad{f}{\xstar}}{\pbrac{\x - \xstar}}}
        + \half \qf{\pbrac{\x - \xstar}}{\hess{f}{\xstar + \xi (\x - \xstar)}},
        \quad \forall \x \in B_{\delta_1}(\xstar)
    \end{equation*}
    Since \( \hess{f}{\xstar} \succ \zero \), and by the continuity of the Hessian, \( \exists \delta_2 > 0 \) such that \( \hess{f}{\z} \succ \zero, \; \forall \z \in B_{\delta_2}(\xstar) \).
    Setting \( \delta = \min(\delta_1, \delta_2) > 0 \), we have
    \begin{align*}
        \func{f}{\x}
        & =
        \func{f}{\xstar}
        + \half \qf{\pbrac{\x - \xstar}}{\hess{f}{\xstar + \xi (\x - \xstar)}},
        \quad \forall \x \in B_{\delta}(\xstar),
        \quad \because \delta \leq \delta_1
        \\
        \because \;
        &
        \xstar + \xi (\x - \xstar)
        \in
        B_{\delta}(\xstar),
        \quad \forall \x \in B_{\delta}(\xstar),
        \quad \because \xi \in (0, 1)
        \\
        \implies
        &
        \hess{f}{\xstar + \xi (\x - \xstar)}
        \succ \zero,
        \quad \forall \x \in B_{\delta}(\xstar),
        \quad \because \delta \leq \delta_2
        \\
        \implies
        &
        \half \qf{\pbrac{\x - \xstar}}{\hess{f}{\xstar + \xi (\x - \xstar)}}
        > 0,
        \quad \forall \x \in B_{\delta}(\xstar) \setminus \set{\xstar}
        \\
        \implies
        &
        \func{f}{\x}
        >
        \func{f}{\xstar},
        \quad \forall \x \in B_{\delta}(\xstar) \setminus \set{\xstar}
    \end{align*}
    which is the definition of a strict local minimum~\pdefref{def:strict-local-minimum-unconstrained}.
\end{proof}

\subsection{First-order sufficient condition for a global minimum under convexity}

\begin{theorem}{First-order sufficient condition for a global minimum under convexity}{first-order-sufficient-condition-for-a-global-minimum-under-convexity}
    For a convex function \( f \in \classC^1 \) with \( \grad{f}{\xstar} = \zero \), then \( \xstar \) is a global minimum of \( f \).
\end{theorem}

\begin{proof}
    From the first-order condition for convexity~\pthmref{thm:first-order-condition-for-convexity}, we have
    \begin{align*}
        \func{f}{\y}
        & \geq
        \func{f}{\xstar} + \cancel{ \dotp{\grad{f}{\xstar}}{(\y - \xstar)} },
        \quad \forall \y \in \calD
        \\
        \implies
        \func{f}{\y}
        & \geq
        \func{f}{\xstar},
        \quad \forall \y \in \calD
    \end{align*}
    which is the definition of a global minimum~\pdefref{def:global-minimum-unconstrained}.
\end{proof}

\section{Convexity}

\begin{definition}{Convex set}{}
    A set \( \calS \subseteq \R^n \) is \textbf{""convex@convex set""} if
    \vspace{-1em}
    \begin{equation*}
        \lambda \x + (1 - \lambda) \y \in \calS,
        \quad \forall \x, \y \in \calS,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\begin{definition}{Convex function}{}
    A function \( f: \calD \subseteq \R^n \to \R \) is \textbf{""convex@convex function""} on a convex set \( \calD \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\lambda \x + (1 - \lambda) \y}
        \leq
        \lambda \func{f}{\x} + (1 - \lambda) \func{f}{\y},
        \quad \forall \x, \y \in \calD,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\subsection{First-order condition for convexity}

\begin{theorem}{First-order condition for convexity}{first-order-condition-for-convexity}
    For a function \( f \in \classC^1 \) with a convex domain \( \calD \), \( f \) is convex on \( \calD \) iff
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\y}
        \geq
        \func{f}{\x} + \dotp{\grad{f}{\x}}{(\y - \x)},
        \quad \forall \x, \y \in \calD
    \end{equation*}
\end{theorem}

\begin{proof}
    \( (\Rightarrow) \)
    Rearranging the definition of convexity, we get
    \begin{equation*}
        \frac{\func{f}{\lambda \x + (1 - \lambda) \y} - \func{f}{\x}}{\lambda}
        \leq
        \func{f}{\y} - \func{f}{\x},
        \quad \forall \x, \y \in \calD,
        \; \lambda \in \linterval{0}{1}
    \end{equation*}
    Taking the limit as \( \lambda \to 0^+ \), we have
    \begin{equation*}
        \dotp{\grad{f}{\x}}{(\y - \x)}
        =
        \lim_{\lambda \to 0^+} \frac{\func{f}{\lambda \x + (1 - \lambda) \y} - \func{f}{\x}}{\lambda}
        \leq
        \func{f}{\y} - \func{f}{\x},
        \quad \forall \x, \y \in \calD
    \end{equation*}
    which is the desired result.
\end{proof}

\begin{corollary}{Monotonicity of gradient \( \iff \) Convexity}{}
    For a function \( f \in \classC^1 \) with a convex domain \( \calD \), \( f \) is convex on \( \calD \) iff
    \vspace{-0.5em}
    \begin{equation*}
        \dotp{\pbrac[\big]{\grad{f}{\x} - \grad{f}{\y}}}{\pbrac[\big]{\x - \y}} \geq 0,
        \quad \forall \x, \y \in \calD
    \end{equation*}
\end{corollary}

\begin{proof}
    \( (\Rightarrow) \)
    From the first-order condition for convexity, we have
    \begin{align*}
        \func{f}{\y}
        & \geq
        \func{f}{\x} + \dotp{\grad{f}{\x}}{\pbrac{\y - \x}},
        \quad \forall \x, \y \in \calD
        \\
        \func{f}{\x}
        & \geq
        \func{f}{\y} + {\grad{f}{\y}}^\top (\x - \y),
        \quad \forall \x, \y \in \calD
    \end{align*}
    Adding the two inequalities, we get
    \begin{equation*}
        0
        \geq
        \dotp{\pbrac[\big]{\grad{f}{\x} - \grad{f}{\y}}}{\pbrac{\y - \x}},
        \quad \forall \x, \y \in \calD
    \end{equation*}
    which is equivalent to the desired result.

    \( (\Leftarrow) \)
    Define \( \func{g}{t} = f(\x + t (\y - \x)), \;\; t \in [0, 1], \;\; \x, \y \in \calD \).

    Then, \( \func{g}{0} = \func{f}{\x}, \quad \func{g}{1} = \func{f}{\y}, \quad \func{g'}{t} = \dotp{(\grad{f}{\x + t (\y - \x)})}{(\y - \x)} \).

    From the given condition, we have
    \begin{align*}
        \func{g'}{t}
        & =
        {\pbrac[\big]{\nabla f(\x + t (\y - \x)) - \grad{f}{\x} + \grad{f}{\x}}}^\top (\y - \x)
        \\ & =
        {\pbrac[\big]{\nabla f(\x + t (\y - \x)) - \grad{f}{\x}}}^\top (\y - \x) + {\pbrac[\big]{\grad{f}{\x}}}^\top (\y - \x)
        \geq
        {\pbrac[\big]{\grad{f}{\x}}}^\top (\y - \x)
    \end{align*}
    for all \( t \in [0, 1] \).
    Integrating from \( 0 \) to \( 1 \), we get
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x}
        =
        \func{g}{1} - \func{g}{0}
        =
        \int_{0}^{1} \func{g'}{t} \, dt
        \geq
        \int_{0}^{1} {\pbrac[\big]{\grad{f}{\x}}}^\top (\y - \x) \, dt
        =
        {\pbrac[\big]{\grad{f}{\x}}}^\top (\y - \x)
    \end{equation*}
    for all \( \x, \y \in \calD \), which is the first-order condition for convexity, and thereby \( f \) is convex.
\end{proof}

\subsection{Second-order condition for convexity}

\begin{theorem}{Second-order condition for convexity}{second-order-condition-for-convexity}
    For a function \( f \in \classC^2 \) with a convex domain \( \calD \), \( f \) is convex on \( \calD \) iff
    \vspace{-0.5em}
    \begin{equation*}
        \hess{f}{\x}
        \succeq \zero,
        \quad \forall \x \in \calD
    \end{equation*}
\end{theorem}

\begin{proof}
    \( (\Rightarrow) \)
    The first-order condition for convexity can be rewritten as
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - \dotp{\grad{f}{\x}}{\pbrac{\y - \x}}
        \geq 0,
        \quad \forall \x, \y \in \calD
    \end{equation*}
    Using Taylor's theorem, we have
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - \dotp{\grad{f}{\x}}{\pbrac{\y - \x}}
        =
        \half {(\y - \x)}^\top \hess{f}{\bfxi} (\y - \x),
        \quad \bfxi = (1 - t) \x + t \y, \; t \in (0, 1)
    \end{equation*}
    for some \( t \in (0, 1) \).
    Therefore, we have
    \begin{equation*}
        {(\y - \x)}^\top \hess{f}{\bfxi} (\y - \x)
        \geq 0,
        \quad \forall \x, \y \in \calD
    \end{equation*}
    which implies \( \hess{f}{\x} \succeq \zero, \; \forall \x \in \calD \).
    (Note: \( \bfxi \in \calD \) since \( \calD \) is convex.)

    \( (\Leftarrow) \)
    From Taylor's theorem, we have
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - \dotp{\grad{f}{\x}}{\pbrac{\y - \x}}
        =
        \half {(\y - \x)}^\top \hess{f}{\bfxi} (\y - \x),
        \quad \bfxi = (1 - t) \x + t \y, \; t \in (0, 1)
    \end{equation*}
    for some \( t \in (0, 1) \).
    Since \( \hess{f}{\x} \succeq \zero, \; \forall \x \in \calD \), we have
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - \dotp{\grad{f}{\x}}{\pbrac{\y - \x}}
        \geq 0,
        \quad \forall \x, \y \in \calD
    \end{equation*}
    which is the first-order condition for convexity, and thereby \( f \) is convex.
\end{proof}

\section{Strong convexity}

\begin{definition}{Strong convexity}{}
    A function \( f: \calD \subseteq \R^n \to \R \) on a convex domain \( \calD \) is \textbf{strongly convex} with parameter \( \mu > 0 \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\lambda \x + (1 - \lambda) \y}
        \leq
        \lambda \func{f}{\x} + (1 - \lambda) \func{f}{\y}
        - \frac{\mu}{2} \lambda (1 - \lambda) \norm{\x - \y}^2,
        \quad \forall \x, \y \in \R^n,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\begin{corollary}{}{}
    A function \( f: \calD \subseteq \R^n \to \R \) on a convex domain \( \calD \) is strongly convex with parameter \( \mu > 0 \) iff the function \( \func{g}{\x} := \pbrac{\func{f}{\x} - \frac{\mu}{2} \norm{\x}^2} \) is convex on \( \calD \).
\end{corollary}

\begin{proof}
    Follows from the identity:
    \begin{align*}
        \lambda (1 - \lambda) \norm{\x - \y}^2
        & =
        \lambda (1 - \lambda) \pbrac[\Big]{\norm{\y}^2 + \norm{\x}^2 - 2 \dotp{\x}{\y}}
        \\ & =
        \lambda \norm{\y}^2 + \lambda \norm{\x}^2 - \lambda^2 \norm{\y}^2 - \norm{\lambda \x}^2 - 2 {(\lambda \x)}^\top \pbrac[\big]{(1 - \lambda) \y}
        \\ & =
        \lambda \norm{\y}^2 + \lambda \norm{\x}^2 - \lambda^2 \norm{\y}^2 - \norm{\lambda \x + (1 - \lambda) \y}^2 + {(1 - \lambda)}^2 \norm{\y}^2
        \\ & =
        \cancel{ \lambda \norm{\y}^2 } + \lambda \norm{\x}^2 - \cancel{ \lambda^2 \norm{\y}^2 } - \norm{\lambda \x + (1 - \lambda) \y}^2 + \norm{\y}^2 - \cancel{ 2 } \lambda \norm{\y}^2 + \cancel{ \lambda^2 \norm{\y}^2 }
        \\ & =
        \lambda \norm{\x}^2 + (1 - \lambda) \norm{\y}^2 - \norm{\lambda \x + (1 - \lambda) \y}^2
    \end{align*}
\end{proof}

\subsection{First-order condition for strong convexity}

\begin{corollary}{First-order condition for strong convexity}{}
    For a function \( f \in \classC^1 \) with a convex domain \( \calD \), \( f \) is strongly convex on \( \calD \) with parameter \( \mu > 0 \) iff
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\y}
        \geq
        \func{f}{\x} + \dotp{\grad{f}{\x}}{\pbrac{\y - \x}} + \frac{\mu}{2} \norm{\y - \x}^2,
        \quad \forall \x, \y \in \calD
    \end{equation*}
\end{corollary}

\begin{proof}
    Define \( \func{g}{\x} = \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 \).
    Then, \( \grad{g}{\x} = \grad{f}{\x} - \mu \x \).
    From the first-order condition for convexity~\pthmref{thm:first-order-condition-for-convexity}, we have that \( g \) is convex iff
    \begin{align*}
        \func{g}{\y}
        & \geq
        \func{g}{\x} + \dotp{\grad{g}{\x}}{\pbrac{\y - \x}},
        \quad \forall \x, \y \in \calD
        \\
        \implies
        \func{f}{\y} - \frac{\mu}{2} \norm{\y}^2
        & \geq
        \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 + {\pbrac[\big]{\grad{f}{\x} - \mu \x}}^\top (\y - \x)
        \\
        \implies
        \func{f}{\y}
        & \geq
        \func{f}{\x} + \dotp{\grad{f}{\x}}{\pbrac{\y - \x}} + \frac{\mu}{2} \pbrac[\Big]{\norm{\y}^2 - \norm{\x}^2 - 2 \dotp{\x}{(\y - \x)}}
        \\ & =
        \func{f}{\x} + \dotp{\grad{f}{\x}}{\pbrac{\y - \x}} + \frac{\mu}{2} \pbrac[\Big]{\norm{\y}^2 - \cancel{ \norm{\x}^2 } - 2 \dotp{\x}{\y} + \cancel{2} \norm{\x}^2}
        \\ & =
        \func{f}{\x} + \dotp{\grad{f}{\x}}{\pbrac{\y - \x}} + \frac{\mu}{2} \norm{\y - \x}^2,
        \quad \forall \x, \y \in \calD
    \end{align*}
\end{proof}

\subsection{Second-order condition for strong convexity}

\begin{corollary}{Second-order condition for strong convexity}{}
    For a function \( f \in \classC^2 \) with a convex domain \( \calD \), \( f \) is strongly convex on \( \calD \) with parameter \( \mu > 0 \) iff
    \vspace{-0.5em}
    \begin{equation*}
        \hess{f}{\x}
        \succeq
        \mu \I,
        \quad \forall \x \in \calD
    \end{equation*}

    \vspace{-0.5em}
    i.e., the Hessian \( \hess{f}{\x} \) is positive definite with all eigenvalues at least \( \mu \), for all \( \x \in \calD \).
\end{corollary}

\begin{proof}
    Define \( \func{g}{\x} = \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 \).
    Then, \( \hess{g}{\x} = \hess{f}{\x} - \mu \I \).
    From the second-order condition for convexity~\pthmref{thm:second-order-condition-for-convexity}, we have that \( g \) is convex iff \( \hess{g}{\x} \succeq \zero, \; \forall \x \in \calD \), which is equivalent to the desired result.
\end{proof}

\section{Strongly convex quadratic minimisation problem}\label{sec:strongly-convex-quadratic-minimisation-problem}

\paragraph{Problem:}
Consider the unconstrained strongly convex quadratic minimisation problem
\begin{align*}
    \min_{\x \in \R^d} \func{f}{\x},
    \quad \text{where }
    \func{f}{\x} = \pbrac{\half \qf{\x}{\Q} + \dotp{\h}{\x} + c},
    \quad \Q \succ \zero, \text{i.e.}, \Q \in \mathbf{S}_d^{++},
    \quad \h \in \R^d,
    \quad c \in \R
\end{align*}

\paragraph{Solution:}
Since \( \hess{f}{\x} = \Q \succ \zero \), \( f \) is strongly convex, and thereby has a unique global minimum which is also the unique local minimum~\pthmref{thm:first-order-sufficient-condition-for-a-global-minimum-under-convexity}.
The gradient of \( f \) can be computed as \( \grad{f}{\x} = \Q \x + \h \), and thereby from the first-order necessary condition for local minimum~\pthmref{thm:first-order-necessary-condition-for-a-local-minimum}, the local minimum \( \xstar \) (that is also the global minimum) must necessarily satisfy
\begin{equation*}
    \grad{f}{\xstar} = \Q \xstar + \h = \zero
    \quad \iff \quad
    \boxed{
        \xstar = -\Qinv \h
    }
\end{equation*}
where \( \Qinv \) exists since \( \Q \succ \zero \).

\paragraph{Challenge:}
Not allowed to compute \( \Qinv \) explicitly.
Matrix-vector products with \( \Q \) are allowed.

\chapter{Algorithmic design}

\section{Oracle}

An \textbf{oracle} is a procedure that provides information about the objective function \( f: \R^d \to \R \) at a given point \( \x \in \R^d \).
An \( n \)-th order oracle provides information up to the \( n \)-th derivative of \( f \) at \( \x \).
\begin{itemize}
    \item \textbf{Zero-order oracle:} Given \( \x \in \R^d \), returns the function value \( \func{f}{\x} \in \R \).

    \item \textbf{First-order oracle:} Given \( \x \in \R^d \), returns a tuple \( \pbrac[\big]{\func{f}{\x}, \grad{f}{\x}} \in (\R, \R^d) \).

    \item \textbf{Second-order oracle:} Given \( \x \in \R^d \), returns a tuple \( \pbrac[\big]{\func{f}{\x}, \grad{f}{\x}, \hess{f}{\x}} \in (\R, \R^d, \R^{d \times d}) \).
\end{itemize}

\section{Iterative algorithm template}

\begin{algorithm}[H]
    \caption{
        Iterative algorithm template for unconstrained minimisation
    }\label{alg:iterative-algorithm-template}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \x^k \) is not optimal}{
        Update the current point: \( \x^{k+1} = \func{\operatorname{ALGO}}{\x^k} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\section{Line search methods}\label{sec:line-search}

\begin{algorithm}[H]
    \caption{
        Algorithm template for unconstrained minimisation using line search
    }\label{alg:line-search}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \x^k \) is not optimal}{
        Choose a descent direction \( \p^k \) from the set of descent directions
        \begin{equation*}
            \func{\mathcal{DS}}{\x^k}
            =
            \set{\p \in \R^d \given \dotp{\grad{f}{\x^k}}{\p} < 0}
        \end{equation*}

        Choose a step length \( \alpha_k \geq 0 \)\;

        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \p^k \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\paragraph{Gradient descent:}
Algorithms that use the gradient \( \grad{f}{\x^k} \) to compute the descent direction \( \p^k \).

\paragraph{Steepest descent method (Cauchy):}
Choose the descent direction as
\begin{equation*}
    \p^k = -\grad{f}{\x^k}
    ,\qquad \text{or normalized} \quad
    \p^k = - \frac{\grad{f}{\x^k}}{\norm{\grad{f}{\x^k}}}
\end{equation*}

\begin{table}[htbp]
    \centering
    \caption{
        Line search methods, given a descent direction \( \p^k \)
    }\label{tab:line-search-methods}
    \begin{tabular}{rcl}
        \toprule
        \textbf{Method}
        &
        Condition
        &
        Parameters
        \\
        \midrule
        Exact line search~\psecref{sec:exact-line-search}\psecref{sec:exact-line-search-quadratic}
        &
        \(
            \displaystyle
            \alpha_k
            =
            \argmin_{\alpha \geq 0} g_k(\alpha),
            \qquad
            \alpha_k
            =
            -\frac
            {\dotp{\grad{f}{\x^k}}{\p^k}}
            {\qf{\p^k}{\Q}}
        \)
        \\
        \midrule
        Armijo~\psecref{sec:armijo-condition}
        &
        \(
            \displaystyle
            \func{f}{\x^k + \alpha_k \p^k}
            \leq
            \func{f}{\x^k} + c \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
        \)
        &
        \( 0 < c < 1 \)
        \\
        \midrule
        Armijo-Goldstein~\psecref{sec:armijo-goldstein-conditions}
        &
        \(
            \displaystyle
            \begin{aligned}
                \func{f}{\x^k + \alpha_k \p^k}
                \leq
                \func{f}{\x^k} + c \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
                \\
                \func{f}{\x^k + \alpha_k \p^k}
                \geq
                \func{f}{\x^k} + (1 - c) \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
            \end{aligned}
        \)
        &
        \( 0 < c < \half \)
        \\
        \midrule
        Wolfe~\psecref{sec:wolfe-conditions}
        &
        \(
            \displaystyle
            \begin{aligned}
                \func{f}{\x^k + \alpha_k \p^k}
                \leq
                \func{f}{\x^k} + c_1 \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
                \\
                \dotp{\grad{f}{\x^k + \alpha_k \p^k}}{\p^k}
                \geq
                c_2 \dotp{\grad{f}{\x^k}}{\p^k}
            \end{aligned}
        \)
        &
        \( 0 < c_1 < c_2 < 1 \)
        \\
        \midrule
        Strong Wolfe~\psecref{sec:strong-wolfe-conditions}
        &
        \(
            \displaystyle
            \begin{aligned}
                \func{f}{\x^k + \alpha_k \p^k}
                \leq
                \func{f}{\x^k} + c_1 \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
                \\
                \abs{\dotp{\grad{f}{\x^k + \alpha_k \p^k}}{\p^k}}
                \leq
                c_2 \abs{\dotp{\grad{f}{\x^k}}{\p^k}}
            \end{aligned}
        \)
        &
        \( 0 < c_1 < c_2 < 1 \)
        \\
        \bottomrule
    \end{tabular}
\end{table}

\chapter{Exact line search}\label{sec:exact-line-search}

Choose the step length \( \alpha_k \) in line search~\psecref{sec:line-search} by solving the one-dimensional optimisation problem
\begin{equation*}
    \alpha_k = \argmin_{\alpha \geq 0} g_k(\alpha),
    \qquad \text{where } \;
    g_k(\alpha) \triangleq f\pbrac{\x^k + \alpha \p^k}
\end{equation*}

\section{Strongly convex quadratic minimisation problem}\label{sec:exact-line-search-quadratic}

Consider the unconstrained strongly convex quadratic minimisation problem~\psecref{sec:strongly-convex-quadratic-minimisation-problem}.

\paragraph{Algorithm design:}

To find the optimal step length \( \alpha_k \), consider
\begin{align*}
    g_k(\alpha)
    & \triangleq
    \func{f}{\x^k + \alpha \p^k}
    \\ & =
    \half \qf{\pbrac{\x^k + \alpha \p^k}}{\Q}
    + \dotp{\h}{\pbrac{\x^k + \alpha \p^k}}
    + c
    \\ & =
    \half \qf{\x^k}{\Q}
    + \alpha \qf{\x^k}{\Q}[\p^k]
    + \frac{\alpha^2}{2} \qf{\p^k}{\Q}
    + \dotp{\h}{\x^k}
    + \alpha \dotp{\h}{\p^k}
    + c
    \\ & =
    \alpha^2 \pbrac{\half \qf{\p^k}{\Q}}
    + \alpha \pbrac{\qf{\x^k}{\Q}[\p^k] + \dotp{\h}{\p^k}}
    + \pbrac{\half \qf{\x^k}{\Q} + \dotp{\h}{\x^k} + c}
    \\ & =
    \alpha^2 \underbrace{ \pbrac{\half \qf{\p^k}{\Q}} }_{p_k}
    + \alpha \underbrace{ \pbrac{\dotp{\grad{f}{\x^k}}{\p^k}} }_{q_k}
    + \underbrace{ \pbrac[\Big]{\func{f}{\x^k}} }_{r_k}
\end{align*}
which is a quadratic function in \( \alpha \) with \( p_k > 0 \; \pbrac{\because \Q \succ \zero} \) and \( q_k < 0 \; \pbrac{\because \p^k \in \func{\mathcal{DS}}{\x^k}} \).

For the \hyperlink{1D-quadratic}{one-dimensional quadratic} case, we can then compute the minimum point and minimum value as
\begin{equation*}
    \alpha_k
    =
    -\frac{q_k}{2 p_k}
    \implies
    \boxed{
        \alpha_k
        =
        -\frac
        {\dotp{\grad{f}{\x^k}}{\p^k}}
        {\qf{\p^k}{\Q}}
    }
    > 0
\end{equation*}
\begin{equation*}
    \func{f}{\x^{k+1}}
    =
    g_k(\alpha_k)
    =
    r_k - \frac{q_k^2}{4 p_k}
    \implies
    \boxed{
        \func{f}{\x^{k+1}}
        =
        \func{f}{\x^k}
        -
        \frac
        {\pbrac{\dotp{\grad{f}{\x^k}}{\p^k}}^2}
        {2 \qf{\p^k}{\Q}}
    }
\end{equation*}

For the choice \( \p^k = -\grad{f}{\x^k} \), we have
\begin{equation*}
    \alpha_k
    =
    \frac
    {\norm{\grad{f}{\x^k}}^2}
    {\qf{\grad{f}{\x^k}}{\Q}},
    \qquad
    \func{f}{\x^{k+1}}
    =
    \func{f}{\x^k}
    -
    \frac{\norm{\grad{f}{\x^k}}^4}
    {2 \qf{\grad{f}{\x^k}}{\Q}}
\end{equation*}

\paragraph{Analysis:}

For a local minimum \( \xstar \), and any \( \p \in \R^d \), we have
\begin{align*}
    \func{f}{\xstar + \p}
    & =
    \half \qf{\pbrac{\xstar + \p}}{\Q} + \dotp{\h}{\pbrac{\xstar + \p}} + c
    \\ & =
    \half \qf{\xstar}{\Q} + \qf{\xstar}{\Q}[\p] + \half \qf{\p}{\Q} + \dotp{\h}{\xstar} + \dotp{\h}{\p} + c
    \\ & =
    \pbrac{\half \qf{\xstar}{\Q} + \dotp{\h}{\xstar} + c}
    + \dotp{\cancel{\pbrac{\Q \xstar + \h}}}{\p}
    + \half \qf{\p}{\Q}
    \\ & =
    \func{f}{\xstar} + \half \qf{\p}{\Q}
    \\
    \implies
    \func{f}{\x}
    & =
    \func{f}{\xstar} + \half \qf{\pbrac{\x - \xstar}}{\Q},
    \quad \forall \x \in \R^d
\end{align*}

Now, consider the error at the \( k^{th} \) iteration defined as
\begin{align*}
    E_k
    & \triangleq
    \func{f}{\x^k} - \func{f}{\xstar}
    \\
    \implies
    E_k
    & =
    \half \qf{\pbrac{\x^k - \xstar}}{\Q}
    \\
    \because
    \grad{f}{\xstar}
    & =
    \Q \xstar + \h
    =
    \zero
    \\
    \implies
    \grad{f}{\x^k}
    & =
    \Q \x^k + \h
    =
    \Q \pbrac{\x^k - \xstar}
    \\
    \implies
    \pbrac{\x^k - \xstar}
    & =
    \Qinv \, \grad{f}{\x^k}
    \\
    \implies
    E_k
    & =
    \half \dotp{\pbrac{\x^k - \xstar}}{\grad{f}{\x^k}}
    =
    \half \dotp{\pbrac[\Big]{\Qinv \, \grad{f}{\x^k}}}{\grad{f}{\x^k}}
    \\ & =
    \half \qf{\grad{f}{\x^k}}{\pbrac{\Qinv}^\top}
    =
    \half \qf{\grad{f}{\x^k}}{\Qinv}
\end{align*}

Now, the successive error reduction can be computed as
\begin{align*}
    E_k - E_{k+1}
    & =
    \func{f}{\x^k} - \func{f}{\x^{k+1}}
    =
    \frac
    {\pbrac{\dotp{\grad{f}{\x^k}}{\p^k}}^2}
    {2 \qf{\p^k}{\Q}}
    \\
    \implies
    \frac{E_{k} - E_{k+1}}{E_k}
    & =
    \frac
    {\pbrac{\dotp{\grad{f}{\x^k}}{\p^k}}^2}
    {\qf{\grad{f}{\x^k}}{\Qinv} \; \qf{\p^k}{\Q}}
\end{align*}

For the choice \( \p^k = -\grad{f}{\x^k} \), we have
\begin{align*}
    \frac{E_{k} - E_{k+1}}{E_k}
    & =
    \frac
    {\bbrac{\dotp{\grad{f}{\x^k}}{\pbrac{-\grad{f}{\x^k}}}}^2}
    {\qf{\grad{f}{\x^k}}{\Qinv} \; \qf{\pbrac{-\grad{f}{\x^k}}}{\Q}}
    \\ & =
    \frac
    {\norm{\grad{f}{\x^k}}^4}
    {\qf{\grad{f}{\x^k}}{\Qinv} \; \qf{\grad{f}{\x^k}}{\Q}}
\end{align*}

From Kantorovich inequality~\psecref{sec:kantorovich-inequality}, we have
\begin{equation*}
    \frac{\norm{\z}^4}{\pbrac{\qf{\z}{\Q}} \, \pbrac{\qf{\z}{\Qinv}}}
    \geq
    \frac{4 \func{\lambda_{\min}}{\Q} \func{\lambda_{\max}}{\Q}}{\pbrac{\func{\lambda_{\min}}{\Q} + \func{\lambda_{\max}}{\Q}}^2},
    \quad \forall \z \in \R^d \setminus \set{\zero}
\end{equation*}
where \( \func{\lambda_{\min}}{\Q} \) and \( \func{\lambda_{\max}}{\Q} \) are the minimum and maximum eigenvalues of \( \Q \), respectively.

Applying this with \( \z = \grad{f}{\x^k} \), we have
\begin{align*}
    \frac{E_k - E_{k+1}}{E_k}
    & \geq
    \frac{4 \func{\lambda_{\min}}{\Q} \func{\lambda_{\max}}{\Q}}{\pbrac{\func{\lambda_{\min}}{\Q} + \func{\lambda_{\max}}{\Q}}^2}
    \\
    \implies
    \frac{E_{k+1}}{E_k}
    & \leq
    1 - \frac{4 \func{\lambda_{\min}}{\Q} \func{\lambda_{\max}}{\Q}}{\pbrac{\func{\lambda_{\min}}{\Q} + \func{\lambda_{\max}}{\Q}}^2}
    =
    \pbrac{\frac{\func{\lambda_{\max}}{\Q} - \func{\lambda_{\min}}{\Q}}{\func{\lambda_{\max}}{\Q} + \func{\lambda_{\min}}{\Q}}}^2
    \triangleq
    \rho
\end{align*}
\begin{equation*}
    \implies
    \frac{E_k}{E_0}
    =
    \prod_{i = 0}^{k-1} \frac{E_{i+1}}{E_i}
    \leq
    \prod_{i = 0}^{k-1} \rho
    =
    \rho^k
    \implies
    E_k
    \leq
    \rho^k E_0
\end{equation*}
where \( \rho \in \rinterval{0}{1} \; \because \Q \succ \zero, \; \func{\lambda_{\max}}{\Q} \geq \func{\lambda_{\min}}{\Q} > 0 \), and \( 4ab \leq {(a+b)}^2, \; \forall a, b \).

\chapter{Inexact line search}

The exact line search method~\psecref{sec:exact-line-search} may be computationally expensive or infeasible.

Instead, we can choose the step length \( \alpha_k \) in line search such that it satisfies certain conditions that ensure sufficient decrease in the objective function value.

\section{Sufficient decrease condition (Armijo condition)}\label{sec:armijo-condition}

\begin{definition}{Sufficient decrease condition (Armijo condition)~\citep{Nocedal2006}}{}
    \begin{equation*}
        \func{f}{\x^k + \alpha_k \p^k}
        \leq
        \func{f}{\x^k} + c \alpha_k \dotp{\grad{f}{\x^k}}{\p^k},
        \quad 0 < c < 1
    \end{equation*}
\end{definition}

\subsection{Forward expansion}

Note that for sufficiently small \( \alpha_k \), the sufficient decrease condition is always satisfied since
\begin{align*}
    \func{f}{\x^k + \alpha_k \p^k}
    & =
    \func{f}{\x^k} + \alpha_k \dotp{\grad{f}{\x^k}}{\p^k} + \smalloh{\p^k}
    \\ & <
    \func{f}{\x^k} + c \alpha_k \dotp{\grad{f}{\x^k}}{\p^k},
    \qquad \because
    \dotp{\grad{f}{\x^k}}{\p^k} < 0,
    \quad 0 < c < 1
\end{align*}

\subsection{Backtracking line search}

Determine the largest number \( \alpha_k \) in the sequence \( \set{\beta^m \bar{\alpha}}_{m = 0}^{\infty} \) that satisfies the \textit{sufficient decrease condition}, where \( \bar{\alpha} > 0 \) is an initial step length and \( 0 < \beta < 1 \) is a scaling factor.

\begin{algorithm}[H]
    \caption{
        Backtracking line search to satisfy the sufficient decrease condition
    }\label{alg:backtracking-line-search}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);

        Initial step length \( \bar{\alpha} > 0 \);
        Scaling factor \( 0 < \beta < 1 \);
        Sufficient decrease parameter \( 0 < c < 1 \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \x^k \) is not optimal}{
        Choose a descent direction \( \p^k \)\;

        Initialize the step length:
        \( \alpha \leftarrow \bar{\alpha} \)\;

        \While{\( \func{f}{\x^k + \alpha \p^k} > \func{f}{\x^k} + c \alpha \dotp{\grad{f}{\x^k}}{\p^k} \)}{
            \( \alpha \leftarrow \beta \alpha \)\;
        }

        \( \alpha_k \leftarrow \alpha \)\;

        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \p^k \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\section{Goldstein conditions / Armijo-Goldstein conditions}\label{sec:armijo-goldstein-conditions}

\begin{definition}{Goldstein conditions / Armijo-Goldstein conditions~\citep{Nocedal2006}}{}
    \vspace*{-1em}
    \begin{align*}
        \func{f}{\x^k} + (1 - c) \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
        & \leq
        \overbrace{
            \func{f}{\x^k + \alpha_k \p^k}
            \leq
            \func{f}{\x^k} + c \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
        }^{\text{sufficient decrease}}
        \\
        \text{where } \;&
        0 < c < \half
    \end{align*}
\end{definition}

\subsection{Rate of convergence}

\begin{equation*}
    \sim
    R_k
    =
    \frac{2 \rho_1 (1 - \rho_2)}{L} \cos^2 \theta^{(k)}
\end{equation*}

\section{Wolfe conditions}\label{sec:wolfe-conditions}

\begin{definition}{Wolfe conditions~\citep{Nocedal2006}}{}
    \vspace*{-1em}
    \begin{align*}
        \func{f}{\x^k + \alpha_k \p^k}
        & \leq
        \func{f}{\x^k} + c_1 \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
        \tag{sufficient decrease}
        \\
        \dotp{\grad{f}{\x^k + \alpha_k \p^k}}{\p^k}
        & \geq
        c_2 \dotp{\grad{f}{\x^k}}{\p^k}
        \tag{curvature}
        \\
        \text{where } \;&
        0 < c_1 < c_2 < 1
    \end{align*}
\end{definition}

\begin{equation*}
    R = \frac{c_1 (1 - c_2)}{L}
\end{equation*}

\subsection{Strong Wolfe conditions}\label{sec:strong-wolfe-conditions}

\begin{definition}{Strong Wolfe conditions~\citep{Nocedal2006}}{}
    \vspace*{-1em}
    \begin{align*}
        \func{f}{\x^k + \alpha_k \p^k}
        & \leq
        \func{f}{\x^k} + c_1 \alpha_k \dotp{\grad{f}{\x^k}}{\p^k}
        \tag{sufficient decrease}
        \\
        \abs{\dotp{\grad{f}{\x^k + \alpha_k \p^k}}{\p^k}}
        & \leq
        c_2 \abs{\dotp{\grad{f}{\x^k}}{\p^k}}
        \tag{strong curvature}
        \\
        \text{where } \;&
        0 < c_1 < c_2 < 1
    \end{align*}
\end{definition}

\section{Sufficient decrease lemma}

\begin{lemma}{Sufficient decrease lemma}{}
    Suppose \( f \in \classC^1_{L} \) is bounded below, then \( \forall \x \in \R^n, \; \alpha \in \linterval[scaled]{0}{\frac{1}{L}} \), the gradient descent update \( \xhat = \x - \alpha \grad{f}{\x} \) yields a sufficient decrease in the function value:
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xhat}
        \leq
        \func{f}{\x} - \frac{\alpha}{2} \norm{\grad{f}{\x}}^2
    \end{equation*}
\end{lemma}

\begin{equation*}
    \frac{\func{f}{\x^k} - \func{f}{\x^{k+1}}}{\norm{\grad{f}{\x^k}}^2}
    \geq R_k \geq R > 0
\end{equation*}

\section{Global convergence criterion}

Starting from an arbitrary \( \x^0 \in \R^d \), if a scheme generates a sequence \( \set{\x^k}_{k = 1}^{\infty} \) such that
\begin{equation*}
    \lim_{k \to \infty} \norm{\grad{f}{\x^k}} = 0
\end{equation*}
then the scheme is said to exhibit \textbf{global convergence}.

The sequence asymptotically approaches a \textbf{stationary point} \( \xstar \), i.e., \( \grad{f}{\xstar} = \zero \).
Without further assumptions on \( f \), nothing can be said about \( \xstar \) being a local or global minimum, maximum, or a saddle point.
The `global' refers to the fact that the initial point \( \x^0 \) is arbitrary, and not about the limit point.

\chapter{Conjugate gradient methods}

\section{\texorpdfstring{\( \Q \)--conjugacy}{Q-conjugacy}}

\begin{definition}{\( \Q \)--conjugacy / \( \Q \)--orthogonality}{}
    Given some real symmetric positive definite matrix \( \Q \in \PD \), a pair of distinct vectors \( \u, \v \in \R^d, \; \u \neq \v \) are said to be \textbf{conjugates with respect to \( \Q \)} \text{ OR } \textbf{\( \Q \)--conjugates} if
    \vspace{-0.5em}
    \begin{equation*}
        \qf{\u}{\Q}[\v] = 0
    \end{equation*}
\end{definition}

\subsection{Properties}

\begin{itemize}
    \item\label{item:orthogonality-implies-q-conjugacy}
        \textit{Generalisation of orthogonality}:\\
        If \( \Q = \I \), then \( \Q \)--conjugacy reduces to orthogonality, i.e.,
        \(
            \Q = \I
            \implies
            \qf{\u}{\Q}[\v] = \dotp{\u}{\v} = 0
        \).

    \item \textit{Symmetry}:
        \(
            \qf{\u}{\Q}[\v] = 0
            \iff
            \qf{\v}{\Q}[\u] = 0,
            \quad \because \Q = \Q^\top
        \)

    \item Not transitive.
        \(
            \qf{\x}{\Q}[\y] = 0,
            \quad
            \qf{\y}{\Q}[\z] = 0
            \notimplies
            \qf{\x}{\Q}[\z] = 0
        \)

    \item The zero vector \( \zero \) is \( \Q \)--conjugate to every vector in \( \R^d \).
        \(
            \qf{\zero}{\Q}[\v]
            =
            \qf{\v}{\Q}[\zero]
            = 0,
            \quad \forall \v \in \R^d
        \)
\end{itemize}

\begin{definition}{Mutually \( \Q \)--conjugate vectors}{}
    A set of distinct vectors \( \set{ \u_i }_{i = 1}^n, \; \u_i \in \R^d, \; \u_i \neq \u_j, \; \forall i \in \set{1, 2, \ldots, n} \) are said to be \textbf{mutually conjugate with respect to \( \Q \)} \text{ OR } \textbf{pairwise/mutually \( \Q \)--conjugate} if
    \vspace{-0.5em}
    \begin{equation*}
        \qf{\u_i}{\Q}[\u_j] = 0,
        \quad \forall i, j \in \set{1, 2, \ldots, n}, \; i \neq j
    \end{equation*}
\end{definition}

\begin{theorem}{Linear independence of non-zero mutually \( \Q \)--conjugate vectors}{}
    Any set of non-zero mutually \( \Q \)--conjugate vectors are linearly independent.
\end{theorem}

\begin{proof}
    Let \( \set{ \u_i }_{i = 1}^n \) be a set of non-zero mutually \( \Q \)--conjugate vectors.
    Define \( \z \triangleq \sum_{i = 1}^{n} \beta_i \u_i, \; \beta_i \in \R \).

    Then, for any \( l \in \set{1, 2, \ldots, n} \), we have
    \begin{equation*}
        \qf{\u_l}{\Q}[\z]
        =
        \qf{\u_l}{\Q}[\pbrac{\sum_{i = 1}^{n} \beta_i \u_i}]
        =
        \sum_{i = 1}^{n} \beta_i \qf{\u_l}{\Q}[\u_i]
        =
        \beta_l \qf{\u_l}{\Q}
        + \sum_{i \neq l} \beta_i \cancel{ \qf{\u_l}{\Q}[\u_i] }
        =
        \beta_l \qf{\u_l}{\Q}
    \end{equation*}

    From this, we can see that if \( \z = \zero \), then \( \qf{\u_l}{\Q}[\z] = \zero = \beta_l \qf{\u_l}{\Q} \).
    Since \( \qf{\u_l}{\Q} > 0, \because \Q \succ \zero, \u_l \neq \zero \), we have that \( \beta_i = 0, \; \forall i \in \set{1, 2, \ldots, n} \).
    Thereby, the vectors \( \set{ \u_i }_{i = 1}^n \) are linearly independent.
\end{proof}

\begin{proof}
    Proof by contradiction.

    Suppose \( \set{ \u_i }_{i = 1}^n \) are linearly dependent.
    Then, there exists some \( l \in \set{1, 2, \ldots, n} \) such that
    \begin{equation*}
        \u_l
        =
        \sum_{i \neq l} \beta_i \u_i,
        \quad \beta_i \in \R
    \end{equation*}

    Then, we have
    \begin{equation*}
        \qf{\u_l}{\Q}
        =
        \qf{\pbrac{\sum_{i \neq l} \beta_i \u_i}}{\Q}[\u_l]
        =
        \sum_{i \neq l} \beta_i \qf{\u_i}{\Q}[\u_l]
        =
        \sum_{i \neq l} \beta_i \cdot 0
        =
        0
    \end{equation*}

    However, since \( \Q \succ \zero, \u_l \neq \zero \), we have \( \qf{\u_l}{\Q} > 0 \), which is a contradiction.

    Thereby, the vectors \( \set{ \u_i }_{i = 1}^n \) are linearly independent.
\end{proof}

\begin{corollary}{}{}
    Any set of non-zero mutually orthogonal vectors are linearly independent.
\end{corollary}

\begin{theorem}{}{}
    The eigenvectors of \( \Q \) are mutually \( \Q \)--conjugate.
\end{theorem}

\begin{proof}
    Let \( \set{ \lambda_i }_{i = 1}^d, \set{ \v_i }_{i = 1}^d, \; \lambda_i > 0 \) be the eigenvalues and eigenvectors of \( \Q \in \PD \), as in~\psecref{sec:spectral-decomposition-theorem}.
    \begin{equation*}
        \implies
        \qf{\v_i}{\Q}[\v_j]
        =
        \dotp{\v_i}{(\lambda_j \v_j)}
        =
        \lambda_j \dotp{\v_i}{\v_j}
        =
        0,
        \quad \because
        \dotp{\v_i}{\v_j} = 0,
        \quad \forall i, j \in \set{1, 2, \ldots, d},
        \; i \neq j
    \end{equation*}
\end{proof}

\section{Conjugate direction methods}

\begin{algorithm}[H]
    \caption{
        Conjugate direction algorithm for unconstrained minimisation
        of a differentiable function \( f : \R^d \to \R \), \( f \in \classC^1 \).
    }\label{alg:conjugate-direction-method}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);

        A set of \( d \) mutually \( \Q \)--conjugate descent directions \( \set{\u_i}_{i = 0}^{d - 1}, \; \u_i \in \R^d, \; \forall i \in \set{0, 1, \ldots, d - 1} \)\;
        \vspace{-0.5em}
        \begin{equation*}
            \qf{\u_i}{\Q}[\u_j] = 0,
            \quad \forall i, j \in \set{0, 1, \ldots, d - 1}, \; i \neq j
        \end{equation*}
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \x^k \) is not optimal}{
        Choose step length \( \alpha_k \) by line search along direction \( \u_k \)\;

        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \u_k \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\begin{algorithm}[H]
    \caption{
        Conjugate direction algorithm for unconstrained minimisation of a strongly convex quadratic function \( f: \R^d \to \R \).
        \vspace{-1em}
        \begin{equation*}
            \func{f}{\x}
            =
            \half \qf{\x}{\Q} + \dotp{\h}{\x},
            \quad \Q \in \PD, \; \h \in \R^d
        \end{equation*}
    }\label{alg:conjugate-direction-method-quadratic}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);

        A set of \( d \) mutually \( \Q \)--conjugate descent directions \( \set{\u_i}_{i = 0}^{d - 1}, \; \u_i \in \R^d, \; \forall i \in \set{0, 1, \ldots, d - 1} \)\;
        \vspace{-0.5em}
        \begin{equation*}
            \qf{\u_i}{\Q}[\u_j] = 0,
            \quad \forall i, j \in \set{0, 1, \ldots, d - 1}, \; i \neq j
        \end{equation*}
    }
    \KwOut{
        Exact solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( k < d \) \emph{ AND } \( \x^k \) is not optimal}{
        Choose step length \( \alpha_k \) by exact line search along direction \( \u_k \) as
        \vspace{-0.5em}
        \begin{equation*}
            \alpha_k
            =
            - \frac{\dotp{\grad{f}{\x^k}}{\u_k}}{\qf{\u_k}{\Q}}
        \end{equation*}

        \vspace{-0.5em}
        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \u_k \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\paragraph{Finite termination property:}
For a convex quadratic function \( f: \R^d \to \R \), the conjugate direction method terminates in at most \( d \) iterations.

\begin{theorem}{Expanding subspace theorem~\citep{Nocedal2006}}{expanding-subspace-theorem}
    For a convex quadratic function \( f: \R^d \to \R \), the sequence of iterates \( \set{\x^k}_{k = 0}^{d} \) generated by the conjugate direction method satisfies
    \vspace{-0.5em}
    \begin{equation*}
        \begin{aligned}
            \x^k
            & =
            \argmin_{\x \in S_k} \func{f}{\x}
            \\
            \quad \text{where }
            S_k
            & \triangleq
            \set{\x \given \x = \x^0 + \p, \; \p \in \spanset{\u_i}_{i = 0}^{k - 1}}
        \end{aligned},
        \quad \forall k \in \set{1, 2, \ldots, d}
    \end{equation*}

    \vspace{-0.5em}
    i.e., the current iterate \( \x^k \) is the minimiser of \( f \) over the affine subspace spanned by the initial point \( \x^0 \) and the first \( k \) conjugate directions \( \set{\u_i}_{i = 0}^{k - 1} \), for all \( k \in \set{1, 2, \ldots, d} \).
\end{theorem}

\begin{proof}
    Define
    \begin{equation*}
        \begin{aligned}
            \U^{(k)}
            & \triangleq
            \begin{bmatrix}
                \u_0 & \u_1 & \cdots & \u_{k - 1}
            \end{bmatrix}
            \in \R^{d \times k}
            \\
            \gamma^{(k)}
            & \triangleq
            \begin{bmatrix}
                \gamma_0 & \gamma_1 & \cdots & \gamma_{k - 1}
            \end{bmatrix}^\top
            \in \R^k
        \end{aligned}
        , \quad \forall k \in \set{1, 2, \ldots, d}
    \end{equation*}
    and
    \begin{equation*}
        \func{g}{\gamma^{(k)}}
        \triangleq
        \func{f}{\x^0 + \U^{(k)} \gamma^{(k)}},
        \quad \forall k \in \set{1, 2, \ldots, d}
    \end{equation*}
    Then, since \( \set{\u_i}_{i = 0}^{d - 1} \) form a basis for \( \R^d \), the matrix \( \U^{(k)} \) has full column rank, i.e., \( \rank{\U^{(k)}} = k, \; \forall k \in \set{1, 2, \ldots, d} \).
    Thereby, for any \( \x \in S_k \), there exists a unique \( \gamma^{(k)} \in \R^k \) such that \( \x = \x^0 + \U^{(k)} \gamma^{(k)} \), for all \( k \in \set{1, 2, \ldots, d} \).
    Hence, we can see that the constrained minimisation problem with \( f \) over the affine subspace \( S_k \) is equivalent to the unconstrained minimisation problem with \( g \) over \( \R^k \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \min_{\x \in S_k} \func{f}{\x}
        \equiv
        \min_{\gamma^{(k)} \in \R^k} \func{g}{\gamma^{(k)}},
        \quad \forall k \in \set{1, 2, \ldots, d}
    \end{equation*}

    Applying Taylor's theorem~\psecref{sec:taylor-theorem} to \( g \) at \( \gamma^{(k)} = \zero \), we have
    \begin{align*}
        \func{g}{\gamma^{(k)}}
        & =
        \func{g}{\zero}
        + \dotp{\grad{g}{\zero}}{\gamma^{(k)}}
        + \half \qf{\gamma^{(k)}}{\hess{g}{\zero}},
        \quad \forall k \in \set{1, 2, \ldots, d}
        \\
        \implies
        \grad{g}{\gamma^{(k)}}
        & =
        \dotp{\U^{(k)}}{\grad{f}{\x^0}}
        + \qf{\U^{(k)}}{\Q} \gamma^{(k)},
        \quad \forall k \in \set{1, 2, \ldots, d}
        \\
        \implies
        \hess{g}{\gamma^{(k)}}
        & =
        \qf{\U^{(k)}}{\Q},
        \quad \forall k \in \set{1, 2, \ldots, d}
    \end{align*}

    Since \( \set{\u_i}_{i = 0}^{d - 1} \) are mutually \( \Q \)--conjugate, it follows that \( \U^{(k)} \) is an orthogonal matrix, i.e., \( \dotp{\U^{(k)}}{\U^{(k)}} = \outp{\U^{(k)}}{\U^{(k)}} = \I, \; \forall k \in \set{1, 2, \ldots, d} \).
    Thereby, we have that \( \qf{\U^{(k)}}{\Q} \) is a diagonal matrix with positive diagonal entries, i.e., \( \hess{g}{\gamma^{(k)}} = \diag{\qf{\u_i}{\Q}}_{i = 0}^{k - 1} \succ \zero, \; \because \Q \succ \zero, \; \forall k \in \set{1, 2, \ldots, d} \).
    \begin{align*}
        \implies
        {\gamma^{(k)}}^\ast
        & =
        \argmin_{\gamma^{(k)} \in \R^k} \func{g}{\gamma^{(k)}}
        =
        - \hessinv{g}{\zero} \grad{g}{\zero}
        \\ & =
        - \inv{\bbrac{\qf{\U^{(k)}}{\Q}}} \dotp{\U^{(k)}}{\grad{f}{\x^0}}
        \\
        \implies
        {\gamma^{(k)}}^\ast_i
        & =
        - \frac{\dotp{\u_i}{\grad{f}{\x^0}}}{\qf{\u_i}{\Q}},
        \quad \forall i \in \set{0, 1, \ldots, k - 1}, \; \forall k \in \set{1, 2, \ldots, d}
    \end{align*}
    \begin{align*}
        \because
        \grad{g}{{\gamma^{(k)}}^\ast}
        & =
        \dotp{\U^{(k)}}{\grad{f}{\x^0 + Q \U^{(k)} \gamma}}
        =
        \zero
        \\
        \implies
        \dotp{\U^{(k)}}{\grad{f}{\x^k}}
        & =
        \zero,
        \quad \forall k \in \set{1, 2, \ldots, d}
    \end{align*}
    \begin{equation}\label{eq:conjugate-direction-orthogonality}
        \therefore
        \boxed{
            \dotp{\u_i}{\grad{f}{\x^k}}
            = 0,
            \quad \forall i \in \set{0, 1, \ldots, k - 1}, \; \forall k \in \set{1, 2, \ldots, d}
        }
    \end{equation}
    Thereby, we have that the gradient \( \grad{f}{\x^k} \) is orthogonal to the subspace spanned by the first \( k \) conjugate directions \( \set{\u_i}_{i = 0}^{k - 1} \), for all \( k \in \set{1, 2, \ldots, d} \).
\end{proof}

\section{Conjugate gradient methods}

\subsection{(Preliminary) conjugate gradient method}

\begin{algorithm}[H]
    \caption{
        {}~\citep{Nocedal2006}
        (Preliminary) Conjugate gradient algorithm for unconstrained minimisation of a strongly convex quadratic function \( f: \R^d \to \R \).
        \vspace{-1em}
        \begin{equation*}
            \func{f}{\x}
            =
            \half \qf{\x}{\Q} + \dotp{\h}{\x},
            \quad \Q \in \PD, \; \h \in \R^d
        \end{equation*}
    }\label{alg:preliminary-conjugate-gradient-method}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);
    }
    \KwOut{
        Exact solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \( \u_0 \leftarrow -\grad{f}{\x^0} \)\;

    \While{\( k < d \) \emph{ AND } \( \x^k \) is not optimal}{
        Choose step length \( \alpha_k \) by exact line search along direction \( \u_k \) as
        \vspace{-0.5em}
        \begin{equation*}
            \alpha_k
            =
            - \frac{\dotp{\grad{f}{\x^k}}{\u_k}}{\qf{\u_k}{\Q}}
        \end{equation*}

        \vspace{-0.5em}
        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \u_k \)\;

        Find the new gradient by using either the first-order oracle or the recurrence relation
        \vspace{-0.5em}
        \begin{equation*}
            \grad{f}{\x^{k+1}}
            =
            \grad{f}{\x^k}
            + \alpha_k \Q \u_k
        \end{equation*}
        or the quadratic structure of \( f \)
        \vspace{-0.5em}
        \begin{equation*}
            \grad{f}{\x^{k+1}}
            =
            \Q \x^{k+1} + \h
        \end{equation*}

        Find the next conjugate direction as
        \vspace{-0.5em}
        \begin{equation*}
            \u_{k+1}
            =
            -\grad{f}{\x^{k+1}}
            + \beta_k \u_k
        \end{equation*}
        such that \( \u_{k+1} \) is \( \Q \)--conjugate to \( \u_k \), thereby
        \vspace{-0.5em}
        \begin{equation*}
            \beta_k
            =
            \frac{\qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k]}{\qf{\u_k}{\Q}}
        \end{equation*}

        \vspace{-0.5em}
        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\paragraph{Finite termination property:}
For a convex quadratic function \( f: \R^d \to \R \), the conjugate gradient method terminates in at most \( d \) iterations.

\begin{theorem}{Conjugate gradient theorem:\\ Mutual \( \Q \)--conjugacy of directions in conjugate gradient method~\citep{Nocedal2006}}{}
    The directions \( \set{\u_k}_{k = 0}^{d - 1} \) generated by the conjugate gradient method are mutually \( \Q \)--conjugate.

    For all \( k \in \set{1, 2, \ldots, d - 1} \), we have
    \vspace{-0.5em}
    \begin{equation}\label{eq:conjugate-gradient-linear-spans}
        \spanset{\u_i}_{i = 0}^{k}
        =
        \spanset{\grad{f}{\x^i}}_{i = 0}^{k}
        =
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{k}
    \end{equation}
    \begin{equation}\label{eq:conjugate-gradient-mutual-Q-conjugacy}
        \qf{\u_k}{\Q}[\u_i]
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, k - 1}
    \end{equation}
\end{theorem}

\begin{proof}
    We prove this using mathematical induction.

    By the construction of \( \u_{k+1} \), we have that the successive directions \( \u_k, \u_{k+1} \) are \( \Q \)--conjugate, i.e.,
    \begin{equation}\label{eq:conjugate-gradient-successive-Q-conjugacy}
        \qf{\u_{k+1}}{\Q}[\u_k]
        = 0,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
    \end{equation}

    \begin{align*}
        \because
        \x^{k+1}
        & =
        \x^k + \alpha_k \u_k,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \implies
        \sum_{k = 0}^{d-1} \x^{k+1}
        & =
        \sum_{k = 0}^{d-1} \x^k
        + \sum_{k = 0}^{d-1} \alpha_k \u_k
        \implies
        \boxed{
            \x^d
            =
            \x^0
            + \sum_{k = 0}^{d-1} \alpha_k \u_k
        }
    \end{align*}

    \begin{align*}
        \because
        \grad{f}{\x}
        & =
        \Q \x + \h,
        \quad \forall \x \in \R^d
        \\
        \implies
        \grad{f}{\x^{k+1}}
        & =
        \Q \x^{k+1} + \h
        =
        \Q \pbrac[\big]{\x^k + \alpha_k \u_k} + \h
        =
        \pbrac{\Q \x^k + \h} + \alpha_k \Q \u_k
        \\ & =
        \grad{f}{\x^k} + \alpha_k \Q \u_k,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \implies
        \spanset{\Q \u_k, \grad{f}{\x^{k+1}}}
        & =
        \spanset{\Q \u_k, \grad{f}{\x^k} + \alpha_k \Q \u_k}
        =
        \spanset{\Q \u_k, \grad{f}{\x^k}}
    \end{align*}
    \begin{equation*}
        \therefore
        \boxed{
            \spanset{\Q \u_k, \grad{f}{\x^{k+1}}}
            =
            \spanset{\Q \u_k, \grad{f}{\x^k}},
            \quad \forall k \in \set{0, 1, \ldots, d - 1}
        }
    \end{equation*}

    \subsubsection*{Base case:}
    For \( k = 1 \), we have
    \begin{align*}
        \boxed{
            \spanset{\u_0, \u_1}
        }
        & =
        \spanset{\u_0, -\grad{f}{\x^1} + \beta_0 \u_0}
        =
        \spanset{\u_0, -\grad{f}{\x^1}}
        \\ & =
        \spanset{-\grad{f}{\x^0}, -\grad{f}{\x^1}}
        =
        \boxed{
            \spanset{\grad{f}{\x^0}, \grad{f}{\x^1}}
        }
        \\ & =
        \spanset{\grad{f}{\x^0}, \grad{f}{\x^0} + \alpha_0 \Q \u_0}
        =
        \spanset{\grad{f}{\x^0}, \Q \u_0}
        \\ & =
        \spanset{\grad{f}{\x^0}, -\Q \, \grad{f}{\x^0}}
        =
        \boxed{
            \spanset{\grad{f}{\x^0}, \Q \, \grad{f}{\x^0}}
        }
    \end{align*}
    and from~\peqref{eq:conjugate-gradient-successive-Q-conjugacy}, we have \( \qf{\u_1}{\Q}[\u_0] = 0 \), thereby proving the base case.

    Note that for \( k = 0 \), the first statement~\peqref{eq:conjugate-gradient-linear-spans} is trivial, since
    \begin{equation*}
        \spanset{\u_0}
        =
        \spanset{-\grad{f}{\x^0}}
        =
        \spanset{\grad{f}{\x^0}}
    \end{equation*}

    \subsubsection*{Inductive step:}
    Assume that the statements~\peqref{eq:conjugate-gradient-linear-spans} and~\peqref{eq:conjugate-gradient-mutual-Q-conjugacy} hold for some \( k = m, \; m < d - 1 \), i.e.,
    \begin{equation*}
        \spanset{\u_i}_{i = 0}^{m}
        =
        \spanset{\grad{f}{\x^i}}_{i = 0}^{m}
        =
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{m}
    \end{equation*}
    \begin{equation*}
        \qf{\u_m}{\Q}[\u_i]
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{equation*}

    We want to show that the statements hold for \( k = m + 1 \), i.e.,
    \begin{equation*}
        \spanset{\u_i}_{i = 0}^{m + 1}
        =
        \spanset{\grad{f}{\x^i}}_{i = 0}^{m + 1}
        =
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{m + 1}
    \end{equation*}
    \begin{equation*}
        \qf{\u_{m + 1}}{\Q}[\u_i]
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, m}
    \end{equation*}

    Now,
    \begin{align*}
        \because
        \grad{f}{\x^{m+1}}
        & =
        \grad{f}{\x^m} + \alpha_m \Q \u_m
        \implies
        \grad{f}{\x^{m+1}}
        \in
        \spanset{\grad{f}{\x^m}, \Q \u_m}
        \\
        \Q \u_{m+1}
        & \in
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{m + 1}
        \\
        \grad{f}{\x^{m+1}}
        & \in
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{m + 1},
        \quad \text{ from the inductive hypothesis}
        \\
        \implies
        \spanset{\grad{f}{\x^i}}_{i = 0}^{m + 1}
        & \subseteq
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{m + 1}
        \\
        \because
        \Q^{m+1} \, \grad{f}{\x^0}
        & =
        \Q \pbrac{\Q^m \, \grad{f}{\x^0}}
        \in
        \spanset{\Q \u_i}_{i = 0}^{m}
        =
        \spanset{\grad{f}{\x^i}}_{i = 0}^{m + 1}
    \end{align*}
    \begin{equation*}
        \therefore
        \boxed{
            \spanset{\grad{f}{\x^i}}_{i = 0}^{m + 1}
            =
            \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{m + 1}
        }
    \end{equation*}
    \begin{align*}
        \because
        \u_{m+1}
        & \in
        \spanset{\grad{f}{\x^{m+1}}, \u_m}
        \\
        \implies
        \spanset{\u_i}_{i = 0}^{m + 1}
        & =
        \spanset{\u_i}_{i = 0}^{m} + \spanset{\grad{f}{\x^{m+1}}}
    \end{align*}
    \begin{equation*}
        \therefore
        \boxed{
            \spanset{\u_i}_{i = 0}^{m + 1}
            =
            \spanset{\grad{f}{\x^i}}_{i = 0}^{m + 1}
        }
    \end{equation*}

    Now,
    \begin{align*}
        \qf{\u_{m+1}}{\Q}[\u_i]
        & =
        \qf{-\grad{f}{\x^{m+1}}}{\Q}[\u_i]
        + \beta_m \qf{\u_m}{\Q}[\u_i],
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
        \\
        \because
        \qf{\u_m}{\Q}[\u_i]
        & = 0,
        \quad \text{ from the inductive hypothesis}
        \\
        \implies
        \qf{\u_{m+1}}{\Q}[\u_i]
        & =
        \qf{-\grad{f}{\x^{m+1}}}{\Q}[\u_i],
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
        \\
        \because
        \Q \u_i
        & \in
        \spanset{\Q^j \, \grad{f}{\x^0}}_{j = 0}^{m}
        =
        \spanset{\u_j}_{j = 0}^{m},
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
        \\
        \implies
        \Q \u_i
        & =
        \sum_{j = 0}^{m} \delta_j \, \u_j,
        \quad \delta_j \in \R,
        \; \forall i \in \set{0, 1, \ldots, m - 1}
        \\
        \implies
        \qf{\u_{m+1}}{\Q}[\u_i]
        & =
        \sum_{j = 0}^{m} \delta_j \, \dotp{\grad{f}{\x^{m+1}}}{\u_j},
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{align*}

    From the expanding subspace theorem~\pthmref{thm:expanding-subspace-theorem}~\peqref{eq:conjugate-direction-orthogonality}, we have
    \begin{equation*}
        \dotp{\grad{f}{\x^{m+1}}}{\u_i}
        = \zero,
        \quad \forall i \in \set{0, 1, \ldots, m}
    \end{equation*}
    \begin{equation*}
        \implies
        \qf{\u_{m+1}}{\Q}[\u_i]
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{equation*}

    From~\peqref{eq:conjugate-gradient-successive-Q-conjugacy}, we also have that \( \qf{\u_{m+1}}{\Q}[\u_m] = 0 \).
    Thereby, we finally have
    \begin{equation*}
        \therefore
        \boxed{
            \qf{\u_{m+1}}{\Q}[\u_i]
            = 0,
            \quad \forall i \in \set{0, 1, \ldots, m}
        }
    \end{equation*}

    Hence, we have shown that the statements hold for \( k = m + 1 \), thereby completing the inductive step.
\end{proof}

\begin{corollary}{{}~\citep{Nocedal2006}}{}
    The \( k \)-th direction \( \u_k \) generated by the conjugate gradient method at iteration \( k \) lies in the Krylov subspace of order \( k \) generated by \( \Q \) and the initial gradient \( \grad{f}{\x^0} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \u_k
        \in
        \calK_k(\Q, \nabla f(\x^0))
        =
        \spanset{\Q^i \, \grad{f}{\x^0}}_{i = 0}^{k - 1}
    \end{equation*}
\end{corollary}

\subsection{(Practical) conjugate gradient method}

\begin{algorithm}[H]
    \caption{
        {}~\citep{Nocedal2006}
        (Practical) Conjugate gradient algorithm for unconstrained minimisation of a strongly convex quadratic function \( f: \R^d \to \R \).
        \vspace{-1em}
        \begin{equation*}
            \func{f}{\x}
            =
            \half \qf{\x}{\Q} + \dotp{\h}{\x},
            \quad \Q \in \PD, \; \h \in \R^d
        \end{equation*}
    }\label{alg:practical-conjugate-gradient-method}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);
    }
    \KwOut{
        Exact solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \( \u_0 \leftarrow -\grad{f}{\x^0} \)\;

    \While{\( k < d \) \emph{ AND } \( \x^k \) is not optimal}{
        Choose step length \( \alpha_k \) as
        \vspace{-0.5em}
        \begin{equation*}
            \alpha_k
            =
            \frac{\norm{\grad{f}{\x^k}}_2^2}{\qf{\u_k}{\Q}}
        \end{equation*}

        \vspace{-0.5em}
        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \u_k \)\;

        Find the new gradient by using either the first-order oracle or the recurrence relation
        \vspace{-0.5em}
        \begin{equation*}
            \grad{f}{\x^{k+1}}
            =
            \grad{f}{\x^k}
            + \alpha_k \Q \u_k
        \end{equation*}
        or the quadratic structure of \( f \)
        \vspace{-0.5em}
        \begin{equation*}
            \grad{f}{\x^{k+1}}
            =
            \Q \x^{k+1} + \h
        \end{equation*}

        Find the next conjugate direction as
        \vspace{-0.5em}
        \begin{align*}
            \u_{k+1}
            & =
            -\grad{f}{\x^{k+1}}
            + \beta_k \u_k,
            \\
            \text{where }
            \beta_k
            & =
            \frac{\norm{\grad{f}{\x^{k+1}}}_2^2}{\norm{\grad{f}{\x^k}}_2^2}
        \end{align*}

        \vspace{-0.5em}
        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

\begin{theorem}{{}~\citep{Nocedal2006}}{}
    The algorithms given in~\palgref{alg:preliminary-conjugate-gradient-method} and~\palgref{alg:practical-conjugate-gradient-method} are mathematically equivalent, under exact arithmetic.
\end{theorem}

\begin{proof}
    The only differences between the two algorithms are in the equations used to compute \( \alpha_k \) and \( \beta_k \).

    The equivalence of \( \alpha_k \)'s follows from the expanding subspace theorem~\pthmref{thm:expanding-subspace-theorem}, as
    \begin{align*}
        \because
        \u_k
        & =
        -\grad{f}{\x^k}
        + \beta_{k - 1} \u_{k - 1},
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\
        \implies
        - \dotp{\grad{f}{\x^k}}{\u_k}
        & =
        \norm{\grad{f}{\x^k}}_2^2
        - \beta_{k - 1} \dotp{\grad{f}{\x^k}}{\u_{k-1}},
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\
        \because
        \dotp{\grad{f}{\x^k}}{\u_{k-1}}
        & = 0,
        \quad \forall k \in \set{1, 2, \ldots, d},
        \qquad \text{ from~\peqref{eq:conjugate-direction-orthogonality}}
        \\
        \implies
        - \dotp{\grad{f}{\x^k}}{\u_k}
        & =
        \norm{\grad{f}{\x^k}}_2^2,
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\
        \text{Also, }
        \because
        \u_0
        & =
        -\grad{f}{\x^0}
        \implies
        - \dotp{\grad{f}{\x^0}}{\u_0}
        =
        \norm{\grad{f}{\x^0}}_2^2
        \\
        \implies
        - \dotp{\grad{f}{\x^k}}{\u_k}
        & =
        \norm{\grad{f}{\x^k}}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \because
        \qf{\u_k}{\Q}
        & > 0,
        \quad \forall k \in \set{0, 1, \ldots, d - 1},
        \qquad \because \Q \succ \zero
    \end{align*}
    \begin{equation}\label{eq:conjugate-gradient-step-length-equivalence}
        \therefore
        \boxed{
            \alpha_k
            =
            - \frac{\dotp{\grad{f}{\x^k}}{\u_k}}{\qf{\u_k}{\Q}}
            =
            \frac{\norm{\grad{f}{\x^k}}_2^2}{\qf{\u_k}{\Q}},
            \quad \forall k \in \set{0, 1, \ldots, d - 1}
        }
    \end{equation}

    \begin{align*}
        \because
        \grad{f}{\x^{k+1}}
        & =
        \grad{f}{\x^k}
        + \alpha_k \Q \u_k,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \implies
        \norm{\grad{f}{\x^{k+1}}}_2^2
        & =
        \norm{\grad{f}{\x^k}}_2^2
        + 2 \alpha_k \dotp{\grad{f}{\x^k}}{\Q \u_k}
        + \alpha_k^2 \norm{\Q \u_k}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \text{Now, }
        \because
        \grad{f}{\x^k}
        & =
        -\u_k
        + \beta_{k - 1} \u_{k - 1},
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\
        \implies
        \qf{\grad{f}{\x^k}}{\Q}[\u_k]
        & =
        -\qf{\u_k}{\Q}
        + \beta_{k - 1} \cancel{ \qf{\u_{k-1}}{\Q}[\u_k] },
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\
        & =
        -\qf{\u_k}{\Q},
        \qquad \text{ from~\peqref{eq:conjugate-gradient-successive-Q-conjugacy}}
        \\
        \text{Also, }
        \because
        \u_0
        & =
        -\grad{f}{\x^0}
        \implies
        \qf{\grad{f}{\x^0}}{\Q}[\u_0]
        =
        -\qf{\u_0}{\Q}
        \\
        \implies
        \qf{\grad{f}{\x^k}}{\Q}[\u_k]
        & =
        -\qf{\u_k}{\Q},
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \implies
        \norm{\grad{f}{\x^{k+1}}}_2^2
        & =
        \norm{\grad{f}{\x^k}}_2^2
        - 2 \alpha_k \qf{\u_k}{\Q}
        + \alpha_k^2 \norm{\Q \u_k}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \because
        \alpha_k \qf{\u_k}{\Q}
        & =
        \norm{\grad{f}{\x^k}}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1},\qquad \text{ from~\peqref{eq:conjugate-gradient-step-length-equivalence}}
        \\
        \implies
        \norm{\grad{f}{\x^{k+1}}}_2^2
        & =
        \cancel{ \norm{\grad{f}{\x^k}}_2^2 }
        - \cancel{2} \norm{\grad{f}{\x^k}}_2^2
        + \alpha_k^2 \norm{\Q \u_k}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\ & =
        - \norm{\grad{f}{\x^k}}_2^2
        + \alpha_k^2 \norm{\Q \u_k}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \text{Now, }
        \qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k]
        & =
        \qf{\grad{f}{\x^k}}{\Q}[\u_k]
        + \alpha_k \norm{\Q \u_k}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\ & =
        -\qf{\u_k}{\Q}
        + \alpha_k \norm{\Q \u_k}_2^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \implies
        \alpha_k \norm{\Q \u_k}_2^2
        & =
        \qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k]
        + \qf{\u_k}{\Q},
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
        \\
        \implies
        \norm{\grad{f}{\x^{k+1}}}_2^2
        & =
        - \norm{\grad{f}{\x^k}}_2^2
        + \alpha_k \qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k]
        + \alpha_k \qf{\u_k}{\Q},
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\ & =
        - \cancel{ \norm{\grad{f}{\x^k}}_2^2 }
        + \alpha_k \qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k]
        + \cancel{ \norm{\grad{f}{\x^k}}_2^2 },
        \quad \forall k \in \set{1, 2, \ldots, d - 1}
        \\
        \implies
        &
        \boxed{
            \norm{\grad{f}{\x^{k+1}}}_2^2
            =
            \alpha_k \qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k],
            \quad \forall k \in \set{0, 1, \ldots, d - 1}
        }
        \\
        \implies
        \norm{\grad{f}{\x^{k+1}}}_2^2
        & =
        \frac{\norm{\grad{f}{\x^k}}_2^2}{\qf{\u_k}{\Q}} \qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k],
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
    \end{align*}
    \begin{equation*}
        \therefore
        \boxed{
            \beta_k
            =
            \frac{\norm{\grad{f}{\x^{k+1}}}_2^2}{\norm{\grad{f}{\x^k}}_2^2}
            =
            \frac{\qf{\grad{f}{\x^{k+1}}}{\Q}[\u_k]}{\qf{\u_k}{\Q}},
            \quad \forall k \in \set{0, 1, \ldots, d - 1}
        }
    \end{equation*}
\end{proof}

\subsection{Preconditioned conjugate gradient method}

\begin{algorithm}[H]
    \caption{
        {}~\citep{Nocedal2006}
        Preconditioned conjugate gradient algorithm for unconstrained minimisation of a strongly convex quadratic function \( f: \R^d \to \R \).
        \vspace{-1em}
        \begin{equation*}
            \func{f}{\x}
            =
            \half \qf{\x}{\Q} + \dotp{\h}{\x},
            \quad \Q \in \PD, \; \h \in \R^d
        \end{equation*}
    }\label{alg:preconditioned-conjugate-gradient-method}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);\\
        Initial point \( \x^0 \in \R^d \);
        Preconditioner \( \M \in \PD \);
    }
    \KwOut{
        Exact solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    Solve \( \M \, \z_0 = -\grad{f}{\x^0} \)\;

    \( \u_0 \leftarrow \z_0 \)\;

    \While{\( k < d \) \emph{ AND } \( \x^k \) is not optimal}{
        Choose step length \( \alpha_k \) as
        \vspace{-0.5em}
        \begin{equation*}
            \alpha_k
            =
            - \frac{\dotp{\grad{f}{\x^k}}{\z_k}}{\qf{\u_k}{\Q}}
        \end{equation*}

        \vspace{-0.5em}
        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \u_k \)\;

        Find the new gradient by using either the first-order oracle or the recurrence relation
        \vspace{-0.5em}
        \begin{equation*}
            \grad{f}{\x^{k+1}}
            =
            \grad{f}{\x^k}
            + \alpha_k \Q \u_k
        \end{equation*}
        or the quadratic structure of \( f \)
        \vspace{-0.5em}
        \begin{equation*}
            \grad{f}{\x^{k+1}}
            =
            \Q \x^{k+1} + \h
        \end{equation*}

        Solve \( \M \, \z_{k+1} = -\grad{f}{\x^{k+1}} \)\;

        Find the next conjugate direction as
        \vspace{-0.5em}
        \begin{align*}
            \u_{k+1}
            & =
            \z_{k+1}
            + \beta_k \u_k,
            \\
            \text{where }
            \beta_k
            & =
            \frac
            {\dotp{\grad{f}{\x^{k+1}}}{\z_{k+1}}}
            {\dotp{\grad{f}{\x^k}}{\z_k}}
        \end{align*}

        \vspace{-0.5em}
        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

Note that the preconditioned conjugate gradient method~\palgref{alg:preconditioned-conjugate-gradient-method} reduces to the (practical) conjugate gradient method~\palgref{alg:practical-conjugate-gradient-method} when \( \M = \I \).

The sequence of iterates \( \set{\x^k}_{k = 0}^{d} \) generated by the preconditioned conjugate gradient method satisfies
\begin{equation*}
    \qf{\grad{f}{\x^i}}{\inv{\M}}[\grad{f}{\x^j}]
    = 0,
    \quad \forall i, j \in \set{0, 1, \ldots, k},
    \; i \neq j,
    \; \forall k \in \set{0, 1, \ldots, d - 1}
\end{equation*}

The preconditioned conjugate gradient method can be interpreted as applying the regular conjugate gradient method to the equivalent optimisation problem with a change of variables
\begin{align*}
    \because
    &
    \M = \outp{\C}{\C},
    \; \y \triangleq \C^\top \x
    \implies
    \func{f}{\x}
    =
    \func{f}{\C^{-\top} \y}
    =
    \half \qf{\y}{\pbrac{\inv{\C} \Q \C^{-\top}}}
    + \dotp{\pbrac{\inv{\C} \h}}{\y}
    \triangleq
    \func{\hat{f}}{\y}
    \\
    \implies
    &
    \func{\hat{f}}{\y}
    =
    \half \qf{\y}{\hat{\Q}} + \dotp{\hat{\h}}{\y},
    \quad
    \hat{\Q} \triangleq \inv{\C} \Q \C^{-\top} \in \PD,
    \; \hat{\h} \triangleq \inv{\C} \h \in \R^d
\end{align*}

NW~\cite{Nocedal2006} provide an equivalent formulation, except that they take a non-singular matrix \( \C \) and construct \( \M = \C^\top \C \), instead of factorising \( \M \) as \( \outp{\C}{\C} \), given a symmetric positive definite preconditioner \( \M \).

\paragraph{Incomplete Cholesky Factorisation:}
\(
    \displaystyle
    \Q
    \approx
    \M
    =
    \outp{\mathbf{L}}{\mathbf{L}},
    \quad
    \mathbf{L} \in \R^{d \times d} \text{ is lower-triangular}
\)

\paragraph{Jacobi Preconditioner:}
\(
    \displaystyle
    \Q
    \approx
    \M
    =
    \diag{\Q}
    \implies
    \inv{\M}
    =
    \frac{1}{\diag{\Q}}
\)

\section{Polynomial optimisation perspective}

Define the polynomial \( P_k^\ast: \bbrac{\;\cdot\;} \to \R \) of degree at most \( k \) as
\begin{equation*}
    P_k^\ast(x)
    \triangleq
    \sum_{i = 0}^{k} \gamma_i \, x^i,
    \quad \gamma_i \in \R,
    \; \forall i \in \set{0, 1, \ldots, k},
    \; \forall k \in \set{0, 1, \ldots, d - 1}
\end{equation*}

\begin{align*}
    \implies
    \x^{k+1}
    & =
    \x^0 + \sum_{i = 0}^{k} \alpha_i \, \u_i
    =
    \x^0 + \sum_{i = 0}^{k} \gamma_i \, \Q^i \, \grad{f}{\x^0}
    \\
    \implies
    &
    \boxed{
        \x^{k+1}
        =
        \x^0 + P_k^\ast(\Q) \, \grad{f}{\x^0},
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
    }
\end{align*}

\begin{align*}
    \implies
    f(\x) - f(\xstar)
    & =
    \pbrac{\half \qf{\x}{\Q} + \dotp{\h}{\x}} - \pbrac{\half \qf{\xstar}{\Q} + \dotp{\h}{\xstar}}
    \\ & =
    \half \qf{\x}{\Q} - \qf{\x}{\Q}[\xstar] + \half \qf{\xstar}{\Q}
    =
    \half \qf{\pbrac{\x - \xstar}}{\Q}
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        \half \norm{\x - \xstar}^2_{\Q}
        =
        f(\x) - f(\xstar),
        \quad \forall \x \in \R^d
    }
\end{equation*}

\begin{align*}
    \implies
    \x^{k+1} - \xstar
    & =
    \x^0 + P_k^\ast(\Q) \, \grad{f}{\x^0} - \xstar
    \\ & =
    (\x^0 - \xstar) + P_k^\ast(\Q) \, \Q \, (\x^0 - \xstar)
    =
    \bbrac{\I + P_k^\ast(\Q) \, \Q} \, (\x^0 - \xstar)
\end{align*}

\begin{equation*}
    \implies
    P_k^\ast
    =
    \argmin_{P_k}
    \norm{\x^0 + P_k(\Q) \, \grad{f}{\x^0} - \xstar}_{\Q}
\end{equation*}

From spectral decomposition theorem~\pthmref{thm:spectral-decomposition-theorem}, we have
\(
    \Q
    =
    \sum_{i = 1}^{d} \lambda_i \, \outp{\v_i}{\v_i}
\).

Since \( \set{\v_i}_{i = 1}^{d} \) forms an orthonormal basis for \( \R^d \), we have
\begin{align*}
    \implies
    \x^0 - \xstar
    & =
    \sum_{i = 1}^{d} \mu_i \, \v_i,
    \quad \mu_i \in \R,
    \; \forall i \in \set{1, 2, \ldots, d}
    \implies
    \norm{\x^0 - \xstar}_{\Q}^2
    =
    \sum_{i = 1}^{d} \lambda_i \, \mu_i^2
    \\
    P_k(\Q) \, \v_i
    & =
    P_k(\lambda_i) \, \v_i,
    \quad \forall i \in \set{1, 2, \ldots, d}
    \\
    \implies
    \x^{k+1} - \xstar
    & =
    \bbrac{\I + P_k^\ast(\Q) \, \Q} \, (\x^0 - \xstar)
    =
    \sum_{i = 1}^{d} \mu_i \, \bbrac{1 + P_k^\ast(\lambda_i) \, \lambda_i} \, \v_i
    \\
    \implies
    \norm{\x^{k+1} - \xstar}_{\Q}^2
    & =
    \sum_{i = 1}^{d} \lambda_i \, \mu_i^2 \, \bbrac{1 + P_k^\ast(\lambda_i) \, \lambda_i}^2
    =
    \min_{P_k}
    \sum_{i = 1}^{d} \lambda_i \, \mu_i^2 \, \bbrac{1 + P_k(\lambda_i) \, \lambda_i}^2
    \\
    \implies
    \norm{\x^{k+1} - \xstar}_{\Q}^2
    & \leq
    \min_{P_k}
    \max_{1 \leq i \leq d}
    \bbrac{1 + P_k(\lambda_i) \, \lambda_i}^2
    \sum_{i = 1}^{d} \lambda_i \, \mu_i^2
\end{align*}
\begin{equation*}
    \therefore
    \boxed{
        \norm{\x^{k+1} - \xstar}_{\Q}^2
        \leq
        \min_{P_k}
        \max_{1 \leq i \leq d}
        \bbrac{1 + P_k(\lambda_i) \, \lambda_i}^2
        \norm{\x^0 - \xstar}_{\Q}^2
    }
\end{equation*}

\begin{theorem}{{}~\citep{Nocedal2006}}{}
    If \( \Q \) has exactly \( s \) distinct eigenvalues, then the CG method converges in at most \( s \) iterations.
\end{theorem}

\begin{proof}
    Let the distinct eigenvalues of \( \Q \) be \( \lambda_1, \lambda_2, \ldots, \lambda_s \).

    Consider the polynomial
    \begin{equation*}
        \Q_s(x)
        =
        {(-1)}^s
        \prod_{i = 1}^{s} \frac{x - \lambda_i}{\lambda_i}
    \end{equation*}
    which is of degree \( s \) and satisfies \( \Q_s(0) = 1 \) and \( \Q_s(\lambda_i) = 0, \; \forall i \in \set{1, 2, \ldots, s} \).

    \begin{align*}
        \implies
        0
        & \leq
        \min_{P_{s - 1}}
        \max_{1 \leq i \leq d}
        \bbrac{1 + P_k(\lambda_i) \, \lambda_i}^2
        \leq
        \max_{1 \leq i \leq d}
        \bbrac{\Q_s(\lambda_i)}^2
        =
        0
        \\
        \implies
        \norm{\x^s - \xstar}_{\Q}^2
        & \leq
        0
        \implies
        \x^s = \xstar
    \end{align*}
\end{proof}

\begin{theorem}{{}~\citep{Nocedal2006}}{}
    If \( Q \) has eigenvalues \( 0 < \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_d \), then the CG method generates iterates that satisfy
    \vspace{-0.5em}
    \begin{equation*}
        \norm{\x^{k+1} - \xstar}_{\Q}^2
        \leq
        \pbrac{\frac{\lambda_{d-k} - \lambda_1}{\lambda_{d-k} + \lambda_1}}^2
        \norm{\x^0 - \xstar}_{\Q}^2,
        \quad \forall k \in \set{0, 1, \ldots, d - 1}
    \end{equation*}
\end{theorem}

\begin{corollary}{{}~\citep{Nocedal2006}}{}
    If \( Q \) has eigenvalues \( 0 < \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_d \), then the CG method generates iterates that satisfy
    \vspace{-0.5em}
    \begin{equation*}
        \norm{\x^{k} - \xstar}_{\Q}
        \leq
        2 \bbrac{\frac{\sqrt{\kappa(\Q)} - 1}{\sqrt{\kappa(\Q)} + 1}}^k
        \norm{\x^0 - \xstar}_{\Q},
        \quad \forall k \in \set{0, 1, \ldots, d}
    \end{equation*}

    \vspace{-0.5em}
    where \( \kappa(\Q) = \frac{\lambda_d}{\lambda_1} \) is the condition number of \( \Q \).
\end{corollary}

\chapter{Second-order methods}

\section{Newton's method}

\begin{algorithm}[H]
    \caption{
        {}~\citep{Nesterov2004}
        Newton's method for unconstrained minimisation.
    }\label{alg:newtons-method}
    \SetAlgoLined{}
    \KwIn{
        Second-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^0 \in \R^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \hess{f}{\x^k} \succ \zero \) \emph{ AND } \( \x^k \) is not optimal}{
        Update the current point:
        \vspace{-1em}
        \begin{equation*}
            \x^{k+1}
            =
            \x^k - \hessinv{f}{\x} \grad{f}{\x^k}
        \end{equation*}

        \vspace{-1em}
        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

Newton's method requires the Hessian matrix to be positive definite at every iteration to ensure that the computed direction is indeed a descent direction.

\paragraph{Newton direction:}
Refers to the descent direction
\begin{equation*}
    \p^k = -\hessinv{f}{\x^k} \grad{f}{\x^k},
    \qquad \hess{f}{\x^k} \succ \zero
\end{equation*}

This satisfies the condition for descent direction since
\begin{align*}
    \hess{f}{\x^k}
    &
    \succ \zero
    \implies
    \hessinv{f}{\x^k}
    \succ \zero
    \\
    \implies
    \dotp{\grad{f}{\x^k}}{\p^k}
    & =
    - \pbrac{\grad{f}{\x^k}}^\top \hessinv{f}{\x^k} \grad{f}{\x^k}
    < 0
\end{align*}

From the fundamental theorem of calculus~\psecref{sec:fundamental-theorem-of-calculus}, with \( \func{g}{t} \triangleq \grad{f}{\x + t(\y - \x)}, \; t \in [0, 1] \), we have
\begin{equation}\label{eq:gradient-difference-via-hessian-integral}
    \func{g}{1} - \func{g}{0}
    =
    \int_{0}^{1} \func{g'}{t} \, dt
    \implies
    \boxed{
        \grad{f}{\y} - \grad{f}{\x}
        =
        \int_{0}^{1} \hess{f}{\x + t(\y - \x)} (\y - \x) \, dt,
        \quad \forall \x, \y \in \R^d
    }
\end{equation}

For a stationary point \( \xstar \), i.e., \( \grad{f}{\xstar} = \zero \), we have
\begin{equation*}
    \implies
    \grad{f}{\x}
    =
    \int_{0}^{1} \hess{f}{\xstar + t\pbrac{\x - \xstar}} \pbrac{\x - \xstar} \, dt,
    \quad \forall \x \in \R^d
\end{equation*}

Suppose \( \hess{f}{\xstar} \succ \zero \), ensuring that \( \xstar \) is a local minimum.
\begin{equation*}
    \implies
    \exists l > 0 \text{ such that } \hess{f}{\xstar} \succeq l \, \I
\end{equation*}

Assuming \( f \in \classC^2_M \), we have
\begin{equation*}
    \boxed{
        \norm{\hess{f}{\y} - \hess{f}{\x}}
        \leq
        M \norm{\y - \x},
        \quad \forall \x, \y \in \R^d
    }
\end{equation*}

From Taylor's theorem, we have
\begin{equation*}
    \func{f}{\y}
    =
    \func{f}{\x}
    + \dotp{\grad{f}{\x}}{(\y - \x)}
    + \half \qf{(\y - \x)}{\hess{f}{\x}}
    + \smalloh{\norm{\y - \x}^2},
    \quad \forall \x, \y \in \R^d
\end{equation*}

Taking the derivative with respect to \( \y \) on both sides, we have
\begin{equation*}
    \implies
    \grad{f}{\y}
    =
    \grad{f}{\x}
    + \hess{f}{\x} (\y - \x)
    + \smalloh{\norm{\y - \x}},
    \quad \forall \x, \y \in \R^d
\end{equation*}

\begin{align*}
    \implies
    \norm{
        \grad{f}{\y}
        - \grad{f}{\x}
        - \hess{f}{\x} (\y - \x)
    }
    & =
    \norm{
        \int_{0}^{1} \pbrac[\Big]{\hess{f}{\x + t(\y - \x)} - \hess{f}{\x}} (\y - \x) \, dt
    }
    \\ & \leq
    \int_{0}^{1} \norm{
        \pbrac[\Big]{\hess{f}{\x + t(\y - \x)} - \hess{f}{\x}} (\y - \x)
    } \, dt
    \\ & \leq
    \int_{0}^{1}
    \norm{\hess{f}{\x + t(\y - \x)} - \hess{f}{\x}}
    \norm{\y - \x}
    \, dt
    \\ & \leq
    \int_{0}^{1}
    M \norm{\cancel{\x} + t(\y - \x) - \cancel{\x}}
    \norm{\y - \x}
    \, dt
    \\ & =
    \int_{0}^{1}
    M
    \norm{\y - \x}^2
    \, t
    \, dt
    =
    M
    \norm{\y - \x}^2
    \int_{0}^{1}
    t
    \, dt
    =
    \frac{M}{2}
    \norm{\y - \x}^2
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        \norm{
            \grad{f}{\y}
            - \grad{f}{\x}
            - \hess{f}{\x} (\y - \x)
        }
        \leq
        \frac{M}{2} \norm{\y - \x}^2,
        \quad \forall \x, \y \in \R^d
    }
\end{equation*}

Now, since \( - \norm{\A} \, \I \preceq \A \preceq \norm{\A} \, \I, \quad \forall \A \in \SD \), we have
\begin{align*}
    - \norm{\hess{f}{\y} - \hess{f}{\x}} \, \I
    \preceq
    \hess{f}{\y} - \hess{f}{\x}
    \preceq
    \norm{\hess{f}{\y} - \hess{f}{\x}} \, \I,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    - M \norm{\y - \x} \, \I
    \preceq
    \hess{f}{\y} - \hess{f}{\x}
    \preceq
    M \norm{\y - \x} \, \I,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \boxed{
        \hess{f}{\x} - M \norm{\y - \x} \, \I
        \preceq
        \hess{f}{\y}
        \preceq
        \hess{f}{\x} + M \norm{\y - \x} \, \I,
        \quad \forall \x, \y \in \R^d
    }
\end{align*}

Let \( r_k \triangleq \norm{\x^k - \xstar} \).
Then, we have
\begin{align*}
    \implies
    &
    \hess{f}{\xstar} - M r_k \, \I
    \preceq
    \hess{f}{\x^k}
    \preceq
    \hess{f}{\xstar} + M r_k \, \I
    \\
    \implies
    &
    (l - M r_k) \, \I
    \preceq
    \hess{f}{\x^k}
    \\
    \therefore
    \quad
    &
    \text{If }
    (l - M r_k) > 0
    \implies
    \hess{f}{\x^k}
    \succ
    \zero
    \\
    \text{i.e.,}
    \quad
    &
    \text{If }
    \boxed{
        r_k < \frac{l}{M}
    }
    \implies
    \hess{f}{\x^k}
    \succ
    \zero
    \\
    \text{Also, }
    \quad
    &
    (l - M r_k) \, \I
    \preceq
    \hess{f}{\x^k}
    \implies
    \frac{1}{(l - M r_k)} \, \I
    \succeq
    \hessinv{f}{\x^k}
    \\
    \text{i.e.,}
    \quad
    &
    \hessinv{f}{\x^k}
    \preceq
    \frac{1}{(l - M r_k)} \, \I
    \implies
    \boxed{
        \norm{\hessinv{f}{\x^k}}
        \leq
        \frac{1}{(l - M r_k)}
    }
\end{align*}

Now,
\begin{align*}
    r_{k+1}
    & =
    \norm{\x^{k+1} - \xstar}
    =
    \norm{\pbrac{\x^k - \hessinv{f}{\x^k} \grad{f}{\x^k}} - \xstar}
    \\ & =
    \norm{\x^k - \xstar - \hessinv{f}{\x^k} \grad{f}{\x^k}}
    \\ & =
    \norm{\hessinv{f}{\x^k} \pbrac[\Big]{\hess{f}{\x^k} \pbrac{\x^k - \xstar} - \grad{f}{\x^k}}}
    \\ & \leq
    \norm{\hessinv{f}{\x^k}} \,
    \norm{\hess{f}{\x^k} \pbrac{\x^k - \xstar} - \grad{f}{\x^k}}
\end{align*}
\begin{align*}
    \implies
    &
    \norm{\hess{f}{\x^k} \pbrac{\x^k - \xstar} - \grad{f}{\x^k}}
    \\ & =
    \norm{\hess{f}{\x^k} \pbrac{\x^k - \xstar} - \int_{0}^{1} \hess{f}{\xstar + t\pbrac{\x^k - \xstar}} \pbrac{\x^k - \xstar} \, dt}
    \\ & =
    \norm{\int_{0}^{1} \hess{f}{\x^k} \pbrac{\x^k - \xstar}  \, dt - \int_{0}^{1} \hess{f}{\xstar + t\pbrac{\x^k - \xstar}} \pbrac{\x^k - \xstar} \, dt}
    \\ & =
    \norm{\int_{0}^{1} \pbrac[\Big]{\hess{f}{\x^k} - \hess{f}{\xstar + t\pbrac{\x^k - \xstar}}} \pbrac{\x^k - \xstar} \, dt}
    \\ & \leq
    \int_{0}^{1} \norm{\pbrac[\Big]{\hess{f}{\x^k} - \hess{f}{\xstar + t\pbrac{\x^k - \xstar}}} \pbrac{\x^k - \xstar}} \, dt
    \\ & \leq
    \int_{0}^{1} \norm[\Big]{\hess{f}{\x^k} - \hess{f}{\xstar + t\pbrac{\x^k - \xstar}}} \, \norm{\x^k - \xstar} \, dt
    \\ & \leq
    \int_{0}^{1} M \norm[\Big]{\x^k - \xstar - t\pbrac{\x^k - \xstar}} \, \norm{\x^k - \xstar} \, dt
    \\ & =
    \int_{0}^{1} M (1 - t) \norm{\x^k - \xstar}^2 \, dt
    \\ & =
    M
    \norm{\x^k - \xstar}^2
    \int_{0}^{1} (1 - t) \, dt
    =
    \frac{M}{2}
    r_k^2
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        r_{k+1}
        \leq
        \frac{M}{2}
        \norm{\hessinv{f}{\x^k}}
        r_k^2
    }
    \implies
    r_{k+1}
    \leq
    \frac{M}{2 (l - M r_k)}
    r_k^2
\end{equation*}

Thereby, rate of convergence for Newton's method is quadratic.

Now, we want \( r_{k+1} < r_k \) to ensure successive iterates converge to \( \xstar \), thereby we get
\begin{equation*}
    \frac{M}{2 (l - M r_k)}
    r_k^2
    <
    r_k
    \implies
    M r_k
    <
    2 l - 2 M r_k
    \implies
    3 M r_k
    <
    2 l
    \implies
    \boxed{
        r_k
        <
        \frac{2 l}{3 M}
    }
\end{equation*}

Note that \( \displaystyle r_k < \frac{2 l}{3 M} \implies r_k < \frac{l}{M} \), ensuring \( \hess{f}{\x^k} \succ \zero \).

Thereby, if we start with \( \displaystyle \boxed{ r_0 < \frac{2 l}{3 M} } \), then since \( \displaystyle r_k < r_{k - 1} < \cdots < r_1 < r_0 < \frac{2 l}{3 M} \), this ensures that \( \displaystyle r_k < \frac{2 l}{3 M} \).
Thereby, we have \( \hess{f}{\x^k} \succ \zero \) throughout the iterations, ensuring that \( \p^k \) is a descent direction, thereby the algorithm converges to the local minimum \( \xstar \).

\chapter{Quasi-Newton methods}

From Taylor's theorem~\psecref{sec:taylor-theorem}, we have
\begin{equation*}
    \grad{f}{\y}
    =
    \grad{f}{\x}
    + \hess{f}{\x} \, \pbrac{\y - \x}
    + \smalloh{\norm{\y - \x}_2}
\end{equation*}

For \( \x^{k+1} \) close to \( \x^k \), we have
\begin{equation*}
    \grad{f}{\x^{k+1}}
    \approx
    \grad{f}{\x^k}
    + \hess{f}{\x^k} \, \pbrac{\x^{k+1} - \x^k}
\end{equation*}

Consider the update
\begin{align*}
    \x^{k+1}
    & =
    \x^k + \alpha_k \, \u_k,
    \quad \alpha_k > 0
    \\
    \u_k
    & =
    - \A_k \, \grad{f}{\x^k},
    \quad \A_k \succ \zero
\end{align*}

\paragraph{Quasi-Newton condition:}
\begin{equation}\label{eq:quasi-newton-condition}
    \boxed{
        \A_{k+1} \, \bfgamma_k
        =
        \bfdelta_k
    },
    \quad
    \bfgamma_k
    \triangleq
    \grad{f}{\x^{k+1}} - \grad{f}{\x^k},
    \quad
    \bfdelta_k
    \triangleq
    \x^{k+1} - \x^k
\end{equation}

\begin{algorithm}[H]
    \caption{
        {}~\citep{Nesterov2004}
        Quasi-Newton method for unconstrained minimisation.
    }\label{alg:quasi-newton-method}
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);\\
        Initial point \( \x^0 \in \R^d \);
        Initial inverse Hessian approximation \( \A_0 \succ \zero \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \x^k \) is not optimal}{
        Compute the search direction: \( \p^k = - \A_k \, \grad{f}{\x^k}, \; \A_k \succ \zero \)\;

        Perform a line search to determine a suitable step size \( \alpha_k > 0 \)\;

        Update the current point: \( \x^{k+1} = \x^k + \alpha_k \, \p^k \)\;

        Update the inverse Hessian approximation \( \A_{k+1} \) using a Quasi-Newton update\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \x^k \)\;}
\end{algorithm}

Note that, for strongly convex quadratic functions~\psecref{sec:strongly-convex-quadratic-minimisation-problem}, from Taylor's theorem~\psecref{sec:taylor-theorem}, we have
\begin{align*}
    \because
    \grad{f}{\y}
    & =
    \grad{f}{\x}
    + \hess{f}{\x} \, \pbrac{\y - \x}
    =
    \grad{f}{\x}
    + \Q \, \pbrac{\y - \x},
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \grad{f}{\x^{k+1}}
    & =
    \grad{f}{\x^k}
    + \Q \, \pbrac{\x^{k+1} - \x^k}
\end{align*}
\begin{equation}\label{eq:secant-equation-quadratic}
    \implies
    \boxed{
        \bfgamma_k
        =
        \Q \, \bfdelta_k,
        \quad \forall k \geq 0
    }
\end{equation}
which is known as the \textbf{secant equation}.

\begin{table}[htbp]
    \centering
    \caption{
        Quasi-Newton methods.
        \quad
        \(
            \bfgamma_k
            \triangleq
            \grad{f}{\x^{k+1}} - \grad{f}{\x^k},
            \quad
            \bfdelta_k
            \triangleq
            \x^{k+1} - \x^k
        \)
    }\label{tab:quasi-newton-methods}
    \begin{tabular}{lll}
        \toprule
        \textbf{Method}
        &
        Inverse Hessian approximation \( \A_{k+1} = \)
        &
        Hessian approximation \( \B_{k+1} = \)
        \\
        \midrule
        SR1~\psecref{sec:sr1}
        &
        \(
            \displaystyle
            \A_k
            +
            \frac{
                \outp
                {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
                {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
            }
            {
                \dotp
                {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
                {\bfgamma_k}
            }
        \)
        &
        \(
            \displaystyle
            \B_k
            +
            \frac{
                \outp
                {\pbrac{\bfgamma_k - \B_k \, \bfdelta_k}}
                {\pbrac{\bfgamma_k - \B_k \, \bfdelta_k}}
            }
            {
                \dotp
                {\pbrac{\bfgamma_k - \B_k \, \bfdelta_k}}
                {\bfdelta_k}
            }
        \)
        \\
        \midrule
        DFP~\psecref{sec:dfp}
        &
        \(
            \displaystyle
            \A_k
            +
            \frac
            {\outp{\bfdelta_k}{\bfdelta_k}}
            {\dotp{\bfdelta_k}{\bfgamma_k}}
            -
            \frac
            {\A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k}
            {\qf{\bfgamma_k}{\A_k}}
        \)
        &
        \(
            \displaystyle
            \pbrac{
                \I
                -
                \frac
                {\outp{\bfgamma_k}{\bfdelta_k}}
                {\dotp{\bfgamma_k}{\bfdelta_k}}
            }
            \B_k
            \pbrac{
                \I
                -
                \frac
                {\outp{\bfdelta_k}{\bfgamma_k}}
                {\dotp{\bfgamma_k}{\bfdelta_k}}
            }
            +
            \frac
            {\outp{\bfgamma_k}{\bfgamma_k}}
            {\dotp{\bfgamma_k}{\bfdelta_k}}
        \)
        \\
        \midrule
        BFGS
        &
        \(
            \displaystyle
            \pbrac{
                \I
                -
                \frac
                {\outp{\bfdelta_k}{\bfgamma_k}}
                {\dotp{\bfgamma_k}{\bfdelta_k}}
            }
            \A_k
            \pbrac{
                \I
                -
                \frac
                {\outp{\bfgamma_k}{\bfdelta_k}}
                {\dotp{\bfgamma_k}{\bfdelta_k}}
            }
            +
            \frac
            {\outp{\bfdelta_k}{\bfdelta_k}}
            {\dotp{\bfgamma_k}{\bfdelta_k}}
        \)
        &
        \(
            \displaystyle
            \B_k
            +
            \frac
            {\outp{\bfgamma_k}{\bfgamma_k}}
            {\dotp{\bfgamma_k}{\bfdelta_k}}
            -
            \frac
            {\outp{\pbrac{\B_k \bfdelta_k}}{\pbrac{\B_k \bfdelta_k}}}
            {\qf{\bfdelta_k}{\B_k}}
        \)
        \\
        \midrule
        Broyden family
        &
        \(
            \displaystyle
            (1 - \varphi_k) \, \A_{k+1}^{\text{BFGS}}
            +
            \varphi_k \, \A_{k+1}^{\text{DFP}},
            \quad \varphi \in [0, 1]
        \)
        &
        \(
            \displaystyle
            (1 - \varphi_k) \, \B_{k+1}^{\text{BFGS}}
            +
            \varphi_k \, \B_{k+1}^{\text{DFP}},
            \quad \varphi \in [0, 1]
        \)
        \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Symmetric Rank-1 (SR1) update}\label{sec:sr1}

Choose the update so that \( \A_k \) is symmetric, and
\begin{equation*}
    \A_{k+1}
    =
    \A_k
    + \beta \, \outp{\v}{\v},
    \quad \beta \in \R,
    \; \v \in \R^d
\end{equation*}
\begin{align*}
    \because
    \bfdelta_k
    =
    \A_{k+1} \, \bfgamma_k
    & =
    \A_k \, \bfgamma_k
    + \beta \, \outp{\v}{\v} \, \bfgamma_k
    \\
    \implies
    \bfdelta_k
    - \A_k \, \bfgamma_k
    & =
    \beta \pbrac{\dotp{\v}{\bfgamma_k}} \, \v
\end{align*}

One such solution to the above equation is
\begin{align*}
    &
    \v
    =
    \bfdelta_k
    - \A_k \, \bfgamma_k,
    \qquad
    \beta \pbrac{\dotp{\v}{\bfgamma_k}}
    =
    1
    \\
    \implies
    &
    \boxed{
        \v
        =
        \bfdelta_k
        - \A_k \, \bfgamma_k,
        \qquad
        \beta
        =
        \frac{1}{
            \dotp{\pbrac{\bfdelta_k
            - \A_k \, \bfgamma_k}}
            {\bfgamma_k}
        }
    }
\end{align*}

Hence, we have the SR1 update as
\begin{equation}\label{eq:sr1}
    \boxed{
        \A_{k+1}
        =
        \A_k
        +
        \frac{
            \outp
            {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
            {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
        }{
            \dotp
            {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
            {\bfgamma_k}
        }
    }
    \triangleq
    \A_{k+1}^{\text{SR1}}
\end{equation}

Note that \( \A_{k+1} \succ \zero \) if \( \A_k \succ \zero \) and \( \beta > 0 \), but \( \beta > 0 \) does not necessarily hold true.\\
The SR1 update does not guarantee positive definiteness of \( \A_{k+1} \).

Since the denominator in~\peqref{eq:sr1} can be very small or zero, we only use the SR1 update when
\begin{equation*}
    \abs{
        \dotp
        {\pbrac{\bfdelta_k - \A_k \, \bfgamma_k}}
        {\bfgamma_k}
    }
    \geq
    \rho
    \, \norm{\bfdelta_k - \A_k \, \bfgamma_k}
    \, \norm{\bfgamma_k},
    \quad
    \text{for some small }
    \rho \in (0, 1)
\end{equation*}

\begin{theorem}{}{}
    For strongly convex quadratic functions~\psecref{sec:strongly-convex-quadratic-minimisation-problem},
    the iterates generated in the SR1 update \( \set{\x^i}_{i = 0}^{k+1} \) satisfy
    \vspace{-0.5em}
    \begin{equation*}
        \A_{k+1} \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, k}
    \end{equation*}
\end{theorem}

\begin{proof}
    By mathematical induction.

    \paragraph{Base case:}
    For \( k = 0 \), the statement
    \(
        \A_1 \, \bfgamma_0
        =
        \bfdelta_0
    \)
    holds true by the Quasi-Newton condition~\peqref{eq:quasi-newton-condition}.

    \paragraph{Induction step:}
    Assume that the statement holds true for \( k = m - 1 \), i.e.,
    \begin{equation*}
        \A_m \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{equation*}
    We need to show that the statement holds true for \( k = m \), i.e.,
    \begin{equation*}
        \A_{m + 1} \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m}
    \end{equation*}

    Now, for any \( i \in \set{0, 1, \ldots, m - 1} \), we have
    \begin{align*}
        \implies
        \dotp
        {\pbrac{\bfdelta_m - \A_m \, \bfgamma_m}}
        {\bfgamma_i}
        & =
        \dotp
        {\bfdelta_m}
        {\bfgamma_i}
        -
        \dotp
        {\pbrac{\A_m \, \bfgamma_m}}
        {\bfgamma_i}
        =
        \dotp
        {\bfdelta_m}
        {\bfgamma_i}
        -
        \qf
        {\bfgamma_m}
        {\A_m}
        [\bfgamma_i]
        =
        \dotp
        {\bfdelta_m}
        {\bfgamma_i}
        -
        \dotp
        {\bfgamma_m}
        {\bfdelta_i}
        \\ & =
        \dotp
        {\bfdelta_m}
        {\pbrac{\Q \bfdelta_i}}
        -
        \dotp
        {\pbrac{\Q \bfdelta_m}}
        {\bfdelta_i}
        =
        \cancel{
            \qf
            {\bfdelta_m}
            {\Q}
            [\bfdelta_i]
        }
        -
        \cancel{
            \qf
            {\bfdelta_m}
            {\Q}
            [\bfdelta_i]
        }
        = 0
    \end{align*}
    where we used the secant equation~\peqref{eq:secant-equation-quadratic} with \( k = i \) and \( k = m \), and the induction hypothesis.

    \begin{equation*}
        \implies
        \boxed{
            \dotp
            {\pbrac{\bfdelta_m - \A_m \, \bfgamma_m}}
            {\bfgamma_i}
            = 0,
            \quad \forall i \in \set{0, 1, \ldots, m - 1}
        }
    \end{equation*}
    \begin{equation*}
        \implies
        \A_{m + 1} \, \bfgamma_i
        =
        \pbrac{
            \A_m
            +
            \frac{
                \outp
                {\pbrac{\bfdelta_m - \A_m \, \bfgamma_m}}
                {\pbrac{\bfdelta_m - \A_m \, \bfgamma_m}}
            }{
                \dotp
                {\pbrac{\bfdelta_m - \A_m \, \bfgamma_m}}
                {\bfgamma_m}
            }
        } \, \bfgamma_i
        =
        \A_m \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{equation*}

    From the Quasi-Newton condition~\peqref{eq:quasi-newton-condition} for \( k = m \), we have
    \(
        \A_{m + 1} \, \bfgamma_m
        =
        \bfdelta_m
    \).

    Combining with the above result, we finally have
    \(
        \quad \therefore
        \A_{m + 1} \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m}
    \).

    Hence, the induction step holds true.

    Thus, by mathematical induction, the result holds true for all \( k \geq 0 \).
\end{proof}

\section{Davidon-Fletcher-Powell (DFP) update}\label{sec:dfp}

Choose the update so that \( \A_k \) is symmetric, and
\begin{equation*}
    \A_{k+1}
    =
    \A_k
    + \beta_1 \, \outp{\u}{\u}
    + \beta_2 \, \outp{\v}{\v},
    \quad \beta_1, \beta_2 \in \R,
    \; \u, \v \in \R^d
\end{equation*}
\begin{align*}
    \because
    \bfdelta_k
    =
    \A_{k+1} \, \bfgamma_k
    & =
    \A_k \, \bfgamma_k
    + \beta_1 \, \outp{\u}{\u} \, \bfgamma_k
    + \beta_2 \, \outp{\v}{\v} \, \bfgamma_k
    \\
    \implies
    \bfdelta_k
    - \A_k \, \bfgamma_k
    & =
    \beta_1 \pbrac{\dotp{\u}{\bfgamma_k}} \, \u
    +
    \beta_2 \pbrac{\dotp{\v}{\bfgamma_k}} \, \v
\end{align*}

One such solution to the above equation is
\begin{align*}
    &
    \u
    =
    \bfdelta_k,
    \quad
    \v
    =
    \A_k \, \bfgamma_k,
    \qquad
    \beta_1 \pbrac{\dotp{\u}{\bfgamma_k}}
    =
    1,
    \quad
    \beta_2 \pbrac{\dotp{\v}{\bfgamma_k}}
    =
    -1
    \\
    \implies
    &
    \boxed{
        \u
        =
        \bfdelta_k,
        \quad
        \v
        =
        \A_k \, \bfgamma_k,
        \qquad
        \beta_1
        =
        \frac{1}{\dotp{\bfdelta_k}{\bfgamma_k}},
        \quad
        \beta_2
        =
        -\frac{1}{\qf{\bfgamma_k}{\A_k}}
    }
\end{align*}

Hence, we have the DFP update as
\begin{equation*}
    \boxed{
        \A_{k+1}
        =
        \A_k
        +
        \frac
        {\outp{\bfdelta_k}{\bfdelta_k}}
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        -
        \frac
        {\A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k}
        {\qf{\bfgamma_k}{\A_k}}
    }
    \triangleq
    \A_{k+1}^{\text{DFP}}
\end{equation*}

The DFP update guarantees positive definiteness of \( \A_{k+1} \) if \( \A_k \succ \zero \) and \( \dotp{\bfdelta_k}{\bfgamma_k} > 0 \).

The condition \( \dotp{\bfdelta_k}{\bfgamma_k} > 0 \) is known as the \textit{curvature condition}, and is satisfied if \( f \) is strongly convex and \( \alpha_k \) is chosen using exact line search or an inexact line search that satisfies the Wolfe conditions.

\begin{theorem}{DFP positive definiteness}{}
    If \( \A_0 \) is symmetric positive-definite and \( \dotp{\bfdelta_k}{\bfgamma_k} > 0, \; \forall k \), then the DFP update preserves positive definiteness, i.e., if \( \A_k \succ \zero \) and \( \dotp{\bfdelta_k}{\bfgamma_k} > 0 \), then \( \A_{k+1} \succ \zero \).
\end{theorem}

\begin{proof}
    Since \( \A_k \succ \zero \), we have the Cholesky decomposition \( \A_k = \outp{\mathbf{L}_k}{\mathbf{L}_k} \), where \( \mathbf{L}_k \) is a lower triangular matrix with positive diagonal entries.
    Setting \( \a = \dotp{\mathbf{L}_k}{\z}, \; \b = \dotp{\mathbf{L}_k}{\bfgamma_k} \), where \( \z \in \R^d \setminus \set{\zero} \), we have
    \begin{align*}
        \implies
        \qf{\z}{\A_{k+1}}
        & =
        \qf{\z}{\A_k}
        +
        \qf{\z}{
            \frac
            {\outp{\bfdelta_k}{\bfdelta_k}}
            {\dotp{\bfdelta_k}{\bfgamma_k}}
        }
        -
        \qf{\z}{
            \frac
            {\A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k}
            {\qf{\bfgamma_k}{\A_k}}
        }
        \\ & =
        \qf{\z}{\outp{\mathbf{L}_k}{\mathbf{L}_k}}
        +
        \frac
        {\pbrac{\dotp{\bfdelta_k}{\z}}^2}
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        -
        \qf{\z}{
            \frac
            {\outp{\mathbf{L}_k}{\mathbf{L}_k} \outp{\bfgamma_k}{\bfgamma_k} \outp{\mathbf{L}_k}{\mathbf{L}_k}}
            {\qf{\bfgamma_k}{\outp{\mathbf{L}_k}{\mathbf{L}_k}}}
        }
        =
        \frac
        {\pbrac{\dotp{\bfdelta_k}{\z}}^2}
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        +
        \bbrac{
            \dotp{\a}{\a}
            -
            \frac{\pbrac{\dotp{\a}{\b}}^2}{\dotp{\b}{\b}}
        }
    \end{align*}

    From Cauchy-Schwarz inequality~\psecref{sec:cauchy-schwarz-inequality}, we have
    \begin{equation*}
        \dotp{\a}{\a}
        -
        \frac{\pbrac{\dotp{\a}{\b}}^2}{\dotp{\b}{\b}}
        \geq 0,
        \quad \forall \a, \b \in \R^d,
        \; \b \neq \zero,
        \quad
        \text{with equality iff } \a = \lambda \b,
        \; \lambda \in \R
    \end{equation*}

    Since \( \mathbf{L}_k^\top \) is invertible, and thereby it's nullspace is \( \set{\zero} \), which implies \( \b = \zero \iff \bfgamma_k = \zero \).\\
    But \( \bfgamma_k = \zero \implies \dotp{\bfdelta_k}{\bfgamma_k} = 0 \), a contradiction.
    Thus, \( \bfgamma_k \neq \zero \), and thereby \( \b \neq \zero \).

    In the equality case, we have that \( \a = \lambda \b, \; \lambda \in \R \), and since \( \mathbf{L}_k^\top \) is invertible, thereby
    \begin{align*}
        \a
        =
        \lambda \b
        &
        \iff
        \dotp{\mathbf{L}_k}{\z}
        =
        \lambda
        \dotp{\mathbf{L}_k}{\bfgamma_k}
        \iff
        \z
        =
        \lambda \bfgamma_k
        \\
        \text{Now, }
        \because
        \z
        &
        \neq
        \zero,
        \;
        \bfgamma_k
        \neq
        \zero
        \implies
        \lambda
        \neq
        0
        \\
        \implies
        \dotp{\bfdelta_k}{\z}
        & =
        \lambda
        \dotp{\bfdelta_k}{\bfgamma_k}
        \neq 0,
        \quad
        \because
        \dotp{\bfdelta_k}{\bfgamma_k} > 0,
        \; \lambda \neq 0
        \\
        \implies
        \qf{\z}{\A_{k+1}}
        & =
        \overbrace{
            \frac
            {\pbrac{\dotp{\bfdelta_k}{\z}}^2}
            {\dotp{\bfdelta_k}{\bfgamma_k}}
        }^{> 0}
        +
        \overbrace{
            \bbrac{
                \dotp{\a}{\a}
                -
                \frac{\pbrac{\dotp{\a}{\b}}^2}{\dotp{\b}{\b}}
            }
        }^{= 0}
        > 0,
        \quad \forall \z \in \spanset{\bfgamma_k} \setminus \set{\zero}
    \end{align*}

    In the strict inequality case, we have \( \z \notin \spanset{\bfgamma_k} \), thereby
    \begin{equation*}
        \qf{\z}{\A_{k+1}}
        =
        \overbrace{
            \frac
            {\pbrac{\dotp{\bfdelta_k}{\z}}^2}
            {\dotp{\bfdelta_k}{\bfgamma_k}}
        }^{\geq 0}
        +
        \overbrace{
            \bbrac{
                \dotp{\a}{\a}
                -
                \frac{\pbrac{\dotp{\a}{\b}}^2}{\dotp{\b}{\b}}
            }
        }^{> 0}
        > 0,
        \quad \forall \z \notin \spanset{\bfgamma_k}
    \end{equation*}

    Combining both the cases, we have \( \qf{\z}{\A_{k+1}} > 0, \; \forall \z \in \R^d \setminus \set{\zero} \implies \A_{k+1} \succ \zero \).

    Thus, the DFP update preserves positive definiteness.
\end{proof}

\begin{theorem}{}{}
    The DFP update generates a sequence of \( \Q \)--conjugate directions on strongly convex quadratic functions~\psecref{sec:strongly-convex-quadratic-minimisation-problem},i.e., the directions generated in the DFP update \( \set{\u^i}_{i = 0}^{k+1} \) satisfy
    \vspace{-1em}
    \begin{equation*}
        \qf{\u^{k+1}}{\Q}[\u^i]
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, k}
    \end{equation*}
    and the iterates \( \set{\x^i}_{i = 0}^{k+1} \) satisfy
    \vspace{-0.5em}
    \begin{equation*}
        \A_{k+1} \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, k}
    \end{equation*}
\end{theorem}

\begin{proof}
    By mathematical induction.

    \paragraph{Base case:}
    For \( k = 0 \), the statement
    \(
        \A_1 \, \bfgamma_0
        =
        \bfdelta_0
    \)
    holds true by the Quasi-Newton condition~\peqref{eq:quasi-newton-condition}.
    \begin{align*}
        \text{Now, }
        \qf{\u^1}{\Q}[\u^0]
        & =
        \qf{\pbrac{-\A_1 \grad{f}{\x^1}}}{\Q}[\u^0]
        =
        -\qf{\grad{f}{\x^1}}{\A_1 \Q}[\u^0]
        \\
        \because
        \x^1
        & =
        \x^0 + \alpha_0 \, \u^0
        \implies
        \u^0
        =
        \frac{1}{\alpha_0} \pbrac{\x^1 - \x^0}
        =
        \frac{1}{\alpha_0} \bfdelta_0
        \\
        \implies
        \qf{\u^1}{\Q}[\u^0]
        & =
        -\frac{1}{\alpha_0} \qf{\grad{f}{\x^1}}{\A_1 \Q}[\bfdelta_0]
        =
        -\frac{1}{\alpha_0} \qf{\grad{f}{\x^1}}{\A_1}[\bfgamma_0]
        \\ & =
        -\frac{1}{\alpha_0} \dotp{\grad{f}{\x^1}}{\bfdelta_0}
        =
        -\dotp{\grad{f}{\x^1}}{\u^0}
        = 0
    \end{align*}
    where we have used the secant equation~\peqref{eq:secant-equation-quadratic} and the Quasi-Newton condition~\peqref{eq:quasi-newton-condition} for \( k = 0 \), and the expanding subspace theorem~\pthmref{thm:expanding-subspace-theorem} holds since the directions \( \u^0, \u^1 \) are \( \Q \)--conjugate.

    Hence, the base case holds true.

    \paragraph{Induction step:}
    Assume that the result holds true for \( k = m - 1 \), i.e.,
    \begin{align*}
        \qf{\u^{m-1}}{\Q}[\u^i]
        & = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 2}
        \\
        \A_m \, \bfgamma_i
        & =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{align*}
    We need to show that the result holds true for \( k = m \), i.e.,
    \begin{align*}
        \qf{\u^m}{\Q}[\u^i]
        & = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
        \\
        \A_{m+1} \, \bfgamma_i
        & =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m}
    \end{align*}

    Now, for any \( i \in \set{0, 1, \ldots, m - 1} \), we have
    \begin{align*}
        \qf{\u^m}{\Q}[\u^i]
        & =
        \qf{\pbrac{-\A_m \grad{f}{\x^m}}}{\Q}[\u^i]
        =
        -\qf{\grad{f}{\x^m}}{\A_m \Q}[\u^i]
        \\
        \because
        \x^{i + 1}
        & =
        \x^i + \alpha_i \, \u^i
        \implies
        \u^i
        =
        \frac{1}{\alpha_i} \pbrac{\x^{i + 1} - \x^i}
        =
        \frac{1}{\alpha_i} \bfdelta_i
        \\
        \implies
        \qf{\u^m}{\Q}[\u^i]
        & =
        -\frac{1}{\alpha_i} \qf{\grad{f}{\x^m}}{\A_m \Q}[\bfdelta_i]
        =
        -\frac{1}{\alpha_i} \qf{\grad{f}{\x^m}}{\A_m}[\bfgamma_i]
        \\ & =
        -\frac{1}{\alpha_i} \dotp{\grad{f}{\x^m}}{\bfdelta_i}
        =
        -\dotp{\grad{f}{\x^m}}{\u^i}
        = 0,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{align*}
    where we have used the secant equation~\peqref{eq:secant-equation-quadratic} with \( k = i \), the induction hypothesis, and the expanding subspace theorem~\pthmref{thm:expanding-subspace-theorem} holds since the directions \( \set{\u^i}_{i = 0}^{m-1} \) are mutually \( \Q \)--conjugate.

    Similarly, for any \( i \in \set{0, 1, \ldots, m - 1} \), we have
    \begin{align*}
        \implies
        \dotp{\bfdelta_m}{\bfgamma_i}
        & =
        \qf{\bfdelta_m}{\Q}[\bfdelta_i]
        =
        \qf{\pbrac{\alpha_m \u^m}}{\Q}[\pbrac{\alpha_i \u^i}]
        =
        \alpha_i \alpha_m
        \qf{\u^m}{\Q}[\u^i]
        = 0
        \\
        \implies
        \qf{\bfgamma_m}{\A_m}[\bfgamma_i]
        & =
        \dotp{\bfgamma_m}{\bfdelta_i}
        =
        \dotp{\pbrac{\Q \u^m}}{\pbrac{\alpha_i \u^i}}
        =
        \alpha_i
        \qf{\u^m}{\Q}[\u^i]
        = 0
    \end{align*}
    where we used the secant equation~\peqref{eq:secant-equation-quadratic} with \( k = i \), the induction hypothesis, and the result above.
    \begin{equation*}
        \implies
        \boxed{
            \dotp{\bfdelta_m}{\bfgamma_i}
            =
            \qf{\bfgamma_m}{\A_m}[\bfgamma_i]
            =
            0,
            \quad \forall i \in \set{0, 1, \ldots, m - 1}
        }
    \end{equation*}
    \begin{equation*}
        \implies
        \A_{m + 1} \, \bfgamma_i
        =
        \pbrac{
            \A_m
            +
            \frac
            {\outp{\bfdelta_m}{\bfdelta_m}}
            {\dotp{\bfdelta_m}{\bfgamma_m}}
            -
            \frac
            {\A_m \outp{\bfgamma_m}{\bfgamma_m} \A_m}
            {\qf{\bfgamma_m}{\A_m}}
        } \, \bfgamma_i
        =
        \A_m \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m - 1}
    \end{equation*}

    From the Quasi-Newton condition~\peqref{eq:quasi-newton-condition} for \( k = m \), we have
    \(
        \A_{m + 1} \, \bfgamma_m
        =
        \bfdelta_m
    \).

    Combining with the above result, we finally have
    \(
        \quad \therefore
        \A_{m + 1} \, \bfgamma_i
        =
        \bfdelta_i,
        \quad \forall i \in \set{0, 1, \ldots, m}
    \).

    Hence, the induction step holds true.

    Thus, by mathematical induction, the result holds true for all \( k \geq 0 \).
\end{proof}

\section{Broyden-Fletcher-Goldfarb-Shanno (BFGS) update}

The BFGS update is the dual of the DFP update, i.e., it updates the Hessian approximation \( \B_k \) instead of the inverse Hessian approximation \( \A_k \).

\section{Broyden family}

Choose the update so that \( \A_k \) is symmetric, and
\begin{equation*}
    \A_{k+1}
    =
    \A_k
    + \beta_1 \, \outp{\bfdelta_k}{\bfdelta_k}
    + \beta_2 \, \pbrac{
        \bfdelta_k \dotp{\bfgamma_k}{\A_k}
        + \outp{\A_k \, \bfgamma_k}{\bfdelta_k}
    }
    + \beta_3 \, \A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k,
    \quad \beta_1, \beta_2, \beta_3 \in \R
\end{equation*}

\begin{align*}
    \because
    \bfdelta_k
    & =
    \A_{k+1} \, \bfgamma_k
    \\
    \implies
    \bfdelta_k
    - \A_k \, \bfgamma_k
    & =
    \beta_1 \outp{\bfdelta_k}{\bfdelta_k} \bfgamma_k
    + \beta_2 \pbrac{
        \bfdelta_k \dotp{\bfgamma_k}{\A_k} \bfgamma_k
        + \A_k \, \bfgamma_k \dotp{\bfdelta_k}{\bfgamma_k}
    }
    + \beta_3 \, \A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k \bfgamma_k
    \\ & =
    \pbrac{
        \beta_1 \dotp{\bfdelta_k}{\bfgamma_k}
        + \beta_2 \qf{\bfgamma_k}{\A_k}
    } \bfdelta_k
    + \pbrac{
        \beta_2 \dotp{\bfdelta_k}{\bfgamma_k}
        + \beta_3 \qf{\bfgamma_k}{\A_k}
    } \A_k \, \bfgamma_k
\end{align*}

\begin{align*}
    \implies
    \beta_1 \dotp{\bfdelta_k}{\bfgamma_k}
    + \beta_2 \qf{\bfgamma_k}{\A_k}
    & = 1
    \\
    \beta_2 \dotp{\bfdelta_k}{\bfgamma_k}
    + \beta_3 \qf{\bfgamma_k}{\A_k}
    & = -1
\end{align*}
which is a system of 2 linear equations in 3 unknowns \( \beta_1, \beta_2, \beta_3 \), and hence has infinitely many solutions.

One such solution to the above system is
\begin{equation*}
    \boxed{
        \beta_1
        =
        \frac{1}{\dotp{\bfdelta_k}{\bfgamma_k}}
        \pbrac{1 + \phi \, \frac{\qf{\bfgamma_k}{\A_k}}{\dotp{\bfdelta_k}{\bfgamma_k}}},
        \quad
        \beta_2
        =
        -\frac{\phi}{\dotp{\bfdelta_k}{\bfgamma_k}},
        \quad
        \beta_3
        =
        -\frac{\pbrac{1 - \phi}}{\qf{\bfgamma_k}{\A_k}},
        \quad
        \; \phi \in [0, 1]
    }
\end{equation*}

Observe that for \( \phi = 0 \), we get the DFP update~\psecref{sec:dfp}.

The Broyden family of updates is given by
\begin{align*}
    \A_{k+1}
    & =
    \A_k
    +
    \frac{1}{\dotp{\bfdelta_k}{\bfgamma_k}}
    \pbrac{1 + \phi \, \frac{\qf{\bfgamma_k}{\A_k}}{\dotp{\bfdelta_k}{\bfgamma_k}}}
    \, \outp{\bfdelta_k}{\bfdelta_k}
    \\ & \qquad -
    \frac{\phi}{\dotp{\bfdelta_k}{\bfgamma_k}}
    \, \pbrac{
        \bfdelta_k \dotp{\bfgamma_k}{\A_k}
        + \outp{\A_k \, \bfgamma_k}{\bfdelta_k}
    }
    -
    \frac{\pbrac{1 - \phi}}{\qf{\bfgamma_k}{\A_k}}
    \, \A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k
    \\ & =
    \pbrac{
        \A_k
        +
        \frac
        {\outp{\bfdelta_k}{\bfdelta_k}}
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        -
        \frac
        {\A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k}
        {\qf{\bfgamma_k}{\A_k}}
    }
    \\ & \qquad +
    \phi \, \pbrac{
        \frac
        {\qf{\bfgamma_k}{\A_k}}
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        \frac
        {\outp{\bfdelta_k}{\bfdelta_k}}
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        -
        \frac
        {
            \bfdelta_k \dotp{\bfgamma_k}{\A_k}
            +
            \outp{\A_k \, \bfgamma_k}{\bfdelta_k}
        }
        {\dotp{\bfdelta_k}{\bfgamma_k}}
        +
        \frac
        {\A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k}
        {\qf{\bfgamma_k}{\A_k}}
    }
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        \A_{k+1}
        =
        \A_{k+1}^{\text{DFP}}
        +
        \phi \, \outp{\v}{\v},
        \quad
        \v
        =
        \sqrt{\qf{\bfgamma_k}{\A_k}}
        \bbrac{
            \frac
            {\bfdelta_k}
            {\dotp{\bfdelta_k}{\bfgamma_k}}
            -
            \frac
            {\A_k \outp{\bfgamma_k}{\bfgamma_k} \A_k}
            {\qf{\bfgamma_k}{\A_k}}
        }
    }
\end{equation*}

Thereby, the Broyden family is of the form
\begin{equation*}
    \A_{k+1}
    =
    \A_k + \phi \B_k \D \B_k^\top,
    \qquad
    \B_k
    =
    \begin{bmatrix}
        \bfdelta_k & \A_k \, \bfgamma_k
    \end{bmatrix}_{d \times 2},
    \quad
    \D
    =
    \begin{bmatrix}
        \beta_1 & \beta_2 \\
        \beta_3 & \beta_4
    \end{bmatrix}_{2 \times 2} \in \R^{2 \times 2},
    \quad
    \phi \in [0, 1]
\end{equation*}
