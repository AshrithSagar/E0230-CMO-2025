
\part{Unconstrained optimisation}

\chapter{Introduction}

\section{Optimisation methods}

\subsection{First-order methods}

Assume \( f \in \C^{1} \), and use \( \func{f}{\x + \p} = \func{f}{\x} + \dotp{\grad{f}{\x}}{\p} + o(\normbf{p}) \).

\subsection{Second-order methods}

Assume \( f \in \C^{2} \), and use \( \func{f}{\x + \p} = \func{f}{\x} + \dotp{\grad{f}{\x}}{\p} + \half \qf{\p}{\hess{f}{\x}} + o(\normbf{p}^2) \).

\section{Types of minimum}

\subsection{Global minimum}

\begin{definition}{Global minimum}{global-minimum}
    The point \( \xstar \in \R^d \) is a \textbf{global minimum} of the function \( f: D \subseteq \R^d \to \R \) if
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x}, \quad \forall \x \in D
    \end{equation*}
\end{definition}

\subsection{Local minimum}

\begin{definition}{Local minimum}{}
    The point \( \xstar \in \R^d \) is a \textbf{local minimum} of the function \( f: \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \) in the \( \delta \)-neighborhood of \( \xstar \), we have \( \func{f}{\xstar} \leq \func{f}{\x} \), i.e.,
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x}, \quad \forall \x \in B_{\delta}(\xstar)
    \end{equation*}
\end{definition}

\subsection{Strict local minimum}

\begin{definition}{Strict local minimum}{}
    The point \( \xstar \in \R^d \) is a \textbf{strict local minimum} of the function \( f: \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \) in the \( \delta \)-neighborhood of \( \xstar \) except \( \xstar \) itself, we have \( \func{f}{\xstar} < \func{f}{\x} \), i.e.,
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x}, \quad \forall \x \in B_{\delta}(\xstar) \setminus \{ \xstar \}
    \end{equation*}
\end{definition}

\section{Necessary and sufficient conditions}

\subsection{First-order necessary condition for a local minimum}

\begin{theorem}{First-order necessary condition for a local minimum}{first-order-necessary-condition-for-a-local-minimum}
    For a function \( f \in \C^{1} \), if \( \xstar \) is a local minimum of \( f \), then \( \grad{f}{\xstar} = \zero \).
\end{theorem}

\subsection{Second-order necessary condition for a local minimum}

\begin{theorem}{Second-order necessary condition for a local minimum}{}
    For a function \( f \in \C^{2} \), if \( \xstar \) is a local minimum of \( f \), then \( \grad{f}{\xstar} = \zero, \; \hess{f}{\xstar} \succeq \zero \).
\end{theorem}

\subsection{Second-order sufficient condition for a strict local minimum}

\begin{theorem}{Second-order sufficient condition for a strict local minimum}{}
    For a function \( f \in \C^{2} \) such that \( \grad{f}{\xstar} = \zero, \; \hess{f}{\xstar} \succ \zero \), then \( \xstar \) is a strict local minimum of \( f \).
\end{theorem}

\subsection{First-order sufficient condition for a global minimum under convexity}

\begin{theorem}{First-order sufficient condition for a global minimum under convexity}{}
    For a convex function \( f \in \C^1 \) with \( \grad{f}{\xstar} = \zero \), then \( \xstar \) is a global minimum of \( f \).
\end{theorem}

\begin{proof}
    From the first-order condition for convexity~\pthmref{thm:first-order-condition-for-convexity}, we have
    \begin{align*}
        \func{f}{\y}
        & \geq
        \func{f}{\xstar} + \cancel{ \dotp{\grad{f}{\xstar}}{(\y - \xstar)} }
        , \quad \forall \y \in D
        \\
        \implies
        \func{f}{\y}
        & \geq
        \func{f}{\xstar}
        , \quad \forall \y \in D
    \end{align*}
    which is the definition of a global minimum~\pdefref{def:global-minimum}.
\end{proof}

\section{Convexity}

\begin{definition}{Convex set}{}
    A set \( S \subseteq \R^n \) is \textbf{convex} if
    \begin{equation*}
        \lambda \x + (1 - \lambda) \y \in S,
        \quad \forall \x, \y \in S,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\begin{definition}{Convex function}{}
    A function $f: D \subseteq \R^n \to \R$ is \textbf{convex} on a convex set \( D \) if
    \begin{equation*}
        \func{f}{\lambda \x + (1 - \lambda) \y}
        \leq
        \lambda \func{f}{\x} + (1 - \lambda) \func{f}{\y},
        \quad \forall \x, \y \in D,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\subsection{First-order condition for convexity}

\begin{theorem}{First-order condition for convexity}{first-order-condition-for-convexity}
    For a function \( f \in \C^1 \) with a convex domain \( D \), \( f \) is convex on \( D \) iff
    \begin{equation*}
        \func{f}{\y}
        \geq
        \func{f}{\x} + \dotp{\grad{f}{\x}}{(\y - \x)},
        \quad \forall \x, \y \in D
    \end{equation*}
\end{theorem}

\begin{proof}
    \( (\Rightarrow) \)
    Rearranging the definition of convexity, we get
    \begin{equation*}
        \frac{\func{f}{\lambda \x + (1 - \lambda) \y} - \func{f}{\x}}{\lambda}
        \leq
        \func{f}{\y} - \func{f}{\x},
        \quad \forall \x, \y \in D,
        \; \lambda \in \linterval{0}{1}
    \end{equation*}
    Taking the limit as \( \lambda \to 0^+ \), we have
    \begin{equation*}
        \dotp{\grad{f}{\x}}{(\y - \x)}
        =
        \lim_{\lambda \to 0^+} \frac{\func{f}{\lambda \x + (1 - \lambda) \y} - \func{f}{\x}}{\lambda}
        \leq
        \func{f}{\y} - \func{f}{\x},
        \quad \forall \x, \y \in D
    \end{equation*}
    which is the desired result.
\end{proof}

\begin{corollary}{Monotonicity of gradient \( \iff \) Convexity}{}
    For a function \( f \in \C^1 \) with a convex domain \( D \), \( f \) is convex on \( D \) iff
    \begin{equation*}
        \dotp{\big( \grad{f}{\x} - \grad{f}{\y} \big)}{\big( \x - \y \big)} \geq 0
        , \quad
        \forall \x, \y \in D
    \end{equation*}
\end{corollary}

\begin{proof}
    \( (\Rightarrow) \)
    From the first-order condition for convexity, we have
    \begin{align*}
        \func{f}{\y}
        & \geq
        \func{f}{\x} + {\grad{f}{\x}}^\top (\y - \x)
        , \quad
        \forall \x, \y \in D
        \\
        \func{f}{\x}
        & \geq
        \func{f}{\y} + {\grad{f}{\y}}^\top (\x - \y)
        , \quad
        \forall \x, \y \in D
    \end{align*}
    Adding the two inequalities, we get
    \begin{equation*}
        0 \geq {\big( \grad{f}{\x} - \grad{f}{\y} \big)}^\top (\y - \x)
        , \quad
        \forall \x, \y \in D
    \end{equation*}
    which is equivalent to the desired result.

    \( (\Leftarrow) \)
    Define \( \func{g}{t} = f(\x + t (\y - \x)), \;\; t \in [0, 1], \;\; \x, \y \in D \).

    Then, \( \func{g}{0} = \func{f}{\x}, \quad \func{g}{1} = \func{f}{\y}, \quad \func{g'}{t} = \dotp{(\grad{f}{\x + t (\y - \x)})}{(\y - \x)} \).

    From the given condition, we have
    \begin{align*}
        \func{g'}{t}
        & =
        {\big( \nabla f(\x + t (\y - \x)) - \grad{f}{\x} + \grad{f}{\x} \big)}^\top (\y - \x)
        \\ & =
        {\big( \nabla f(\x + t (\y - \x)) - \grad{f}{\x} \big)}^\top (\y - \x) + {\big( \grad{f}{\x} \big)}^\top (\y - \x)
        \geq
        {\big( \grad{f}{\x} \big)}^\top (\y - \x)
    \end{align*}
    for all \( t \in [0, 1] \).
    Integrating from \( 0 \) to \( 1 \), we get
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x}
        =
        \func{g}{1} - \func{g}{0}
        =
        \int_{0}^{1} \func{g'}{t} \, dt
        \geq
        \int_{0}^{1} {\big( \grad{f}{\x} \big)}^\top (\y - \x) \, dt
        =
        {\big( \grad{f}{\x} \big)}^\top (\y - \x)
    \end{equation*}
    for all \( \x, \y \in D \), which is the first-order condition for convexity, and thereby \( f \) is convex.
\end{proof}

\subsection{Second-order condition for convexity}

\begin{theorem}{Second-order condition for convexity}{second-order-condition-for-convexity}
    For a function \( f \in \C^2 \) with a convex domain \( D \), \( f \) is convex on \( D \) iff
    \begin{equation*}
        \hess{f}{\x} \succeq \zero
        , \quad
        \forall \x \in D
    \end{equation*}
\end{theorem}

\begin{proof}
    \( (\Rightarrow) \)
    The first-order condition for convexity can be rewritten as
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x) \geq 0
        , \quad
        \forall \x, \y \in D
    \end{equation*}
    Using Taylor's theorem, we have
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x)
        =
        \half {(\y - \x)}^\top \hess{f}{\boldsymbol{\xi}} (\y - \x)
        , \quad
        \boldsymbol{\xi} = (1 - t) \x + t \y, \; t \in (0, 1)
    \end{equation*}
    for some \( t \in (0, 1) \).
    Therefore, we have
    \begin{equation*}
        {(\y - \x)}^\top \hess{f}{\boldsymbol{\xi}} (\y - \x) \geq 0
        , \quad
        \forall \x, \y \in D
    \end{equation*}
    which implies \( \hess{f}{\x} \succeq \zero, \; \forall \x \in D \).
    (Note: \( \boldsymbol{\xi} \in D \) since \( D \) is convex.)

    \( (\Leftarrow) \)
    From Taylor's theorem, we have
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x)
        =
        \half {(\y - \x)}^\top \hess{f}{\boldsymbol{\xi}} (\y - \x)
        , \quad
        \boldsymbol{\xi} = (1 - t) \x + t \y, \; t \in (0, 1)
    \end{equation*}
    for some \( t \in (0, 1) \).
    Since \( \hess{f}{\x} \succeq \zero, \; \forall \x \in D \), we have
    \begin{equation*}
        \func{f}{\y} - \func{f}{\x} - {\grad{f}{\x}}^\top (\y - \x) \geq 0
        , \quad
        \forall \x, \y \in D
    \end{equation*}
    which is the first-order condition for convexity, and thereby \( f \) is convex.
\end{proof}

\section{Strong convexity}

\begin{definition}{Strong convexity}{}
    A function \( f: D \subseteq \R^n \to \R \) on a convex domain \( D \) is \textbf{strongly convex} with parameter \( \mu > 0 \) if
    \begin{equation*}
        \func{f}{\lambda \x + (1 - \lambda) \y}
        \leq
        \lambda \func{f}{\x} + (1 - \lambda) \func{f}{\y}
        - \frac{\mu}{2} \lambda (1 - \lambda) \normbf{x - y}^2,
        \quad \forall \x, \y \in \R^n, \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\begin{corollary}{}{}
    A function \( f: D \subseteq \R^n \to \R \) on a convex domain \( D \) is strongly convex with parameter \( \mu > 0 \) iff the function \( \func{g}{\x} := \left( \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 \right) \) is convex on \( D \).
\end{corollary}

\begin{proof}
    Follows from the identity:
    \begin{align*}
        \lambda (1 - \lambda) \normbf{x - y}^2
        & =
        \lambda (1 - \lambda) \Big( \norm{\y}^2 + \norm{\x}^2 - 2 \dotp{\x}{\y} \Big)
        \\ & =
        \lambda \norm{\y}^2 + \lambda \norm{\x}^2 - \lambda^2 \norm{\y}^2 - \norm{\lambda \x}^2 - 2 {(\lambda \x)}^\top \big( (1 - \lambda) \y \big)
        \\ & =
        \lambda \norm{\y}^2 + \lambda \norm{\x}^2 - \lambda^2 \norm{\y}^2 - \norm{\lambda \x + (1 - \lambda) \y}^2 + {(1 - \lambda)}^2 \norm{\y}^2
        \\ & =
        \cancel{ \lambda \norm{\y}^2 } + \lambda \norm{\x}^2 - \cancel{ \lambda^2 \norm{\y}^2 } - \norm{\lambda \x + (1 - \lambda) \y}^2 + \norm{\y}^2 - \cancel{ 2 } \lambda \norm{\y}^2 + \cancel{ \lambda^2 \norm{\y}^2 }
        \\ & =
        \lambda \norm{\x}^2 + (1 - \lambda) \norm{\y}^2 - \norm{\lambda \x + (1 - \lambda) \y}^2
    \end{align*}
\end{proof}

\subsection{First-order condition for strong convexity}

\begin{corollary}{First-order condition for strong convexity}{}
    For a function \( f \in \C^1 \) with a convex domain \( D \), \( f \) is strongly convex on \( D \) with parameter \( \mu > 0 \) iff
    \begin{equation*}
        \func{f}{\y}
        \geq
        \func{f}{\x} + {\grad{f}{\x}}^\top (\y - \x) + \frac{\mu}{2} \normbf{y - x}^2,
        \quad \forall \x, \y \in D
    \end{equation*}
\end{corollary}

\begin{proof}
    Define \( \func{g}{\x} = \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 \).
    Then, \( \grad{g}{\x} = \grad{f}{\x} - \mu \x \).
    From the first-order condition for convexity~\pthmref{thm:first-order-condition-for-convexity}, we have that \( g \) is convex iff
    \begin{align*}
        \func{g}{\y}
        & \geq
        \func{g}{\x} + {\grad{g}{\x}}^\top (\y - \x),
        \quad \forall \x, \y \in D
        \\
        \implies
        \func{f}{\y} - \frac{\mu}{2} \norm{\y}^2
        & \geq
        \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 + {\big( \grad{f}{\x} - \mu \x \big)}^\top (\y - \x)
        \\
        \implies
        \func{f}{\y}
        & \geq
        \func{f}{\x} + {\grad{f}{\x}}^\top (\y - \x) + \frac{\mu}{2} \Big( \norm{\y}^2 - \norm{\x}^2 - 2 \dotp{\x}{(\y - \x)} \Big)
        \\ & =
        \func{f}{\x} + {\grad{f}{\x}}^\top (\y - \x) + \frac{\mu}{2} \Big( \norm{\y}^2 - \cancel{ \norm{\x}^2 } - 2 \dotp{\x}{\y} + \cancel{2} \norm{\x}^2 \Big)
        \\ & =
        \func{f}{\x} + {\grad{f}{\x}}^\top (\y - \x) + \frac{\mu}{2} \normbf{y - x}^2
        , \quad
        \forall \x, \y \in D
    \end{align*}
\end{proof}

\subsection{Second-order condition for strong convexity}

\begin{corollary}{Second-order condition for strong convexity}{}
    For a function \( f \in \C^2 \) with a convex domain \( D \), \( f \) is strongly convex on \( D \) with parameter \( \mu > 0 \) iff
    \begin{equation*}
        \hess{f}{\x}
        \succeq
        \mu \I
        , \quad
        \forall \x \in D
    \end{equation*}
    i.e., the Hessian \( \hess{f}{\x} \) is positive definite with all eigenvalues at least \( \mu \), for all \( \x \in D \).
\end{corollary}

\begin{proof}
    Define \( \func{g}{\x} = \func{f}{\x} - \frac{\mu}{2} \norm{\x}^2 \).
    Then, \( \hess{g}{\x} = \hess{f}{\x} - \mu \I \).
    From the second-order condition for convexity~\pthmref{thm:second-order-condition-for-convexity}, we have that \( g \) is convex iff \( \hess{g}{\x} \succeq \zero, \; \forall \x \in D \), which is equivalent to the desired result.
\end{proof}

\chapter{Algorithmic design}

\section{Oracle}

An \textbf{oracle} is a procedure that provides information about the objective function \( f: \R^d \to \R \) at a given point \( \x \in \R^d \).
An \( n \)-th order oracle provides information up to the \( n \)-th derivative of \( f \) at \( \x \).
\begin{itemize}
    \item \textbf{Zero-order oracle:} Given \( \x \in \R^d \), returns the function value \( \func{f}{\x} \in \R \).

    \item \textbf{First-order oracle:} Given \( \x \in \R^d \), returns a tuple \( \big( \func{f}{\x}, \grad{f}{\x} \big) \in (\R, \R^d) \).

    \item \textbf{Second-order oracle:} Given \( \x \in \R^d \), returns a tuple \( \big( \func{f}{\x}, \grad{f}{\x}, \hess{f}{\x} \big) \in (\R, \R^d, \R^{d \times d}) \).
\end{itemize}

\section{Iterative algorithm template}

\begin{algorithm}[H]
    \caption{
        Iterative algorithm template for unconstrained minimisation
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^{(0)} \in \R^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \xk \) is not optimal}{
        Update the current point: \( \xkp = \func{\operatorname{ALGO}}{\xk} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \xk \)\;}
\end{algorithm}

\section{Line search methods}\label{sec:line-search}

\begin{algorithm}[H]
    \caption{
        Algorithm template for unconstrained minimisation using line search
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^{(0)} \in \R^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \xk \) is not optimal}{
        Choose a descent direction \( \pk \) from the set of descent directions
        \begin{equation*}
            \func{\mathcal{DS}}{\xk} = \left\{ \p \in \R^d \;\Big|\; \dotp{\grad{f}{\xk}}{\p} < 0 \right\}
        \end{equation*}

        Choose a step length \( \alpha_k \geq 0 \)\;

        Update the current point: \( \xkp = \xk + \alpha_k \pk \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \xk \)\;}
\end{algorithm}

\paragraph{Gradient descent:}
Algorithms that use the gradient \( \grad{f}{\xk} \) to compute the descent direction \( \pk \).

\paragraph{Steepest descent method (Cauchy):}
Choose the descent direction as
\begin{equation*}
    \pk = -\grad{f}{\xk}
    ,\qquad \text{or normalized} \quad
    \pk = - \frac{\grad{f}{\xk}}{\norm{\grad{f}{\xk}}}
\end{equation*}

\chapter{Exact line search}

Choose the step length \( \alpha_k \) in line search~\psecref{sec:line-search} by solving the one-dimensional optimisation problem
\begin{equation*}
    \alpha_k = \argmin_{\alpha \geq 0} g_k(\alpha),
    \qquad \text{where } \;
    g_k(\alpha) \triangleq f\left( \xk + \alpha \pk \right)
\end{equation*}

\section{Convex quadratic minimisation problem}

\paragraph{Problem:}
Consider the unconstrained quadratic minimisation problem
\begin{align*}
    \min_{\x \in \R^d} \func{f}{\x},
    \quad \text{where }
    \func{f}{\x} = \left( \half \qfbf{x}{Q} + \dotp{\h}{\x} + c \right),
    \quad \Q \succ \zero, \text{i.e.}, \Q \in \mathbf{S}_d^{++},
    \quad \mathbf{h} \in \R^d,
    \quad c \in \R
\end{align*}

\paragraph{Solution:}
Since \( \hess{f}{\x} = \Q \succ \zero \), \( f \) is strongly convex, and thereby has a unique global minimum which is also the unique local minimum.
The gradient of \( f \) can be computed as \( \grad{f}{\x} = \Q \x + \mathbf{h} \), and thereby from the first-order necessary condition for local minimum~\pthmref{thm:first-order-necessary-condition-for-a-local-minimum}, the local minimum \( \xstar \) must necessarily satisfy
\begin{equation*}
    \grad{f}{\xstar} = \Q \xstar + \mathbf{h} = \zero
    \quad \iff \quad
    \boxed{
        \xstar = -\Qinv \mathbf{h}
    }
\end{equation*}
where \( \Qinv \) exists since \( \Q \succ \zero \).

\paragraph{Challenge:}
Not allowed to compute \( \Qinv \) explicitly.
Matrix-vector products with \( \Q \) are allowed.

\paragraph{Algorithm design:}

To find the optimal step length \( \alpha_k \), consider
\begin{align*}
    g_k(\alpha)
    & \triangleq
    \func{f}{\xk + \alpha \pk}
    \\ & =
    \half \qf{\left( \xk + \alpha \pk \right)}{\Q}
    + \dotp{\mathbf{h}}{\!\left( \xk + \alpha \pk \right)}
    + c
    \\ & =
    \half \qf{\xk}{\Q}
    + \alpha \qf{\xk}{\Q}[\pk]
    + \frac{\alpha^2}{2} \qf{\pk}{\Q}
    + \dotp{\mathbf{h}}{\xk}
    + \alpha \dotp{\mathbf{h}}{\pk}
    + c
    \\ & =
    \alpha^2 \left( \half \qf{\pk}{\Q} \right)
    + \alpha \left( \qf{\xk}{\Q}[\pk] + \dotp{\mathbf{h}}{\pk} \right)
    + \left( \half \qf{\xk}{\Q} + \dotp{\mathbf{h}}{\xk} + c \right)
    \\ & =
    \alpha^2 \underbrace{ \left( \half \qf{\pk}{\Q} \right) }_{p_k}
    + \alpha \underbrace{ \left( \dotp{\grad{f}{\xk}}{\pk} \right) }_{q_k}
    + \underbrace{ \Big( \func{f}{\xk} \Big) }_{r_k}
\end{align*}
which is a quadratic function in \( \alpha \) with \( p_k > 0 \; \left( \because \Q \succ \zero \right) \) and \( q_k < 0 \; \left( \because \pk \in \func{\mathcal{DS}}{\xk} \right) \).

For the \hyperlink{1D-quadratic}{one-dimensional quadratic} case, we can then compute the minimum point and minimum value as
\begin{equation*}
    \alpha_k
    = -\frac{q_k}{2 p_k}
    \implies
    \boxed{
        \alpha_k
        = -\frac{\dotp{\grad{f}{\xk}}{\pk}}{\qf{\pk}{\Q}}
    }
    > 0
\end{equation*}
\begin{equation*}
    \func{f}{\xkp}
    =
    g_k(\alpha_k)
    =
    r_k - \frac{q_k^2}{4 p_k}
    \implies
    \boxed{
        \func{f}{\xkp}
        =
        \func{f}{\xk} - \frac{{\left( \dotp{\grad{f}{\xk}}{\pk} \right)}^2}{2 \qf{\pk}{\Q}}
    }
\end{equation*}

For the choice \( \pk = -\grad{f}{\xk} \), we have
\begin{equation*}
    \alpha_k
    =
    \frac{\norm{\grad{f}{\xk}}^2} {\qf{\grad{f}{\xk}}{\Q}},
    \qquad
    \func{f}{\xkp}
    =
    \func{f}{\xk} - \frac{\norm{\grad{f}{\xk}}^4}{2 \qf{\grad{f}{\xk}}{\Q}}
\end{equation*}

\paragraph{Analysis:}

For a local minimum \( \xstar \), and any \( \p \in \R^d \), we have
\begin{align*}
    \func{f}{\xstar + \p}
    & =
    \half \qf{(\xstar + \p)}{\Q} + \dotp{\h}{(\xstar + \p)} + c
    \\ & =
    \half \qf{\xstar}{\Q} + \qf{\xstar}{\Q}[\p] + \half \qf{\p}{\Q} + \dotp{\h}{\xstar} + \dotp{\h}{\p} + c
    \\ & =
    \left( \half \qf{\xstar}{\Q} + \dotp{\h}{\xstar} + c \right)
    + {\cancel{\left( \Q \xstar + \mathbf{h} \right)}}^\top \p
    + \half \qf{\p}{\Q}
    \\ & =
    \func{f}{\xstar} + \half \qf{\p}{\Q}
    \\
    \implies
    \func{f}{\x}
    & =
    \func{f}{\xstar} + \half \qf{(\x - \xstar)}{\Q},
    \quad \forall \x \in \R^d
\end{align*}

Now, consider the error at the \( k^{th} \) iteration defined as
\begin{align*}
    E_k
    & \triangleq
    \func{f}{\xk} - \func{f}{\xstar}
    \\
    \implies
    E_k
    & =
    \half \qf{(\xk - \xstar)}{\Q}
    \\
    \because
    \grad{f}{\xstar}
    & =
    \Q \xstar + \mathbf{h}
    =
    \zero
    \\
    \implies
    \grad{f}{\xk}
    & =
    \Q \xk + \mathbf{h}
    =
    \Q \left( \xk - \xstar \right)
    \\
    \implies
    \left( \xk - \xstar \right)
    & =
    \Qinv \, \grad{f}{\xk}
    \\
    \implies
    E_k
    & =
    \half \dotp{\left( \xk - \xstar \right)}{\grad{f}{\xk}}
    =
    \half \dotp{\Big( \Qinv \, \grad{f}{\xk} \Big)}{\grad{f}{\xk}}
    \\ & =
    \half \qf{\grad{f}{\xk}}{{\left( \Qinv \right)}^\top}
    =
    \half \qf{\grad{f}{\xk}}{\Qinv}
\end{align*}

Now, the successive error reduction can be computed as
\begin{align*}
    E_k - E_{k+1}
    & =
    \func{f}{\xk} - \func{f}{\xkp}
    =
    \frac{{\left( \dotp{\grad{f}{\xk}}{\pk} \right)}^2}{2 \qf{\pk}{\Q}}
    \\
    \implies
    \frac{E_{k} - E_{k+1}}{E_k}
    & =
    \frac{{\left( \dotp{\grad{f}{\xk}}{\pk} \right)}^2}{ {\left( \grad{f}{\xk} \right)}^\top \Qinv \, \grad{f}{\xk} \; \qf{\pk}{\Q}}
\end{align*}

For the choice \( \pk = -\grad{f}{\xk} \), we have
\begin{align*}
    \frac{E_{k} - E_{k+1}}{E_k}
    & =
    \frac{{\left( {\grad{f}{\xk}}^\top \left( -\grad{f}{\xk} \right) \right)}^2}{ {\left( \grad{f}{\xk} \right)}^\top \Qinv \, \grad{f}{\xk} \; {\left( -\grad{f}{\xk} \right)}^\top \Q \left( -\grad{f}{\xk} \right)}
    \\ & =
    \frac{{\Vert \grad{f}{\xk} \Vert}^4}{ {\left( \grad{f}{\xk} \right)}^\top \Qinv \, \grad{f}{\xk} \; {\grad{f}{\xk}}^\top \Q \grad{f}{\xk}}
\end{align*}

From Kantorovich inequality~\psecref{sec:kantorovich-inequality}, we have
\begin{equation*}
    \frac{\norm{\z}^4}{(\qf{\z}{\Q})(\qf{\z}{\Qinv})}
    \geq
    \frac{4 \func{\lambda_{\min}}{\Q} \func{\lambda_{\max}}{\Q}}{{\left( \func{\lambda_{\min}}{\Q} + \func{\lambda_{\max}}{\Q} \right)}^2},
    \quad \forall \z \in \R^d \setminus \{ \zero \}
\end{equation*}
where \( \func{\lambda_{\min}}{\Q} \) and \( \func{\lambda_{\max}}{\Q} \) are the minimum and maximum eigenvalues of \( \Q \), respectively.

Applying this with \( \z = \grad{f}{\xk} \), we have
\begin{align*}
    \frac{E_k - E_{k+1}}{E_k}
    & \geq
    \frac{4 \func{\lambda_{\min}}{\Q} \func{\lambda_{\max}}{\Q}}{{\left( \func{\lambda_{\min}}{\Q} + \func{\lambda_{\max}}{\Q} \right)}^2}
    \\
    \implies
    \frac{E_{k+1}}{E_k}
    & \leq
    1 - \frac{4 \func{\lambda_{\min}}{\Q} \func{\lambda_{\max}}{\Q}}{{\left( \func{\lambda_{\min}}{\Q} + \func{\lambda_{\max}}{\Q} \right)}^2}
    =
    {\left( \frac{\func{\lambda_{\max}}{\Q} - \func{\lambda_{\min}}{\Q}}{\func{\lambda_{\max}}{\Q} + \func{\lambda_{\min}}{\Q}} \right)}^2
    \triangleq
    \rho
\end{align*}
\begin{equation*}
    \implies
    \frac{E_k}{E_0}
    =
    \prod_{i = 0}^{k-1} \frac{E_{i+1}}{E_i}
    \leq
    \prod_{i = 0}^{k-1} \rho
    =
    \rho^k
    \implies
    E_k
    \leq
    \rho^k E_0
\end{equation*}
where \( \rho \in \rinterval{0}{1} \; \because \Q \succ \zero, \; \func{\lambda_{\max}}{\Q} \geq \func{\lambda_{\min}}{\Q} > 0 \), and \( 4ab \leq {(a+b)}^2, \; \forall a, b \).

\chapter{Inexact line search}

\section{Sufficient decrease condition (Armijo condition)}

\begin{definition}{Sufficient decrease condition (Armijo condition)}{}
    \begin{equation*}
        \func{f}{\xk + \alpha_k \pk}
        \leq
        \func{f}{\xk} + c \alpha_k \dotp{\grad{f}{\xk}}{\pk},
        \quad 0 < c < 1
    \end{equation*}
\end{definition}

\subsection{Forward expansion}

Note that for sufficiently small \( \alpha_k \), the sufficient decrease condition is always satisfied since
\begin{align*}
    \func{f}{\xk + \alpha_k \pk}
    & =
    \func{f}{\xk} + \alpha_k \dotp{\grad{f}{\xk}}{\pk} + o(\alpha_k)
    \\ & <
    \func{f}{\xk} + c \alpha_k \dotp{\grad{f}{\xk}}{\pk},
    \qquad \because
    \dotp{\grad{f}{\xk}}{\pk} < 0,
    \quad 0 < c < 1
\end{align*}

\subsection{Backtracking line search}

Determine the largest number \( \alpha_k \) in the sequence \( {\{ \beta^m \bar{\alpha} \}}_{m = 0}^{\infty} \) that satisfies the \textit{sufficient decrease condition}, where \( \bar{\alpha} > 0 \) is an initial step length and \( 0 < \beta < 1 \) is a scaling factor.

\section{Goldstein conditions / Armijo-Goldstein conditions}

\begin{definition}{Goldstein conditions / Armijo-Goldstein conditions}{}
    \vspace*{-1em}
    \begin{align*}
        \func{f}{\xk} + (1 - c) \alpha_k \dotp{\grad{f}{\xk}}{\pk}
        & \leq
        \overbrace{
            \func{f}{\xk + \alpha_k \pk}
            \leq
            \func{f}{\xk} + c \alpha_k \dotp{\grad{f}{\xk}}{\pk}
        }^{\text{sufficient decrease}}
        \\
        \text{where } \;&
        0 < c < \half
    \end{align*}
\end{definition}

\subsection{Rate of convergence}

\begin{equation*}
    \sim
    R_k
    =
    \frac{2 \rho_1 (1 - \rho_2)}{L} \cos^2 \theta^{(k)}
\end{equation*}

\section{Wolfe conditions}

\begin{definition}{Wolfe conditions}{}
    \vspace*{-1em}
    \begin{align*}
        \func{f}{\xk + \alpha_k \pk}
        & \leq
        \func{f}{\xk} + c_1 \alpha_k \dotp{\grad{f}{\xk}}{\pk}
        \tag{sufficient decrease}
        \\
        \dotp{\grad{f}{\xk + \alpha_k \pk}}{\pk}
        & \geq
        c_2 \dotp{\grad{f}{\xk}}{\pk}
        \tag{curvature}
        \\
        \text{where } \;&
        0 < c_1 < c_2 < 1
    \end{align*}
\end{definition}

\begin{equation*}
    R = \frac{c_1 (1 - c_2)}{L}
\end{equation*}

\subsection{Strong Wolfe conditions}

\begin{definition}{Strong Wolfe conditions}{}
    \vspace*{-1em}
    \begin{align*}
        \func{f}{\xk + \alpha_k \pk}
        & \leq
        \func{f}{\xk} + c_1 \alpha_k \dotp{\grad{f}{\xk}}{\pk}
        \tag{sufficient decrease}
        \\
        \abs{\dotp{\grad{f}{\xk + \alpha_k \pk}}{\pk}}
        & \leq
        c_2 \abs{\dotp{\grad{f}{\xk}}{\pk}}
        \tag{strong curvature}
        \\
        \text{where } \;&
        0 < c_1 < c_2 < 1
    \end{align*}
\end{definition}

\section{Sufficient decrease lemma}

\begin{lemma}{Sufficient decrease lemma}{}
    Suppose \( f \in \C^{1}_{L} \) is bounded below, then \( \forall \x \in \R^n, \; \alpha \in \linterval[scaled]{0}{\frac{1}{L}} \), the gradient descent update \( \x^+ = \x - \alpha \grad{f}{\x} \) yields a sufficient decrease in the function value:
    \begin{equation*}
        \func{f}{\x^+} \leq \func{f}{\x} - \frac{\alpha}{2} \norm{\grad{f}{\x}}^2
    \end{equation*}
\end{lemma}

\begin{equation*}
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{\norm{\grad{f}{\xk}}^2}
    \geq R_k \geq R > 0
\end{equation*}

\section{Global convergence criterion}

Starting from an arbitrary \( \x_0 \in \R^d \), if a scheme generates a sequence \( {\{ \xk \}}_{k = 1}^{\infty} \) such that
\begin{equation*}
    \lim_{k \to \infty} \norm{\grad{f}{\xk}} = 0
\end{equation*}
then the scheme is said to exhibit \textbf{global convergence}.

The sequence asymptotically approaches a \textbf{stationary point} \( \xstar \), i.e., \( \grad{f}{\xstar} = \zero \).
Without further assumptions on \( f \), nothing can be said about \( \xstar \) being a local or global minimum, maximum, or a saddle point.
The `global' refers to the fact that the initial point \( \x_0 \) is arbitrary, and not about the limit point.

\chapter{Second-order methods}

\begin{equation*}
    \xkp
    =
    \xk - \hessinv{f}{\x} \grad{f}{\xk},
    \qquad
    \hess{f}{\xk} \succ \zero
\end{equation*}

\paragraph{Newton direction:}
Refers to the descent direction
\begin{equation*}
    \pk = -\hessinv{f}{\xk} \grad{f}{\xk},
    \qquad \hess{f}{\xk} \succ \zero
\end{equation*}

This satisfies the condition for descent direction since
\begin{align*}
    \hess{f}{\xk}
    &
    \succ \zero
    \implies
    \hessinv{f}{\xk}
    \succ \zero
    \\
    \implies
    \dotp{\grad{f}{\xk}}{\pk}
    & =
    - {\left( \grad{f}{\xk} \right)}^\top \hessinv{f}{\xk} \grad{f}{\xk}
    < 0
\end{align*}

From the fundamental theorem of calculus~\psecref{sec:fundamental-theorem-of-calculus}, with \( \func{g}{t} \triangleq \grad{f}{\x + t(\y - \x)}, \; t \in [0, 1] \), we have
\begin{equation*}
    \func{g}{1} - \func{g}{0}
    =
    \int_{0}^{1} \func{g'}{t} \, dt
    \implies
    \boxed{
        \grad{f}{\y} - \grad{f}{\x}
        =
        \int_{0}^{1} \hess{f}{\x + t(\y - \x)} (\y - \x) \, dt,
        \quad \forall \x, \y \in \R^d
    }
\end{equation*}

For a stationary point \( \xstar \), i.e., \( \grad{f}{\xstar} = \zero \), we have
\begin{equation*}
    \implies
    \grad{f}{\x}
    =
    \int_{0}^{1} \hess{f}{\xstar + t(\x - \xstar)} (\x - \xstar) \, dt,
    \quad \forall \x \in \R^d
\end{equation*}

Suppose \( \hess{f}{\xstar} \succ \zero \), ensuring that \( \xstar \) is a local minimum.
\begin{equation*}
    \implies
    \exists l > 0 \text{ such that } \hess{f}{\xstar} \succeq l \, \I
\end{equation*}

Assuming \( f \in \C^2_M \), we have
\begin{equation*}
    \boxed{
        \norm{\hess{f}{\y} - \hess{f}{\x}}
        \leq
        M \norm{\y - \x},
        \quad \forall \x, \y \in \R^d
    }
\end{equation*}

From Taylor's theorem, we have
\begin{equation*}
    \func{f}{\y}
    =
    \func{f}{\x}
    + \dotp{\grad{f}{\x}}{(\y - \x)}
    + \half \qf{(\y - \x)}{\hess{f}{\x}}
    + o(\norm{\y - \x}^2),
    \quad \forall \x, \y \in \R^d
\end{equation*}

Taking the derivative with respect to \( \y \) on both sides, we have
\begin{equation*}
    \implies
    \grad{f}{\y}
    =
    \grad{f}{\x}
    + \hess{f}{\x} (\y - \x)
    + o(\norm{\y - \x}),
    \quad \forall \x, \y \in \R^d
\end{equation*}

\begin{align*}
    \implies
    \norm{
        \grad{f}{\y}
        - \grad{f}{\x}
        - \hess{f}{\x} (\y - \x)
    }
    & =
    \norm{
        \int_{0}^{1} \Big( \hess{f}{\x + t(\y - \x)} - \hess{f}{\x} \Big) (\y - \x) \, dt
    }
    \\ & \leq
    \int_{0}^{1} \norm{
        \Big( \hess{f}{\x + t(\y - \x)} - \hess{f}{\x} \Big) (\y - \x)
    } \, dt
    \\ & \leq
    \int_{0}^{1}
    \norm{\hess{f}{\x + t(\y - \x)} - \hess{f}{\x}}
    \norm{\y - \x}
    \, dt
    \\ & \leq
    \int_{0}^{1}
    M \norm{\cancel{\x} + t(\y - \x) - \cancel{\x}}
    \norm{\y - \x}
    \, dt
    \\ & =
    \int_{0}^{1}
    M
    \norm{\y - \x}^2
    \, t
    \, dt
    =
    M
    \norm{\y - \x}^2
    \int_{0}^{1}
    t
    \, dt
    =
    \frac{M}{2}
    \norm{\y - \x}^2
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        \norm{
            \grad{f}{\y}
            - \grad{f}{\x}
            - \hess{f}{\x} (\y - \x)
        }
        \leq
        \frac{M}{2} \norm{\y - \x}^2,
        \quad \forall \x, \y \in \R^d
    }
\end{equation*}

Now, since \( - \norm{\mathbf{A}} \, \I \preceq \mathbf{A} \preceq \norm{\mathbf{A}} \, \I, \quad \forall \mathbf{A} \in \SD \), we have
\begin{align*}
    - \norm{\hess{f}{\y} - \hess{f}{\x}} \, \I
    \preceq
    \hess{f}{\y} - \hess{f}{\x}
    \preceq
    \norm{\hess{f}{\y} - \hess{f}{\x}} \, \I,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    - M \norm{\y - \x} \, \I
    \preceq
    \hess{f}{\y} - \hess{f}{\x}
    \preceq
    M \norm{\y - \x} \, \I,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \boxed{
        \hess{f}{\x} - M \norm{\y - \x} \, \I
        \preceq
        \hess{f}{\y}
        \preceq
        \hess{f}{\x} + M \norm{\y - \x} \, \I,
        \quad \forall \x, \y \in \R^d
    }
\end{align*}

Let \( r_k \triangleq \norm{\xk - \xstar} \).
Then, we have
\begin{align*}
    \implies
    &
    \hess{f}{\xstar} - M r_k \, \I
    \preceq
    \hess{f}{\xk}
    \preceq
    \hess{f}{\xstar} + M r_k \, \I
    \\
    \implies
    &
    (l - M r_k) \, \I
    \preceq
    \hess{f}{\xk}
    \\
    \therefore
    \quad
    &
    \text{If }
    (l - M r_k) > 0
    \implies
    \hess{f}{\xk}
    \succ
    \zero
    \\
    \text{i.e.,}
    \quad
    &
    \text{If }
    \boxed{
        r_k < \frac{l}{M}
    }
    \implies
    \hess{f}{\xk}
    \succ
    \zero
    \\
    \text{Also, }
    \quad
    &
    (l - M r_k) \, \I
    \preceq
    \hess{f}{\xk}
    \implies
    \frac{1}{(l - M r_k)} \, \I
    \succeq
    \hessinv{f}{\xk}
    \\
    \text{i.e.,}
    \quad
    &
    \hessinv{f}{\xk}
    \preceq
    \frac{1}{(l - M r_k)} \, \I
    \implies
    \boxed{
        \norm{\hessinv{f}{\xk}}
        \leq
        \frac{1}{(l - M r_k)}
    }
\end{align*}

Now,
\begin{align*}
    r_{k+1}
    & =
    \norm{\xkp - \xstar}
    =
    \norm{\left( \xk - \hessinv{f}{\xk} \grad{f}{\xk} \right) - \xstar}
    \\ & =
    \norm{\xk - \xstar - \hessinv{f}{\xk} \grad{f}{\xk}}
    \\ & =
    \norm{\hessinv{f}{\xk} \Big( \hess{f}{\xk} (\xk - \xstar) - \grad{f}{\xk} \Big)}
    \\ & \leq
    \norm{\hessinv{f}{\xk}} \,
    \norm{\hess{f}{\xk} (\xk - \xstar) - \grad{f}{\xk}}
\end{align*}
\begin{align*}
    \implies
    &
    \norm{\hess{f}{\xk} (\xk - \xstar) - \grad{f}{\xk}}
    \\ & =
    \norm{\hess{f}{\xk} (\xk - \xstar) - \int_{0}^{1} \hess{f}{\xstar + t(\xk - \xstar)} (\xk - \xstar) \, dt}
    \\ & =
    \norm{\int_{0}^{1} \hess{f}{\xk} (\xk - \xstar)  \, dt - \int_{0}^{1} \hess{f}{\xstar + t(\xk - \xstar)} (\xk - \xstar) \, dt}
    \\ & =
    \norm{\int_{0}^{1} \Big( \hess{f}{\xk} - \hess{f}{\xstar + t(\xk - \xstar)} \Big) (\xk - \xstar) \, dt}
    \\ & \leq
    \int_{0}^{1} \norm{\Big( \hess{f}{\xk} - \hess{f}{\xstar + t(\xk - \xstar)} \Big) (\xk - \xstar)} \, dt
    \\ & \leq
    \int_{0}^{1} \norm[\Big]{\hess{f}{\xk} - \hess{f}{\xstar + t(\xk - \xstar)}} \, \norm{\xk - \xstar} \, dt
    \\ & \leq
    \int_{0}^{1} M \norm[\Big]{\xk - \xstar - t(\xk - \xstar)} \, \norm{\xk - \xstar} \, dt
    \\ & =
    \int_{0}^{1} M (1 - t) \norm{\xk - \xstar}^2 \, dt
    \\ & =
    M
    \norm{\xk - \xstar}^2
    \int_{0}^{1} (1 - t) \, dt
    =
    \frac{M}{2}
    r_k^2
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        r_{k+1}
        \leq
        \frac{M}{2}
        \norm{\hessinv{f}{\xk}}
        r_k^2
    }
    \implies
    r_{k+1}
    \leq
    \frac{M}{2 (l - M r_k)}
    r_k^2
\end{equation*}

Thereby, rate of convergence for Newton's method is quadratic.

Now, we want \( r_{k+1} < r_k \) to ensure successive iterates converge to \( \xstar \), thereby we get
\begin{equation*}
    \frac{M}{2 (l - M r_k)}
    r_k^2
    <
    r_k
    \implies
    M r_k
    <
    2 l - 2 M r_k
    \implies
    3 M r_k
    <
    2 l
    \implies
    \boxed{
        r_k
        <
        \frac{2 l}{3 M}
    }
\end{equation*}

Note that \( \displaystyle r_k < \frac{2 l}{3 M} \implies r_k < \frac{l}{M} \), ensuring \( \hess{f}{\xk} \succ \zero \).

Thereby, if we start with \( \displaystyle \boxed{ r_0 < \frac{2 l}{3 M} } \), then since \( \displaystyle r_k < r_{k - 1} < \cdots < r_1 < r_0 < \frac{2 l}{3 M} \), this ensures that \( \displaystyle r_k < \frac{2 l}{3 M} \).
Thereby, we have \( \hess{f}{\xk} \succ \zero \) throughout the iterations, ensuring that \( \pk \) is a descent direction, thereby the algorithm converges to the local minimum \( \xstar \).

\chapter{Conjugate direction methods}

\section{\texorpdfstring{\( \Q \)--conjugates}{Q-conjugates}}

\begin{definition}{\( \Q \)--conjugates}{}
    Given some real symmetric positive definite matrix \( \Q \in \PD \), a set of vectors \( \set{ \mathbf{u}_i }_{i = 1}^n, \; \mathbf{u}_i \in \R^d \) are said to be \textbf{conjugates with respect to \( \Q \)} \text{ OR } \textbf{\( \Q \)--conjugates} if
    \vspace{-0.5em}
    \begin{equation*}
        \qf{\mathbf{u}_i}{\Q}[\mathbf{u}_j] = 0,
        \quad \forall i, j \in \{ 1, 2, \ldots, n \}, \; i \neq j
    \end{equation*}
\end{definition}

\begin{theorem}{Linear independence of \( \Q \)--conjugates}{}
    The set of \( \Q \)--conjugate vectors are linearly independent.
\end{theorem}

\begin{proof}
    Let \( \set{ \mathbf{u}_i }_{i = 1}^n, \; \mathbf{u}_i \in \R^d \) be a set of \( \Q \)--conjugate vectors.
    Define \( \z \triangleq \sum_{i = 1}^{n} \beta_i \mathbf{u}_i, \; \beta_i \in \R \).

    Then, for any \( l \in \{ 1, 2, \ldots, n \} \), we have
    \begin{equation*}
        \qf{\mathbf{u}_l}{\Q}[\z]
        =
        \qf{\mathbf{u}_l}{\Q}[\left( \sum_{i = 1}^{n} \beta_i \mathbf{u}_i \right)]
        =
        \sum_{i = 1}^{n} \beta_i \qf{\mathbf{u}_l}{\Q}[\mathbf{u}_i]
        =
        \beta_l \qf{\mathbf{u}_l}{\Q}
        + \sum_{i \neq l} \beta_i \cancel{ \qf{\mathbf{u}_l}{\Q}[\mathbf{u}_i] }
        =
        \beta_l \qf{\mathbf{u}_l}{\Q}
    \end{equation*}

    From this, we can see that if \( \z = \zero \), then \( \qf{\mathbf{u}_l}{\Q}[\z] = \zero = \beta_l \qf{\mathbf{u}_l}{\Q} \).
    Since \( \qf{\mathbf{u}_l}{\Q} > 0, \; \because \Q \succ \zero \), we have that \( \beta_i = 0, \; \forall i \in \{ 1, 2, \ldots, n \} \).
    Thereby, the vectors \( \set{ \mathbf{u}_i }_{i = 1}^n \) are linearly independent.
\end{proof}

\section{Conjugate direction methods}

\begin{algorithm}[H]
    \caption{
        Conjugate direction algorithm for unconstrained minimisation
        of a differentiable function \( f : \R^d \to \R \), \( f \in \C^1 \).
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^{(0)} \in \R^d \);

        A set of \( d \) mutually \( \Q \)--conjugate descent directions \( \set{\mathbf{u}_i}_{i = 0}^{d - 1}, \; \mathbf{u}_i \in \R^d, \; \forall i \in \set{0, 1, \ldots, d - 1} \)\;
        \vspace{-0.5em}
        \begin{equation*}
            \qf{\mathbf{u}_i}{\Q}[\mathbf{u}_j] = 0,
            \quad \forall i, j \in \set{0, 1, \ldots, d - 1}, \; i \neq j
        \end{equation*}
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \xstar = \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \xk \) is not optimal}{
        Choose step length \( \alpha_k \) by line search along direction \( \mathbf{u}_k \)\;

        Update the current point: \( \xkp = \xk + \alpha_k \mathbf{u}_k \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \xk \)\;}
\end{algorithm}

\begin{algorithm}[H]
    \caption{
        Conjugate direction algorithm for unconstrained minimisation of a convex quadratic function \( f: \R^d \to \R \).
        \vspace{-1em}
        \begin{equation*}
            \func{f}{\x}
            =
            \half \qf{\x}{\Q} + \dotp{\mathbf{h}}{\x},
            \quad \Q \in \PD, \; \mathbf{h} \in \R^d
        \end{equation*}
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( \func{f}{\x} \);
        Initial point \( \x^{(0)} \in \R^d \);

        A set of \( d \) mutually \( \Q \)--conjugate descent directions \( \set{\mathbf{u}_i}_{i = 0}^{d - 1}, \; \mathbf{u}_i \in \R^d, \; \forall i \in \set{0, 1, \ldots, d - 1} \)\;
        \vspace{-0.5em}
        \begin{equation*}
            \qf{\mathbf{u}_i}{\Q}[\mathbf{u}_j] = 0,
            \quad \forall i, j \in \set{0, 1, \ldots, d - 1}, \; i \neq j
        \end{equation*}
    }
    \KwOut{
        Exact solution to the unconstrained minimisation problem \( \displaystyle \xstar = \argmin_{\x \in \R^d} \func{f}{\x} \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( k < d \) \emph{ AND } \( \xk \) is not optimal}{
        Choose step length \( \alpha_k \) by exact line search as
        \vspace{-0.5em}
        \begin{equation*}
            \alpha_k
            =
            - \frac{\dotp{\grad{f}{\xk}}{\mathbf{u}_k}}{\qf{\mathbf{u}_k}{\Q}}
        \end{equation*}

        \vspace{-0.5em}
        Update the current point: \( \xkp = \xk + \alpha_k \mathbf{u}_k \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \xk \)\;}
\end{algorithm}
