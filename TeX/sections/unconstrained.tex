\chapter{Unconstrained optimisation}

\section{Optimisation methods}

\subsection{First-order methods}

Assume \( f \in \mathcal{C}^{1} \), and use \( f(\mathbf{x} + \mathbf{p}) = f(\mathbf{x}) + {\nabla f(\mathbf{x})}^{\top} \mathbf{p} + o(\| \mathbf{p} \|) \).

\subsection{Second-order methods}

Assume \( f \in \mathcal{C}^{2} \), and use \( f(\mathbf{x} + \mathbf{p}) = f(\mathbf{x}) + {\nabla f(\mathbf{x})}^{\top} \mathbf{p} + \frac{1}{2} \mathbf{p}^{\top} \nabla^{2} f(\mathbf{x}) \mathbf{p} + o(\| \mathbf{p} \|^2) \).

\section{Types of minimum}

\subsection{Global minimum}

\begin{definition}{Global minimum}{global-minimum}
    The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{global minimum} of the function \( f: D \subseteq \mathbb{R}^d \to \mathbb{R} \) if
    \begin{equation*}
        f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in D
    \end{equation*}
\end{definition}

\subsection{Local minimum}

\begin{definition}{Local minimum}{}
    The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{local minimum} of the function \( f: \mathbb{R}^d \to \mathbb{R} \) if there exists a \( \delta > 0 \) such that for all \( \mathbf{x} \) in the \( \delta \)-neighborhood of \( \mathbf{x}^* \), we have \( f(\mathbf{x}^*) \leq f(\mathbf{x}) \), i.e.,
    \begin{equation*}
        f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*)
    \end{equation*}
\end{definition}

\subsection{Strict local minimum}

\begin{definition}{Strict local minimum}{}
    The point \( \mathbf{x}^* \in \mathbb{R}^d \) is a \textbf{strict local minimum} of the function \( f: \mathbb{R}^d \to \mathbb{R} \) if there exists a \( \delta > 0 \) such that for all \( \mathbf{x} \) in the \( \delta \)-neighborhood of \( \mathbf{x}^* \) except \( \mathbf{x}^* \) itself, we have \( f(\mathbf{x}^*) < f(\mathbf{x}) \), i.e.,
    \begin{equation*}
        f(\mathbf{x}^*) < f(\mathbf{x}), \quad \forall \mathbf{x} \in B_{\delta}(\mathbf{x}^*) \setminus \{ \mathbf{x}^* \}
    \end{equation*}
\end{definition}

\section{Necessary and sufficient conditions}

\subsection{First-order necessary condition for a local minimum}

\begin{theorem}{First-order necessary condition for a local minimum}{}
    For a function \( f \in \mathcal{C}^{1} \), if \( \mathbf{x}^* \) is a local minimum of \( f \), then \( \nabla f(\mathbf{x}^*) = \mathbf{0} \).
\end{theorem}

\subsection{Second-order necessary condition for a local minimum}

\begin{theorem}{Second-order necessary condition for a local minimum}{}
    For a function \( f \in \mathcal{C}^{2} \), if \( \mathbf{x}^* \) is a local minimum of \( f \), then \( \nabla f(\mathbf{x}^*) = \mathbf{0}, \; \nabla^2 f(\mathbf{x}^*) \succeq 0 \).
\end{theorem}

\subsection{Second-order sufficient condition for a strict local minimum}

\begin{theorem}{Second-order sufficient condition for a strict local minimum}{}
    For a function \( f \in \mathcal{C}^{2} \) such that \( \nabla f(\mathbf{x}^*) = \mathbf{0}, \; \nabla^2 f(\mathbf{x}^*) \succ 0 \), then \( \mathbf{x}^* \) is a strict local minimum of \( f \).
\end{theorem}

\subsection{First-order sufficient condition for a global minimum under convexity}

\begin{theorem}{First-order sufficient condition for a global minimum under convexity}{}
    For a convex function \( f \in \mathcal{C}^1 \) with \( \nabla f(\mathbf{x}^*) = \mathbf{0} \), then \( \mathbf{x}^* \) is a global minimum of \( f \).
\end{theorem}

\begin{proof}
    From the first-order condition for convexity~\pthmref{thm:first-order-condition-for-convexity}, we have
    \begin{align*}
        f(\mathbf{y})
        & \geq
        f(\mathbf{x}^*) + \cancel{ {\nabla f(\mathbf{x}^*)}^{\top} (\mathbf{y} - \mathbf{x}^*) }
        , \quad \forall \mathbf{y} \in D
        \\
        \implies
        f(\mathbf{y})
        & \geq
        f(\mathbf{x}^*)
        , \quad \forall \mathbf{y} \in D
    \end{align*}
    which is the definition of a global minimum~\pdefref{def:global-minimum}.
\end{proof}

\subsection{Convexity}

\begin{definition}{Convex set}{}
    A set \( S \subseteq \mathbb{R}^n \) is \textbf{convex} if
    \begin{equation*}
        \lambda \mathbf{x} + (1 - \lambda) \mathbf{y} \in S,
        \quad \forall \mathbf{x}, \mathbf{y} \in S,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\begin{definition}{Convex function}{}
    A function $f: D \subseteq \mathbb{R}^n \to \mathbb{R}$ is \textbf{convex} on a convex set \( D \) if
    \begin{equation*}
        f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y})
        \leq
        \lambda f(\mathbf{x}) + (1 - \lambda) f(\mathbf{y}),
        \quad \forall \mathbf{x}, \mathbf{y} \in D,
        \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\subsubsection{First-order condition for convexity}

\begin{theorem}{First-order condition for convexity}{first-order-condition-for-convexity}
    For a function \( f \in \mathcal{C}^1 \) with a convex domain \( D \), \( f \) is convex on \( D \) iff
    \begin{equation*}
        f(\mathbf{y})
        \geq
        f(\mathbf{x}) + {\nabla f(\mathbf{x})}^\top (\mathbf{y} - \mathbf{x}),
        \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n
    \end{equation*}
\end{theorem}

\begin{proof}
    \( (\Rightarrow) \)
    Rearranging the definition of convexity, we get
    \begin{equation*}
        \frac{f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y}) - f(\mathbf{x})}{\lambda}
        \leq
        f(\mathbf{y}) - f(\mathbf{x}),
        \quad \forall \mathbf{x}, \mathbf{y} \in D,
        \; \lambda \in \linterval{0}{1}
    \end{equation*}
    Taking the limit as \( \lambda \to 0^+ \), we have
    \begin{equation*}
        {\nabla f(\mathbf{x})}^{\top} (\mathbf{y} - \mathbf{x})
        =
        \lim_{\lambda \to 0^+} \frac{f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y}) - f(\mathbf{x})}{\lambda}
        \leq
        f(\mathbf{y}) - f(\mathbf{x}),
        \quad \forall \mathbf{x}, \mathbf{y} \in D
    \end{equation*}
    which is the desired result.
\end{proof}

\begin{corollary}{}{}
    For a function \( f \in \mathcal{C}^1 \) with a convex domain \( D \), \( f \) is convex on \( D \) iff
    \begin{equation*}
        {\big( \nabla f(\mathbf{x}) - \nabla f(\mathbf{y}) \big)}^{\top} \big( \mathbf{x} - \mathbf{y} \big) \geq 0
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
    \end{equation*}
\end{corollary}

\begin{proof}
    \( (\Rightarrow) \)
    From the first-order condition for convexity, we have
    \begin{align*}
        f(\mathbf{y})
        & \geq
        f(\mathbf{x}) + {\nabla f(\mathbf{x})}^{\top} (\mathbf{y} - \mathbf{x})
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
        \\
        f(\mathbf{x})
        & \geq
        f(\mathbf{y}) + {\nabla f(\mathbf{y})}^{\top} (\mathbf{x} - \mathbf{y})
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
    \end{align*}
    Adding the two inequalities, we get
    \begin{equation*}
        0 \geq {\big( \nabla f(\mathbf{x}) - \nabla f(\mathbf{y}) \big)}^{\top} (\mathbf{y} - \mathbf{x})
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
    \end{equation*}
    which is equivalent to the desired result.

    \( (\Leftarrow) \)
    Define \( g(t) = f(\mathbf{x} + t (\mathbf{y} - \mathbf{x})), \;\; t \in [0, 1], \;\; \mathbf{x}, \mathbf{y} \in D \).

    Then, \( g(0) = f(\mathbf{x}), \quad g(1) = f(\mathbf{y}), \quad g'(t) = {(\nabla f(\mathbf{x} + t (\mathbf{y} - \mathbf{x})))}^{\top} (\mathbf{y} - \mathbf{x}) \).

    From the given condition, we have
    \begin{align*}
        g'(t)
        & =
        {\big( \nabla f(\mathbf{x} + t (\mathbf{y} - \mathbf{x})) - \nabla f(\mathbf{x}) + \nabla f(\mathbf{x}) \big)}^{\top} (\mathbf{y} - \mathbf{x})
        \\ & =
        {\big( \nabla f(\mathbf{x} + t (\mathbf{y} - \mathbf{x})) - \nabla f(\mathbf{x}) \big)}^{\top} (\mathbf{y} - \mathbf{x}) + {\big( \nabla f(\mathbf{x}) \big)}^{\top} (\mathbf{y} - \mathbf{x})
        \geq
        {\big( \nabla f(\mathbf{x}) \big)}^{\top} (\mathbf{y} - \mathbf{x})
    \end{align*}
    for all \( t \in [0, 1] \).
    Integrating from \( 0 \) to \( 1 \), we get
    \begin{equation*}
        f(\mathbf{y}) - f(\mathbf{x})
        =
        g(1) - g(0)
        =
        \int_0^1 g'(t) \, dt
        \geq
        \int_0^1 {\big( \nabla f(\mathbf{x}) \big)}^{\top} (\mathbf{y} - \mathbf{x}) \, dt
        =
        {\big( \nabla f(\mathbf{x}) \big)}^{\top} (\mathbf{y} - \mathbf{x})
    \end{equation*}
    for all \( \mathbf{x}, \mathbf{y} \in D \), which is the first-order condition for convexity, and thereby \( f \) is convex.
\end{proof}

\subsubsection{Second-order condition for convexity}

\begin{theorem}{Second-order condition for convexity}{}
    For a function \( f \in \mathcal{C}^2 \) with a convex domain \( D \), \( f \) is convex on \( D \) iff
    \begin{equation*}
        \nabla^2 f(\mathbf{x}) \succeq 0
        , \quad
        \forall \mathbf{x} \in D
    \end{equation*}
\end{theorem}

\begin{proof}
    \( (\Rightarrow) \)
    The first-order condition for convexity can be rewritten as
    \begin{equation*}
        f(\mathbf{y}) - f(\mathbf{x}) - {\nabla f(\mathbf{x})}^{\top} (\mathbf{y} - \mathbf{x}) \geq 0
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
    \end{equation*}
    Using Taylor's theorem, we have
    \begin{equation*}
        f(\mathbf{y}) - f(\mathbf{x}) - {\nabla f(\mathbf{x})}^{\top} (\mathbf{y} - \mathbf{x})
        =
        \frac{1}{2} {(\mathbf{y} - \mathbf{x})}^{\top} \nabla^2 f(\boldsymbol{\xi}) (\mathbf{y} - \mathbf{x})
        , \quad
        \boldsymbol{\xi} = (1 - t) \mathbf{x} + t \mathbf{y}, \; t \in (0, 1)
    \end{equation*}
    for some \( t \in (0, 1) \).
    Therefore, we have
    \begin{equation*}
        {(\mathbf{y} - \mathbf{x})}^{\top} \nabla^2 f(\boldsymbol{\xi}) (\mathbf{y} - \mathbf{x}) \geq 0
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
    \end{equation*}
    which implies \( \nabla^2 f(\mathbf{x}) \succeq 0, \; \forall \mathbf{x} \in D \).
    (Note: \( \boldsymbol{\xi} \in D \) since \( D \) is convex.)

    \( (\Leftarrow) \)
    From Taylor's theorem, we have
    \begin{equation*}
        f(\mathbf{y}) - f(\mathbf{x}) - {\nabla f(\mathbf{x})}^{\top} (\mathbf{y} - \mathbf{x})
        =
        \frac{1}{2} {(\mathbf{y} - \mathbf{x})}^{\top} \nabla^2 f(\boldsymbol{\xi}) (\mathbf{y} - \mathbf{x})
        , \quad
        \boldsymbol{\xi} = (1 - t) \mathbf{x} + t \mathbf{y}, \; t \in (0, 1)
    \end{equation*}
    for some \( t \in (0, 1) \).
    Since \( \nabla^2 f(\mathbf{x}) \succeq 0, \; \forall \mathbf{x} \in D \), we have
    \begin{equation*}
        f(\mathbf{y}) - f(\mathbf{x}) - {\nabla f(\mathbf{x})}^{\top} (\mathbf{y} - \mathbf{x}) \geq 0
        , \quad
        \forall \mathbf{x}, \mathbf{y} \in D
    \end{equation*}
    which is the first-order condition for convexity, and thereby \( f \) is convex.
\end{proof}

\subsubsection{Strong convexity}

\begin{definition}{Strong convexity}{}
    A function \( f: \mathbb{R}^n \to \mathbb{R} \) is \textbf{strongly convex} with parameter \( m > 0 \) if
    \begin{equation*}
        f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y})
        \leq
        \lambda f(\mathbf{x}) + (1 - \lambda) f(\mathbf{y})
        - \frac{m}{2} \lambda (1 - \lambda) \Vert \mathbf{x} - \mathbf{y} \Vert^2,
        \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^n, \; \lambda \in [0, 1]
    \end{equation*}
\end{definition}

\section{Algorithmic design}

\subsection{Iterative algorithm template}

\begin{algorithm}[H]
    \caption{
        Iterative algorithm template for unconstrained minimisation
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Update the current point: \( \mathbf{x}^{(k+1)} = \operatorname{ALGO}(\mathbf{x^{(k)}}) \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \mathbf{x}^{(k)} \)\;}
\end{algorithm}

\paragraph{First-order oracle:}
Given \( \mathbf{x} \in \mathbb{R}^d \), returns a tuple \( \big( f(\mathbf{x}), \nabla f(\mathbf{x}) \big) \).

\subsection{Line search methods}

\begin{algorithm}[H]
    \caption{
        Algorithm template for unconstrained minimisation using line search
    }
    \SetAlgoLined{}
    \KwIn{
        First-order oracle for the objective function \( f(\mathbf{x}) \);
        Initial point \( \mathbf{x}^{(0)} \in \mathbb{R}^d \);
    }
    \KwOut{
        Approximate solution to the unconstrained minimisation problem \( \displaystyle \argmin_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}) \)\;
    }

    \( k \leftarrow 0 \)\;

    \While{\( \mathbf{x}^{(k)} \) is not optimal}{
        Choose a descent direction \( \mathbf{p}^{(k)} \) from the set of descent directions
        \begin{equation*}
            \mathcal{DS} \left( \mathbf{x}^{(k)} \right) = \left\{ \mathbf{p} \in \mathbb{R}^d \;\big|\; {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p} < 0 \right\}
        \end{equation*}

        Choose a step size \( \alpha_k \geq 0 \)\;

        Update the current point: \( \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \)\;

        \( k \leftarrow k + 1 \)\;
    }
    \Return{\( \mathbf{x}^{(k)} \)\;}
\end{algorithm}

\paragraph{Gradient descent:}
Algorithms that use the gradient \( \nabla f(\mathbf{x}^{(k)}) \) to compute the descent direction \( \mathbf{p}^{(k)} \).

\paragraph{Steepest descent method (Cauchy):}
Choose the descent direction as
\begin{equation*}
    \mathbf{p}^{(k)} = -\nabla f(\mathbf{x}^{(k)})
    ,\qquad \text{or normalized} \quad
    \mathbf{p}^{(k)} = - \frac{\nabla f(\mathbf{x}^{(k)})}{{\Vert \nabla f(\mathbf{x}^{(k)}) \Vert}}
\end{equation*}

\subsection{Exact line search}

Choose the step size \( \alpha_k \) by solving the one-dimensional optimisation problem
\begin{equation*}
    \alpha_k = \argmin_{\alpha \geq 0} g_k(\alpha),
    \qquad \text{where } \;
    g_k(\alpha) \triangleq f\left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
\end{equation*}

\subsection*{Problem:}
Consider the unconstrained quadratic minimisation problem
\begin{align*}
    \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x}),
    \quad \text{where }
    f(\mathbf{x}) = \left( \frac{1}{2} \mathbf{x}^{\top} \mathbf{Q} \mathbf{x} + \mathbf{h}^{\top} \mathbf{x} + c \right),
    \quad \mathbf{Q} \succ \mathbf{0}, \text{i.e.}, \mathbf{Q} \in \mathbf{S}_d^{++},
    \quad \mathbf{h} \in \mathbb{R}^d,
    \quad c \in \mathbb{R}
\end{align*}

\paragraph{Solution:}

The gradient of \( f \) can be computed as
\begin{equation*}
    \nabla f(\mathbf{x}) = \mathbf{Q} \mathbf{x} + \mathbf{h}
\end{equation*}
and thereby from the first-order necessary condition for local minimum, the local minimum \( \mathbf{x}^* \) must necessarily satisfy
\begin{equation*}
    \nabla f(\mathbf{x}^*) = \mathbf{Q} \mathbf{x}^* + \mathbf{h} = \mathbf{0}
    \quad \iff \quad
    \boxed{
        \mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{h}
    }
\end{equation*}

\paragraph{Challenge:}
Not allowed to compute \( \mathbf{Q}^{-1} \) explicitly.
Matrix-vector products with \( \mathbf{Q} \) are allowed.

\paragraph{Algorithm design:}

To find the optimal step size \( \alpha_k \), consider
\begin{align*}
    g_k(\alpha)
    & \triangleq
    f \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
    \\ & =
    \frac{1}{2} {\left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)}^{\top} \mathbf{Q} \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
    + \mathbf{h}^{\top} \left( \mathbf{x}^{(k)} + \alpha \mathbf{p}^{(k)} \right)
    + c
    \\ & =
    \frac{1}{2} {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{x}^{(k)}
    + \alpha {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}
    + \frac{\alpha^2}{2} {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}
    + \mathbf{h}^{\top} \mathbf{x}^{(k)}
    + \alpha \mathbf{h}^{\top} \mathbf{p}^{(k)}
    + c
    \\ & =
    \alpha^2 \left( \frac{1}{2} {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)} \right)
    + \alpha \left( {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)} + \mathbf{h}^{\top} \mathbf{p}^{(k)} \right)
    + \left( \frac{1}{2} {\mathbf{x}^{(k)}}^{\top} \mathbf{Q} \mathbf{x}^{(k)} + \mathbf{h}^{\top} \mathbf{x}^{(k)} + c \right)
    \\ & =
    \alpha^2 \underbrace{ \left( \frac{1}{2} {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)} \right) }_{p_k}
    + \alpha \underbrace{ \left( {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)} \right) }_{q_k}
    + \underbrace{ \left( f(\mathbf{x}^{(k)}) \right) }_{r_k}
\end{align*}
which is a quadratic function in \( \alpha \) with \( p_k > 0 \; \left( \because \mathbf{Q} \succ \mathbf{0} \right) \) and \( q_k < 0 \; \left( \because \mathbf{p}^{(k)} \in \mathcal{DS}(\mathbf{x}^{(k)}) \right) \).

For the \hyperlink{1D-quadratic}{one-dimensional quadratic} case, we can then compute the minimum point and minimum value as
\begin{equation*}
    \alpha_k
    = -\frac{q_k}{2 p_k}
    \implies
    \boxed{
        \alpha_k
        = -\frac{{\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)}}{{\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
    }
    > 0
\end{equation*}
\begin{equation*}
    f \left( \mathbf{x}^{(k+1)} \right)
    =
    g_k(\alpha_k)
    =
    r_k - \frac{q_k^2}{4 p_k}
    \implies
    \boxed{
        f(\mathbf{x}^{(k+1)})
        =
        f(\mathbf{x}^{(k)}) - \frac{{\left( {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)} \right)}^2}{2 {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
    }
\end{equation*}

For the choice \( \mathbf{p}^{(k)} = -\nabla f(\mathbf{x}^{(k)}) \), we have
\begin{equation*}
    \alpha_k
    =
    \frac{{\Vert \nabla f(\mathbf{x}^{(k)}) \Vert}^2} {{\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{Q} \nabla f(\mathbf{x}^{(k)})},
    \qquad
    f(\mathbf{x}^{(k+1)})
    =
    f(\mathbf{x}^{(k)}) - \frac{{\Vert \nabla f(\mathbf{x}^{(k)}) \Vert}^4}{2 {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{Q} \nabla f(\mathbf{x}^{(k)})}
\end{equation*}

\paragraph{Analysis:}

For a local minimum \( \mathbf{x}^* \), and any \( \mathbf{p} \in \mathbb{R}^d \), we have
\begin{align*}
    f(\mathbf{x}^* + \mathbf{p})
    & =
    \frac{1}{2} {(\mathbf{x}^* + \mathbf{p})}^{\top} \mathbf{Q} (\mathbf{x}^* + \mathbf{p}) + \mathbf{h}^{\top} (\mathbf{x}^* + \mathbf{p}) + c
    \\ & =
    \frac{1}{2} {\mathbf{x}^*}^{\top} \mathbf{Q} \mathbf{x}^* + {\mathbf{x}^*}^{\top} \mathbf{Q} \mathbf{p} + \frac{1}{2} \mathbf{p}^{\top} \mathbf{Q} \mathbf{p} + \mathbf{h}^{\top} \mathbf{x}^* + \mathbf{h}^{\top} \mathbf{p} + c
    \\ & =
    \left( \frac{1}{2} {\mathbf{x}^*}^{\top} \mathbf{Q} \mathbf{x}^* + \mathbf{h}^{\top} \mathbf{x}^* + c \right)
    + {\cancel{\left( \mathbf{Q} \mathbf{x}^* + \mathbf{h} \right)}}^{\top} \mathbf{p}
    + \frac{1}{2} \mathbf{p}^{\top} \mathbf{Q} \mathbf{p}
    \\ & =
    f(\mathbf{x}^*) + \frac{1}{2} \mathbf{p}^{\top} \mathbf{Q} \mathbf{p}
    \\
    \implies
    f(\mathbf{x})
    & =
    f(\mathbf{x}^*) + \frac{1}{2} {(\mathbf{x} - \mathbf{x}^*)}^{\top} \mathbf{Q} (\mathbf{x} - \mathbf{x}^*), \quad \forall \mathbf{x} \in \mathbb{R}^d
\end{align*}

Now, consider the error at the \( k^{th} \) iteration defined as
\begin{align*}
    E_k
    & \triangleq
    f(\mathbf{x}^{(k)}) - f(\mathbf{x}^*)
    \\
    \implies
    E_k
    & =
    \frac{1}{2} {\left( \mathbf{x}^{(k)} - \mathbf{x}^* \right)}^{\top} \mathbf{Q} \left( \mathbf{x}^{(k)} - \mathbf{x}^* \right)
    \\
    \because
    \nabla f(\mathbf{x}^*)
    & =
    \mathbf{Q} \mathbf{x}^* + \mathbf{h}
    =
    \mathbf{0}
    \\
    \implies
    \nabla f(\mathbf{x}^{(k)})
    & =
    \mathbf{Q} \mathbf{x}^{(k)} + \mathbf{h}
    =
    \mathbf{Q} \left( \mathbf{x}^{(k)} - \mathbf{x}^* \right)
    \\
    \implies
    \left( \mathbf{x}^{(k)} - \mathbf{x}^* \right)
    & =
    \mathbf{Q}^{-1} \, \nabla f(\mathbf{x}^{(k)})
    \\
    \implies
    E_k
    & =
    \frac{1}{2} {\left( \mathbf{x}^{(k)} - \mathbf{x}^* \right)}^{\top} \, \nabla f(\mathbf{x}^{(k)})
    =
    \frac{1}{2} {\Big( \mathbf{Q}^{-1} \, \nabla f \left( \mathbf{x}^{(k)} \right) \Big)}^{\top} \nabla f(\mathbf{x}^{(k)})
    \\ & =
    \frac{1}{2} {\left( \nabla f(\mathbf{x}^{k}) \right)}^{\top} {\left( \mathbf{Q}^{-1} \right)}^\top \nabla f(\mathbf{x}^{(k)})
    =
    \frac{1}{2} {\left( \nabla f(\mathbf{x}^{k}) \right)}^{\top} \mathbf{Q}^{-1} \, \nabla f(\mathbf{x}^{(k)})
\end{align*}

Now, the successive error reduction can be computed as
\begin{align*}
    E_k - E_{k+1}
    & =
    f(\mathbf{x}^{(k)}) - f(\mathbf{x}^{(k+1)})
    =
    \frac{{\left( {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)} \right)}^2}{2 {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
    \\
    \implies
    \frac{E_{k} - E_{k+1}}{E_k}
    & =
    \frac{{\left( {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)} \right)}^2}{ {\left( \nabla f(\mathbf{x}^{k}) \right)}^{\top} \mathbf{Q}^{-1} \, \nabla f(\mathbf{x}^{(k)}) \; {\mathbf{p}^{(k)}}^{\top} \mathbf{Q} \mathbf{p}^{(k)}}
\end{align*}

For the choice \( \mathbf{p}^{(k)} = -\nabla f(\mathbf{x}^{(k)}) \), we have
\begin{align*}
    \frac{E_{k} - E_{k+1}}{E_k}
    & =
    \frac{{\left( {\nabla f(\mathbf{x}^{(k)})}^{\top} \left( -\nabla f(\mathbf{x}^{(k)}) \right) \right)}^2}{ {\left( \nabla f(\mathbf{x}^{k}) \right)}^{\top} \mathbf{Q}^{-1} \, \nabla f(\mathbf{x}^{(k)}) \; {\left( -\nabla f(\mathbf{x}^{(k)}) \right)}^{\top} \mathbf{Q} \left( -\nabla f(\mathbf{x}^{(k)}) \right)}
    \\ & =
    \frac{{\Vert \nabla f(\mathbf{x}^{(k)}) \Vert}^4}{ {\left( \nabla f(\mathbf{x}^{k}) \right)}^{\top} \mathbf{Q}^{-1} \, \nabla f(\mathbf{x}^{(k)}) \; {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{Q} \nabla f(\mathbf{x}^{(k)})}
\end{align*}

From Kantorovich inequality~\psecref{sec:kantorovich-inequality}, we have
\begin{equation*}
    \frac{{\Vert \mathbf{z} \Vert}^4}{(\mathbf{z}^{\top} \mathbf{Q} \mathbf{z}) (\mathbf{z}^{\top} \mathbf{Q}^{-1} \mathbf{z})}
    \geq
    \frac{4 \lambda_{\min}(\mathbf{Q}) \lambda_{\max}(\mathbf{Q})}{{\left( \lambda_{\min}(\mathbf{Q}) + \lambda_{\max}(\mathbf{Q}) \right)}^2},
    \quad \forall \mathbf{z} \in \mathbb{R}^d \setminus \{ \mathbf{0} \}
\end{equation*}
where \( \lambda_{\min}(\mathbf{Q}) \) and \( \lambda_{\max}(\mathbf{Q}) \) are the minimum and maximum eigenvalues of \( \mathbf{Q} \), respectively.

Applying this with \( \mathbf{z} = \nabla f(\mathbf{x}^{(k)}) \), we have
\begin{align*}
    \frac{E_k - E_{k+1}}{E_k}
    & \geq
    \frac{4 \lambda_{\min}(\mathbf{Q}) \lambda_{\max}(\mathbf{Q})}{{\left( \lambda_{\min}(\mathbf{Q}) + \lambda_{\max}(\mathbf{Q}) \right)}^2}
    \\
    \implies
    \frac{E_{k+1}}{E_k}
    & \leq
    1 - \frac{4 \lambda_{\min}(\mathbf{Q}) \lambda_{\max}(\mathbf{Q})}{{\left( \lambda_{\min}(\mathbf{Q}) + \lambda_{\max}(\mathbf{Q}) \right)}^2}
    =
    {\left( \frac{\lambda_{\max}(\mathbf{Q}) - \lambda_{\min}(\mathbf{Q})}{\lambda_{\max}(\mathbf{Q}) + \lambda_{\min}(\mathbf{Q})} \right)}^2
    \triangleq
    \rho
\end{align*}
\begin{equation*}
    \implies
    \frac{E_k}{E_0}
    =
    \prod_{i=0}^{k-1} \frac{E_{i+1}}{E_i}
    \leq
    \prod_{i=0}^{k-1} \rho
    =
    \rho^k
    \implies
    E_k
    \leq
    \rho^k E_0
\end{equation*}
where \( \rho \in \rinterval{0}{1} \) since \( \mathbf{Q} \succ \mathbf{0} \implies \lambda_{\max}(\mathbf{Q}) \geq \lambda_{\min}(\mathbf{Q}) > 0 \), and \( 4ab \leq {(a+b)}^2, \; \forall a, b \).

\subsection{Inexact line search}

\paragraph{Sufficient decrease condition (Armijo rule):}
\begin{equation*}
    f \left( \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \right)
    \leq
    f(\mathbf{x}^{(k)}) + c_1 \alpha_k {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)},
    \quad c_1 \in (0, 1)
\end{equation*}

\paragraph{Curvature condition (Wolfe condition):}
\begin{equation*}
    {\nabla f \left( \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \right)}^{\top} \mathbf{p}^{(k)}
    \geq
    c_2 {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)},
    \quad c_2 \in (c_1, 1)
\end{equation*}

\paragraph{Strong curvature condition (Strong Wolfe condition):}
\begin{equation*}
    \left| {\nabla f \left( \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)} \right)}^{\top} \mathbf{p}^{(k)} \right|
    \leq
    c_2 \left| {\nabla f(\mathbf{x}^{(k)})}^{\top} \mathbf{p}^{(k)} \right|,
    \quad c_2 \in (c_1, 1)
\end{equation*}

\begin{lemma}{Sufficient decrease lemma (Armijo condition)}{}
    Suppose \( f \in \mathcal{C}^{1}_{L} \) is bounded below, then \( \forall \mathbf{x} \in \mathbb{R}^n, \; \alpha \in \linterval[scaled]{0}{\frac{1}{L}} \), the gradient descent update \( \mathbf{x}^+ = \mathbf{x} - \alpha \nabla f(\mathbf{x}) \) yields a sufficient decrease in the function value:
    \begin{equation*}
        f(\mathbf{x}^+) \leq f(\mathbf{x}) - \frac{\alpha}{2} {\Vert \nabla f(\mathbf{x}) \Vert}^2
    \end{equation*}
\end{lemma}
