
\part{Constrained optimisation}

\chapter{Introduction}

\section{Constrained optimisation problem}

A \textbf{constrained minimisation problem} aims to find a point \( \xstar \in \calC \) that minimises a function \( f: \calD \subseteq \R^d \to \R \), i.e.,
\vspace{-1.5em}
\begin{align*}
    &
    \minimize_{\x \in \calC} \func{f}{\x}
    \\
    \func{f}{\xstar}
    =
    \min_{\x \in \calC} \func{f}{\x}
    \quad \iff \quad
    \xstar
    & =
    \argmin_{\x \in \calC} \func{f}{\x}
    \quad \iff \quad
    \func{f}{\xstar}
    \leq
    \func{f}{\x},
    \quad \forall \x \in \calC
\end{align*}

The set \( \calC \subseteq \calD \) is called the \textbf{feasible region} / \textbf{feasible set} / \textbf{solution space}.

If \( \calC = \emptyset \), then the problem is said to be \textbf{infeasible}.

\section{Types of minimum}

\subsection{Global minimum}

\begin{definition}{Global minimum}{global-minimum-constrained}
    The point \( \xstar \in \calD \) is a \textbf{global minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x},
        \quad \forall \x \in \calD
    \end{equation*}
\end{definition}

\subsection{Strict global minimum}

\begin{definition}{Strict global minimum}{}
    The point \( \xstar \in \calD \) is a \textbf{strict global minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x},
        \quad \forall \x \in \calD \setminus \set{\xstar}
    \end{equation*}
\end{definition}

\subsection{Local minimum}

\begin{definition}{Local minimum}{local-minimum-constrained}
    The point \( \xstar \in \calD \) is a \textbf{local minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \in \calD \) in the \( \delta \)-neighborhood of \( \xstar \), we have \( \func{f}{\xstar} \leq \func{f}{\x} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x},
        \quad \forall \x \in \calD \cap B_{\delta}(\xstar)
    \end{equation*}
\end{definition}

\begin{definition}{Strict local minimum}{}
    The point \( \xstar \in \calD \) is a \textbf{strict local minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \in \calD \) in the \( \delta \)-neighborhood of \( \xstar \) except \( \xstar \) itself, we have \( \func{f}{\xstar} < \func{f}{\x} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x},
        \quad \forall \x \in \calD \cap B_{\delta}(\xstar) \setminus \set{\xstar}
    \end{equation*}
\end{definition}

\section{Projection onto a convex set}

\begin{definition}{Projection onto a convex set}{}
    The \textbf{projection} \( P_{\calC}: \R^d \to \calC \) of a point \( \x \in \R^d \) onto a non-empty, closed, convex set \( \calC \subseteq \R^d \) is defined as
    \vspace{-0.5em}
    \begin{equation*}
        \func{P_{\calC}}{\x}
        \triangleq
        \argmin_{\z \in \calC}
        \half \norm{\x - \z}^2
    \end{equation*}
\end{definition}

From the definition, we can see that the following holds true:
\begin{itemize}
    \item The projection always exists since \( \calC \) is non-empty and closed.

    \item The projection is unique since \( \calC \) is convex.

    \item \( \func{P_{\calC}}{\x} \in \calC, \; \forall \x \in \R^d \).

    \item \( \func{P_{\calC}}{\x} \) is the point in \( \calC \) closest to \( \x \).

    \item \( \func{P_{\calC}}{\x} = \x, \; \forall \x \in \calC \).
\end{itemize}

\begin{theorem}{Characterisation of projection onto a convex set}{}
    Let \( \calC \subseteq \R^d \) be a non-empty, closed, convex set.
    The projection \( \func{P_{\calC}}{\x} \) exists and is unique for any \( \x \in \R^d \).
    Furthermore, \( \z = \func{P_{\calC}}{\x} \) if and only if \( \dotp{\pbrac{\x - \z}}{\pbrac{\y - \z}} \leq 0, \; \forall \y \in \calC \).
\end{theorem}

\begin{proof}
    Let \( \z = \func{P_{\calC}}{\x} \).
    Since \( \calC \) is non-empty and closed, the projection exists.
    Since \( \calC \) is convex, the projection is unique.

    For any \( \y \in \calC, \; t \in (0, 1) \), we have \( \z + t(\y - \z) \in \calC \).
    Thus,
    \begin{align*}
        &
        \half \norm{\x - \z}^2
        \leq
        \half \norm{\x - (\z + t(\y - \z))}^2
        =
        \half \norm{(\x - \z) - t(\y - \z)}^2
        \\ &
        0
        \leq
        -2t\dotp{\pbrac{\x - \z}}{\pbrac{\y - \z}} + t^2\norm{\y - \z}^2
    \end{align*}
    Dividing by \( t > 0 \) and letting \( t \to 0^+ \), we get \( 0 \leq -2\dotp{\pbrac{\x - \z}}{\pbrac{\y - \z}} \), i.e., \( \dotp{\pbrac{\x - \z}}{\pbrac{\y - \z}} \leq 0, \; \forall \y \in \calC. \)
\end{proof}

\section{Separating hyperplane theorem}

\begin{theorem}{Separating hyperplane theorem}{separating-hyperplane-theorem}
    Let \( \calC, \calD \subseteq \R^d \) be non-empty, disjoint, closed, convex sets.
    If one of the sets is compact, then there exists a hyperplane that separates them.
    That is, there exists \( \b \in \R^d \setminus \set{\zero}, \; c \in \R \) such that
    \vspace{-0.5em}
    \begin{equation*}
        \dotp{\b}{\x}
        \leq c
        <
        \dotp{\b}{\y},
        \quad
        \forall \x \in \calC,
        \; \forall \y \in \calD
    \end{equation*}
\end{theorem}

\begin{proof}
    Since one of the sets is compact, there exists \( \x_0 \in \calC, \; \y_0 \in \calD \) such that
    \begin{equation*}
        \norm{\x_0 - \y_0}
        =
        \min_{\x \in \calC, \, \y \in \calD} \norm{\x - \y}
        >
        0
    \end{equation*}
    Let \( \b = \y_0 - \x_0, \; c = \dotp{\b}{\pbrac{\x_0 + \y_0}/2} \).
    We will show that this hyperplane separates \( \calC, \calD \).

    Suppose there exists \( \x' \in \calC \) such that \( \dotp{\b}{\x'} > c \).
    Then,
    \begin{align*}
        &
        0
        <
        2\dotp{\b}{\x' - (\x_0 + \y_0)/2}
        =
        2\dotp{\b}{\pbrac{\x' - \x_0}/2} - 2\dotp{\b}{\pbrac{\y_0 - \x_0}/2}
        =
        2\dotp{\b}{\pbrac{\x' - \x_0}/2} - \norm{\b}^2
        \\
        \implies
        &
        4 \norm{\b}^2 \norm{\pbrac{\x' - \x_0}/2}^2
        >
        4 \norm{\b}^2 \dotp{\b}{\pbrac{\x' - \x_0}/2}
        >
        4 \norm{\b}^4
        \\ &
        4 \norm{\pbrac{\x' - \x_0}/2}^2
        >
        4 \norm{\b}^2
        =
        4 \norm{\y_0 - \x_0}^2
    \end{align*}
    This is a contradiction since \( \x', \x_0 \in \calC, \; \y_0 \in \calD \) and \( \calC, \calD \) are disjoint.
    Thus, \( \dotp{\b}{\x} \leq c, \; \forall \x \in \calC \).
\end{proof}

\section{Farkas' lemma}

\begin{theorem}{Farkas' lemma}{}
    For \( \A \in \R^{m \times n}, \, \b \in \R^m \), exactly one of the following statements is true:
    \begin{enumerate}
        \item \( \exists \x \in \R^n \text{ such that } \A \x = \b \text{ and } \x \geq 0 \).

        \item \( \exists \y \in \R^m \text{ such that } \dotp{\A}{\y} \geq 0 \text{ and } \dotp{\b}{\y} < 0 \).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Suppose both statements are true.
    Then, we have a contradiction since this implies
    \begin{equation*}
        \dotp{\b}{\y}
        =
        \dotp{\pbrac{\A \x}}{\y}
        =
        \dotp{\x}{\dotp{\A}{\y}} \geq 0
    \end{equation*}

    Now, suppose the first statement is false.
    Then, consider the closed convex cone \( \func{C}{\A} \triangleq \set{\A \x \given \x \geq 0} \), which does not contain \( \b \).
    By the separating hyperplane theorem~\pthmref{thm:separating-hyperplane-theorem}, there exists a hyperplane that separates \( \func{C}{\A} \) and \( \b \).
    Thus, there exists \( \y \in \R^m, \; c \in \R \) such that
    \begin{equation*}
        \dotp{\pbrac{\A \x}}{\y}
        =
        \dotp{\x}{\dotp{\A}{\y}}
        \geq c
        > \dotp{\b}{\y},
        \quad \forall \x \geq 0
    \end{equation*}
    Since this holds \( \forall \x \geq 0, \; c > 0, \) we must have \( \dotp{\A}{\y} \geq 0 \),
    Thus, the second statement is true.
\end{proof}

\section{Constrained optimisation problem form}\label{sec:constrained-optimisation-problem-form}

A constrained optimisation problem can be put into the following form
\begin{align*}
    \minimize_{\x} \quad & \func{f}{\x} \\
    \subjectto \quad
    & \func{g_i}{\x} \leq 0, \quad i \in \set{1, 2, \ldots, m} \\
    & \func{h_j}{\x} = 0, \quad j \in \set{1, 2, \ldots, p}
\end{align*}

The functions \( g_i \) are called \textbf{inequality constraints} and the functions \( h_j \) are called \textbf{equality constraints}.

\section{Lagrangian}

\subsection{Lagrangian function}

The \textbf{Lagrangian function} \( \calL: \R^d \times \R^m \times \R^p \to \R \) associated with the constrained optimisation problem in~\psecref{sec:constrained-optimisation-problem-form} is defined as
\begin{equation*}
    \func{\calL}{\x, \boldsymbol{\lambda}, \boldsymbol{\mu}}
    \triangleq
    \func{f}{\x}
    +
    \sum_{i = 1}^{m} \lambda_i \, \func{g_i}{\x}
    +
    \sum_{j = 1}^{p} \mu_j \, \func{h_j}{\x}
\end{equation*}

\subsection{Lagrange multipliers}

The variables \( \lambda_i, \; i \in \set{1, 2, \ldots, m}, \) and \( \mu_j, \; j \in \set{1, 2, \ldots, p}, \) are called the \textbf{Lagrange multipliers} associated with the inequality and equality constraints respectively.

\subsection{Lagrange dual function}

The \textbf{Lagrange dual function} \( q: \R^m \times \R^p \to \R \) associated with the constrained optimisation problem in~\psecref{sec:constrained-optimisation-problem-form} is defined as
\begin{equation*}
    \func{q}{\boldsymbol{\lambda}, \boldsymbol{\mu}}
    \triangleq
    \inf_{\x} \func{\calL}{\x, \boldsymbol{\lambda}, \boldsymbol{\mu}}
\end{equation*}

\section{Duality}

Consider the constrained optimisation problem as in~\psecref{sec:constrained-optimisation-problem-form}, referred to as the \textbf{primal problem}.
The corresponding \textbf{Lagrange dual problem} is given by
\begin{align*}
    \maximize_{\boldsymbol{\lambda}, \boldsymbol{\mu}} \quad & \func{q}{\boldsymbol{\lambda}, \boldsymbol{\mu}} \\
    \subjectto \quad
    & \lambda_i \geq 0, \quad i \in \set{1, 2, \ldots, m}
\end{align*}

\subsection{Weak duality}

Let \( \xhat \) be any feasible point of the primal problem and \( (\hat{\boldsymbol{\lambda}}, \hat{\boldsymbol{\mu}}) \) be any feasible point of the dual problem.
Then, we have
\begin{equation*}
    \func{q}{\hat{\boldsymbol{\lambda}}, \hat{\boldsymbol{\mu}}}
    \leq
    \func{f}{\xhat}
\end{equation*}

\subsection{Strong duality}

For a convex primal problem satisfying the Slater's conditions, if \( \xstar \) and \( (\boldsymbol{\lambda}^{\star}, \boldsymbol{\mu}^{\star}) \) are optimal solutions of the primal and dual problems respectively, then
\begin{equation*}
    \func{f}{\xstar}
    =
    \func{q}{\boldsymbol{\lambda}^{\star}, \boldsymbol{\mu}^{\star}}
\end{equation*}

\section{Karush-Kuhn-Tucker (KKT) conditions}\label{sec:kkt-conditions}

Consider the constrained optimisation problem as in~\psecref{sec:constrained-optimisation-problem-form} where \( f, g_i, h_j \) are differentiable functions.
Let \( \xstar \) be a local minimum and suppose that the gradients of the active constraints at \( \xstar \) are linearly independent.
Then, there exist \( \lambda_i \geq 0, \; i \in \set{1, 2, \ldots, m}, \ \mu_j \in \R, \; j \in \set{1, 2, \ldots, p} \) such that
\begin{align*}
    \func{\nabla_{\x} \calL}{\xstar, \boldsymbol{\lambda}, \boldsymbol{\mu}}
    & =
    \func{\nabla_{\x} f}{\xstar}
    +
    \sum_{i = 1}^{m} \lambda_i \, \func{\nabla_{\x} g_i}{\xstar}
    +
    \sum_{j = 1}^{p} \mu_j \, \func{\nabla_{\x} h_j}{\xstar}
    = \zero
    \tag{stationarity}
    \\ &
    \begin{aligned}
        \func{g_i}{\xstar}
        & \leq 0,
        \quad \forall i \in \set{1, 2, \ldots, m}
        \\
        \func{h_j}{\xstar}
        & = 0,
        \quad \forall j \in \set{1, 2, \ldots, p}
    \end{aligned}
    \tag{primal feasibility}
    \\
    \lambda_i
    & \geq 0,
    \quad \forall i \in \set{1, 2, \ldots, m}
    \tag{dual feasibility}
    \\
    \lambda_i \, \func{g_i}{\xstar}
    & = 0,
    \quad \forall i \in \set{1, 2, \ldots, m}
    \tag{complementary slackness}
\end{align*}

\section{Convex programming}

\begin{equation*}
    \minimize_{\x} \func{f}{\x}
    \quad \subjectto \quad
    \func{f_i}{\x} \leq 0, \quad i \in \set{1, 2, \ldots, m}
\end{equation*}
where \( f, \,\, f_i, \, i \in \set{1, 2, \dots, m} \) are convex functions is called a \textbf{convex programming} problem.

For a convex programming problem satisfying the Slater's conditions, if \( \xstar \) and \( \boldsymbol{\lambda}^{\star} \) are optimal solutions of the primal and dual problems respectively, then the KKT conditions~\psecref{sec:kkt-conditions} are necessary and sufficient for optimality.

\subsection{Convex Equality Quadratic Programming (CEQP)}

\begin{align*}
    \minimize_{\x}
    \quad &
    \half \qf{\x}{\Q} + \dotp{\h}{\x} + c
    \\ \subjectto \quad &
    \A \x = \b
\end{align*}
where \( \Q \in \PSD, \; \h \in \R^d, \; \A \in \R^{m \times d}, \; \b \in \R^m \).

The Lagrangian function is given by
\begin{align*}
    \func{\calL}{\x, \boldsymbol{\mu}}
    & =
    \half \qf{\x}{\Q} + \dotp{\h}{\x} + c
    -
    \dotp{\boldsymbol{\mu}}{\pbrac{\A \x - \b}}
    \\
    \implies
    \func{\nabla_{\x} \calL}{\x, \boldsymbol{\mu}}
    & =
    \Q \x + \h - \dotp{\A}{\boldsymbol{\mu}}
\end{align*}

KKT conditions:
\begin{align*}
    \func{\nabla_{\x} \calL}{\xstar, \boldsymbol{\mu}^\ast}
    & =
    \Q \xstar + \h - \dotp{\A}{\boldsymbol{\mu}^\ast}
    =
    \zero
    \\
    \A \xstar
    & =
    \b
\end{align*}

\subsection{Convex Inequality Quadratic Programming (CIQP)}

\begin{align*}
    \minimize_{\x}
    \quad &
    \half \qf{\x}{\Q} + \dotp{\h}{\x} + c
    \\ \subjectto \quad &
    \A \x \geq \b
\end{align*}
where \( \Q \in \PSD, \; \h \in \R^d, \; \A \in \R^{m \times d}, \; \b \in \R^m \).

The Lagrangian function is given by
\begin{align*}
    \func{\calL}{\x, \boldsymbol{\lambda}}
    & =
    \half \qf{\x}{\Q} + \dotp{\h}{\x} + c
    -
    \dotp{\boldsymbol{\lambda}}{\pbrac{\A \x - \b}}
    \\
    \implies
    \func{\nabla_{\x} \calL}{\x, \boldsymbol{\lambda}}
    & =
    \Q \x + \h - \dotp{\A}{\boldsymbol{\lambda}}
\end{align*}

KKT conditions:
\begin{align*}
    \func{\nabla_{\x} \calL}{\xstar, \boldsymbol{\lambda}^\ast}
    & =
    \Q \xstar + \h - \dotp{\A}{\boldsymbol{\lambda}^\ast}
    =
    \zero
    \\
    \A \xstar
    & =
    \b
\end{align*}

\section{Active set methods}

Active set methods are iterative methods for solving convex constrained optimisation problems.
At each iteration, an estimate of the active set of constraints is maintained and used to solve a simpler optimisation problem.
The active set is updated based on the solution obtained at each iteration until convergence is achieved.
\begin{equation*}
    A(\x)
    \triangleq
    \set{i \in \set{1, 2, \ldots, m} \given \func{g_i}{\x} = 0}
\end{equation*}
