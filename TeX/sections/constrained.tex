
\part{Constrained optimisation}

\chapter{Introduction}

\section{Types of minimum}

\subsection{Global minimum}

\begin{definition}{Global minimum}{global-minimum-constrained}
    The point \( \xstar \in \calD \) is a \textbf{global minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x},
        \quad \forall \x \in \calD
    \end{equation*}
\end{definition}

\subsection{Strict global minimum}

\begin{definition}{Strict global minimum}{}
    The point \( \xstar \in \calD \) is a \textbf{strict global minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x},
        \quad \forall \x \in \calD \setminus \set{\xstar}
    \end{equation*}
\end{definition}

\subsection{Local minimum}

\begin{definition}{Local minimum}{local-minimum-constrained}
    The point \( \xstar \in \calD \) is a \textbf{local minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \in \calD \) in the \( \delta \)-neighborhood of \( \xstar \), we have \( \func{f}{\xstar} \leq \func{f}{\x} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} \leq \func{f}{\x},
        \quad \forall \x \in \calD \cap B_{\delta}(\xstar)
    \end{equation*}
\end{definition}

\begin{definition}{Strict local minimum}{}
    The point \( \xstar \in \calD \) is a \textbf{strict local minimum} of the function \( f: \calD \subseteq \R^d \to \R \) if there exists a \( \delta > 0 \) such that for all \( \x \in \calD \) in the \( \delta \)-neighborhood of \( \xstar \) except \( \xstar \) itself, we have \( \func{f}{\xstar} < \func{f}{\x} \), i.e.,
    \vspace{-0.5em}
    \begin{equation*}
        \func{f}{\xstar} < \func{f}{\x},
        \quad \forall \x \in \calD \cap B_{\delta}(\xstar) \setminus \set{\xstar}
    \end{equation*}
\end{definition}

\section{Constrained optimisation problem}\label{sec:constrained-optimisation-problem}

A \textbf{constrained optimisation problem} is of the form
\begin{align*}
    \minimize_{\x} \quad & \func{f}{\x} \\
    \subjectto \quad
    & \func{g_i}{\x} \leq 0, \quad i \in \set{1, 2, \ldots, m} \\
    & \func{h_j}{\x} = 0, \quad j \in \set{1, 2, \ldots, p}
\end{align*}

The functions \( g_i \) are called \textbf{inequality constraints} and the functions \( h_j \) are called \textbf{equality constraints}.

\section{Separating hyperplane theorem}

\begin{theorem}{Separating hyperplane theorem}{separating-hyperplane-theorem}
    Let \( \calC, \calD \subseteq \R^d \) be non-empty, disjoint, closed, convex sets.
    If one of the sets is compact, then there exists a hyperplane that separates them.
    That is, there exists \( \b \in \R^d \setminus \set{\zero}, \; c \in \R \) such that
    \vspace{-0.5em}
    \begin{equation*}
        \dotp{\b}{\x}
        \leq c
        <
        \dotp{\b}{\y},
        \quad
        \forall \x \in \calC,
        \; \forall \y \in \calD
    \end{equation*}
\end{theorem}

\begin{proof}
    Since one of the sets is compact, there exists \( \x_0 \in \calC, \; \y_0 \in \calD \) such that
    \begin{equation*}
        \norm{\x_0 - \y_0}
        =
        \min_{\x \in \calC, \, \y \in \calD} \norm{\x - \y}
        >
        0
    \end{equation*}
    Let \( \b = \y_0 - \x_0, \; c = \dotp{\b}{\pbrac{\x_0 + \y_0}/2} \).
    We will show that this hyperplane separates \( \calC, \calD \).

    Suppose there exists \( \x' \in \calC \) such that \( \dotp{\b}{\x'} > c \).
    Then,
    \begin{align*}
        &
        0
        <
        2\dotp{\b}{\x' - (\x_0 + \y_0)/2}
        =
        2\dotp{\b}{\pbrac{\x' - \x_0}/2} - 2\dotp{\b}{\pbrac{\y_0 - \x_0}/2}
        =
        2\dotp{\b}{\pbrac{\x' - \x_0}/2} - \norm{\b}^2
        \\
        \implies
        &
        4 \norm{\b}^2 \norm{\pbrac{\x' - \x_0}/2}^2
        >
        4 \norm{\b}^2 \dotp{\b}{\pbrac{\x' - \x_0}/2}
        >
        4 \norm{\b}^4
        \\ &
        4 \norm{\pbrac{\x' - \x_0}/2}^2
        >
        4 \norm{\b}^2
        =
        4 \norm{\y_0 - \x_0}^2
    \end{align*}
    This is a contradiction since \( \x', \x_0 \in \calC, \; \y_0 \in \calD \) and \( \calC, \calD \) are disjoint.
    Thus, \( \dotp{\b}{\x} \leq c, \; \forall \x \in \calC \).
\end{proof}

\section{Farkas' lemma}

\begin{theorem}{Farkas' lemma}{}
    For \( \A \in \R^{m \times n}, \, \b \in \R^m \), exactly one of the following statements is true:
    \begin{enumerate}
        \item \( \exists \x \in \R^n \text{ such that } \A \x = \b \text{ and } \x \geq 0 \).

        \item \( \exists \y \in \R^m \text{ such that } \dotp{\A}{\y} \geq 0 \text{ and } \dotp{\b}{\y} < 0 \).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Suppose both statements are true.
    Then, we have a contradiction since this implies
    \begin{equation*}
        \dotp{\b}{\y}
        =
        \dotp{\pbrac{\A \x}}{\y}
        =
        \dotp{\x}{\dotp{\A}{\y}} \geq 0
    \end{equation*}

    Now, suppose the first statement is false.
    Then, consider the closed convex cone \( \func{C}{\A} \triangleq \set{\A \x \given \x \geq 0} \), which does not contain \( \b \).
    By the separating hyperplane theorem~\pthmref{thm:separating-hyperplane-theorem}, there exists a hyperplane that separates \( \func{C}{\A} \) and \( \b \).
    Thus, there exists \( \y \in \R^m, \; c \in \R \) such that
    \begin{equation*}
        \dotp{\pbrac{\A \x}}{\y}
        =
        \dotp{\x}{\dotp{\A}{\y}}
        \geq c
        > \dotp{\b}{\y},
        \quad \forall \x \geq 0
    \end{equation*}
    Since this holds \( \forall \x \geq 0, \; c > 0, \) we must have \( \dotp{\A}{\y} \geq 0 \),
    Thus, the second statement is true.
\end{proof}

\section{Karush-Kuhn-Tucker (KKT) conditions}

Consider the constrained optimisation problem as in~\psecref{sec:constrained-optimisation-problem} where \( f, g_i, h_j \) are differentiable functions.
Let \( \xstar \) be a local minimum and suppose that the gradients of the active constraints at \( \xstar \) are linearly independent.
Then, there exist \( \lambda_i \geq 0, \; i \in \set{1, 2, \ldots, m}, \ \mu_j \in \R, \; j \in \set{1, 2, \ldots, p} \) such that
\begin{align*}
    \partial \func{f}{\xstar}
    & +
    \sum_{i = 1}^{m} \lambda_i \, \partial \func{g_i}{\xstar}
    +
    \sum_{j = 1}^{p} \mu_j \, \partial \func{h_j}{\xstar}
    \ni \zero
    \tag{Stationarity}
    \\ &
    \begin{aligned}
        \func{g_i}{\xstar}
        & \leq 0,
        \quad i \in \set{1, 2, \ldots, m}
        \\
        \func{h_j}{\xstar}
        & = 0,
        \quad j \in \set{1, 2, \ldots, p}
    \end{aligned}
    \tag{Primal feasibility}
    \\
    \lambda_i
    & \geq 0,
    \quad i \in \set{1, 2, \ldots, m}
    \tag{Dual feasibility}
    \\
    \lambda_i \, \func{g_i}{\xstar}
    & = 0,
    \quad i \in \set{1, 2, \ldots, m}
    \tag{Complementary slackness}
\end{align*}
