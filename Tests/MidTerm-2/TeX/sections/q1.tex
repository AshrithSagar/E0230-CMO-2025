\section*{Question 1}

Answer True (T) or False (F)
\begin{enumerate}[label= (\alph*)]
    \item Speed up of Newton Method can be observed only when initialized from a point closed to local minimum.

    \item For a \( \classC^2 \) function the Hessian is always positive semi-definite.

    \item Newton method applies to thrice differentiable functions.

    \item At every iteration of the Conjugate Direction Algorithm, when applied to (\texttt{CQ}) function, the function value decreases.

    \item Expanding subspace theorem has an unique minimum when applied to (\texttt{CQ}).

    \item There are functions defined over \( \R^d \) such that Conjugate gradient algorithm is guaranteed to finish in 2 iterations.

    \item In conjugate gradient algorithm the subspace spanned by conjugate directions is same as the subspace spanned by gradients of the iterates.

    \item In Rank one update in Quasi Newton method it can be shown that the algorithm recovers the Inverse of the Hessian for (\texttt{CQ}) functions.

    \item Broyden family based Quasi Newton method is not guaranteed to recover the minimum of (\texttt{CQ}).

    \item Quasi Newton method has lower computational complexity per iteration then Newton method.

\end{enumerate}
