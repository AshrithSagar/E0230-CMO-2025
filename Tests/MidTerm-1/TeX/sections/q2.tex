\section*{Question 2}

Let \( f: \R^{d} \rightarrow \R \) be \( \mathcal{C}_{L}^{1} \).
Consider implementing the Wolfe method of inexact line search for the Algorithm \texttt{DESCENT}.
\begin{enumerate}[label= (\alph*)]
    \item State the Wolfe conditions.

    \item Compute the minimum guaranteed stepsize at \( \mathbf{x}^{(k)} \).

    \item Does the Wolfe method satisfy the Sufficient Decrease Lemma.
        Justify.
\end{enumerate}

\subsection*{Solution}

\subsection*{(a) Wolfe conditions}

\begin{align*}
    \func{f}{\xk + \alpha_k \pk}
    & \leq
    \func{f}{\xk} + c_1 \alpha_k \dotp{\grad{f}{\xk}}{\pk}
    \tag{sufficient decrease}
    \\
    \dotp{\grad{f}{\xk + \alpha_k \pk}}{\pk}
    & \geq
    c_2 \dotp{\grad{f}{\xk}}{\pk}
    \tag{curvature}
    \\
    \text{where } \;&
    0 < c_1 < c_2 < 1
\end{align*}
\begin{align*}
    \func{g}{\alpha_k}
    & \leq
    \func{g}{0} + c_1 \alpha_k \func{g^\prime}{0}
    \tag{sufficient decrease}
    \\
    \func{g^\prime}{\alpha_k}
    & \geq
    c_2 \func{g^\prime}{0}
    \tag{curvature}
    \\
    \text{where } \;&
    0 < c_1 < c_2 < 1,
    \\ &
    \func{g}{\alpha}
    \triangleq
    \func{f}{\xk + \alpha \pk}
    \implies
    \func{g^\prime}{\alpha}
    =
    \dotp{\grad{f}{\xk + \alpha \pk}}{\pk}
\end{align*}

\subsection*{(b) Minimum guaranteed stepsize at \( \xk \)}

Since \( f \in \C^1_L \), we have
\begin{align*}
    \norm{\grad{f}{\y} - \grad{f}{\x}}
    & \leq
    L \norm{\y - \x},
    \quad
    \forall \x, \y \in \R^d
    \\
    \implies
    \norm{\grad{f}{\xk + \alpha \pk} - \grad{f}{\xk}}
    & \leq
    L \norm{\alpha \pk}
    =
    L \alpha \norm{\pk},
    \quad
    \forall \alpha > 0
\end{align*}

Now, from the curvature condition, we have
\begin{align*}
    \implies
    c_2 \dotp{\grad{f}{\xk}}{\pk}
    & \leq
    \dotp{\grad{f}{\xk + \alpha_k \pk}}{\pk}
    \\
    \implies
    (c_2 - 1) \dotp{\grad{f}{\xk}}{\pk}
    & \leq
    \dotp{\Big( \grad{f}{\xk + \alpha_k \pk} - \grad{f}{\xk} \Big)}{\pk}
    \\
\end{align*}
Now, by Cauchy-Schwarz inequality, we get
\begin{align*}
    \implies
    \abs{\dotp{\Big( \grad{f}{\xk + \alpha_k \pk} - \grad{f}{\xk} \Big)}{\pk}}
    & \leq
    \norm{\grad{f}{\xk + \alpha_k \pk} - \grad{f}{\xk}}
    \; \norm{\pk}
    \\ & \leq
    L \alpha_k \norm{\pk}^2
\end{align*}
And since \( x \leq \abs{x}, \; \forall x \in \R \), we finally get
\begin{align*}
    \implies
    (c_2 - 1) \dotp{\grad{f}{\xk}}{\pk}
    & \leq
    L \alpha_k \norm{\pk}^2
\end{align*}
\begin{equation*}
    \implies
    \boxed{
        \alpha_k
        \geq
        \frac{(c_2 - 1)}{L}
        \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\pk}^2}
    }
    > 0
\end{equation*}

\subsection*{(c) Sufficient decrease lemma}

Yes, the Wolfe method satisfies the sufficient decrease lemma.

Sufficient decrease lemma states
\begin{equation*}
    \frac{\func{f}{\xk} - \func{f}{\xk + \alpha_k \pk}}{\norm{\grad{f}{\xk}}^2}
    \geq
    R_k
    \geq
    R
    > 0
\end{equation*}
\begin{align*}
    \implies
    \func{f}{\xk + \alpha_k \pk}
    & \leq
    \func{f}{\xk} + c_1 \alpha_k \dotp{\grad{f}{\xk}}{\pk}
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xk + \alpha_k \pk}
    & \geq
    - c_1 \alpha_k \dotp{\grad{f}{\xk}}{\pk}
\end{align*}
Since \( -x \geq -\abs{x}, \; \forall x \in \R \),
and by Cauchy-Schwarz inequality, we get
\begin{align*}
    \implies
    - c_1 \alpha_k \dotp{\grad{f}{\xk}}{\pk}
    & \geq
    - c_1 \alpha_k \abs{\dotp{\grad{f}{\xk}}{\pk}}
    \\
    \abs{\dotp{\grad{f}{\xk}}{\pk}}
    & \leq
    \norm{\grad{f}{\xk}}
    \; \norm{\pk}
    \\
    \implies
    - c_1 \alpha_k \abs{\dotp{\grad{f}{\xk}}{\pk}}
    & \geq
    - c_1 \alpha_k \norm{\grad{f}{\xk}}
    \; \norm{\pk}
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xk + \alpha_k \pk}
    & \geq
    - c_1 \alpha_k \norm{\grad{f}{\xk}}
    \; \norm{\pk}
    \\
    \because
    \alpha_k
    & \geq
    \frac{(c_2 - 1)}{L}
    \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\pk}^2}
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xk + \alpha_k \pk}
    & \geq
    - c_1
    \frac{(c_2 - 1)}{L}
    \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\pk}^2}
    \norm{\grad{f}{\xk}}
    \; \norm{\pk}
    \\
    \implies
    \frac{\func{f}{\xk} - \func{f}{\xk + \alpha_k \pk}}{\norm{\grad{f}{\xk}}^2}
    & \geq
    - c_1
    \frac{(c_2 - 1)}{L}
    \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\grad{f}{\xk}} \norm{\pk}}
    \\
    \implies
    R_k
    & \triangleq
    - c_1
    \frac{(c_2 - 1)}{L}
    \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\grad{f}{\xk}} \norm{\pk}}
    \\
    \because
    -1
    & \leq
    \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\grad{f}{\xk}} \norm{\pk}}
    \leq
    1
    \\
    \implies
    - \frac{\dotp{\grad{f}{\xk}}{\pk}}{\norm{\grad{f}{\xk}} \norm{\pk}}
    & \geq
    - 1
    \\
    \implies
    R_k
    & \geq
    - c_1
    \frac{(c_2 - 1)}{L}
    =
    \frac{c_1 (1 - c_2)}{L}
    \\
    \implies
    R
    & \triangleq
    \frac{c_1 (1 - c_2)}{L}
\end{align*}

Thus, the Wolfe method satisfies the sufficient decrease lemma with
\begin{equation*}
    \boxed{
        R
        =
        \frac{c_1 (1 - c_2)}{L}
    }
    > 0
\end{equation*}
since \( 0 < c_1 < c_2 < 1 \) and \( L > 0 \).
