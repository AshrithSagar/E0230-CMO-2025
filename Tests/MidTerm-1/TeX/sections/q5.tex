\section*{Question 5}

Let \( \mathbf{x}^{*} \) be the global minimum of a convex function \( f \in \mathcal{C}_{L}^{1} \).
Use \texttt{DESCENT} with \( \mathbf{u}^{(k)} = -\nabla f\left(\mathbf{x}^{(k)}\right) \).
Let \( \alpha_{k} = \frac{\beta}{L} \) where \( 0<\beta<2 \).
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Do all iterates lie in \( S = \left\{ \mathbf{x} \mid\left\Vert\mathbf{x}-\mathbf{x}^{*}\right\Vert \leq\left\Vert\mathbf{x}^{(0)}-\mathbf{x}^{*}\right\Vert \right\} \)?

    \item Do the iterates obey the sufficient decrease lemma?

    \item Find the minimum number of iterations required to ensure a point whose function value is at most \( .01 \) more than the value of global minimum value.
        Assume that \( \left\Vert\mathbf{x}^{(0)}-\mathbf{x}^{*}\right\Vert = 1 \).
\end{enumerate}

\subsection*{Solution}

\paragraph{Given:}
\begin{equation*}
    f \in \C^1_L,
    \quad
    \xstar = \argmin_{\x} \func{f}{\x},
    \quad
    \mathbf{u}^{(k)} = -\grad{f}{\xk},
    \quad
    \alpha_{k} = \frac{\beta}{L}, \; 0 < \beta < 2
\end{equation*}

\subsubsection*{(a)}

Since \( f \in \C^1_L \), we have
\begin{align*}
    \norm{\grad{f}{\x} - \grad{f}{\y}}
    & \leq
    L \, \norm{\x - \y},
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \norm{\grad{f}{\x}}
    & =
    \norm{\grad{f}{\x} - \grad{f}{\xstar}}
    \leq
    L \, \norm{\x - \xstar},
    \quad \forall \x \in \R^d
    \\
    \implies
    \norm{\grad{f}{\x}}^2
    & \leq
    L^2 \, \norm{\x - \xstar}^2,
    \quad \forall \x \in \R^d
\end{align*}
Now, by Cauchy-Schwarz inequality, we have
\begin{align*}
    \abs{\dotp{\x}{\y}}
    & \leq
    \norm{\x} \, \norm{\y},
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \abs{\dotp{\left( \grad{f}{\x} - \grad{f}{\y} \right)}{(\x - \y)}}
    & \leq
    \norm{\grad{f}{\x} - \grad{f}{\y}} \, \norm{\x - \y},
    \quad \forall \x, \y \in \R^d
    \\ & \leq
    L \, \norm{\x - \y}^2,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \abs{\dotp{\grad{f}{\x}}{(\x - \xstar)}}
    & \leq
    L \, \norm{\x - \xstar}^2,
    \quad \forall \x \in \R^d
\end{align*}
Now, we can see that
\begin{align*}
    \xkp
    & =
    \xk + \alpha_k \mathbf{u}^{(k)}
    =
    \xk - \frac{\beta}{L} \grad{f}{\xk}
    \\
    \implies
    \xkp - \xstar
    & =
    \xk - \xstar - \frac{\beta}{L} \grad{f}{\xk}
    \\
    \norm{\xkp - \xstar}^2
    & =
    \norm{\big( \xk - \xstar \big) - \frac{\beta}{L} \grad{f}{\xk}}^2
    \\ & =
    \norm{\xk - \xstar}^2
    + \norm{\frac{\beta}{L} \grad{f}{\xk}}^2
    - 2 \frac{\beta}{L} \dotp{\grad{f}{\xk}}{(\xk - \xstar)}
    \\ & \leq
    \norm{\xk - \xstar}^2
    + \frac{\beta^2}{\cancel{L^2}} \cancel{L^2} \norm{\xk - \xstar}^2
    - 2 \frac{\beta}{\cancel{L}} \cancel{L} \norm{\xk - \xstar}^2
    \\ & =
    (1 + \beta^2 - 2\beta) \, \norm{\xk - \xstar}^2
    =
    {(1 - \beta)}^2 \, \norm{\xk - \xstar}^2
\end{align*}

Since \( 0 < \beta < 2 \implies -2 < -\beta < 0 \implies -1 < 1 - \beta < 1 \implies {(1 - \beta)}^2 < 1 \), we have
\begin{equation*}
    \norm{\xkp - \xstar}^2
    < \norm{\xk - \xstar}^2,
    \quad \forall k \geq 0
\end{equation*}

By induction, we can see that all the iterates lie in the set \( S = \set{ \x \;\middle|\; \norm{\x - \xstar} \leq \norm{\x^{(0)} - \xstar} } \).

\subsubsection*{(b)}

Sufficient decrease lemma states
\begin{equation*}
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{\norm{\grad{f}{\xk}}^2}
    \geq
    R_k
    \geq
    R
    > 0
\end{equation*}

Since \( f \in \C^1_L \), we have
\begin{align*}
    \func{f}{\y}
    & \leq
    \func{f}{\x}
    + \dotp{\grad{f}{\x}}{(\y - \x)}
    + \frac{L}{2} \, \norm{\y - \x}^2,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \func{f}{\xkp}
    & \leq
    \func{f}{\xk}
    + \dotp{\grad{f}{\xk}}{\big( \xkp - \xk \big)}
    + \frac{L}{2} \, \norm{\xkp - \xk}^2
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xkp}
    & \geq
    - \dotp{\grad{f}{\xk}}{\big( \xkp - \xk \big)}
    - \frac{L}{2} \, \norm{\xkp - \xk}^2
    \\ & =
    \frac{\beta}{L} \norm{\grad{f}{\xk}}^2
    - \frac{\cancel{L}}{2} \frac{\beta^2}{L^{\cancel{2}}} \, \norm{\grad{f}{\xk}}^2
    \\
    \implies
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{\norm{\grad{f}{\xk}}^2}
    & \geq
    \frac{\beta}{L}
    - \frac{\beta^2}{2 L}
    =
    \frac{\beta (2 - \beta)}{2 L}
    \\
    \implies
    &
    \boxed{
        R
        \triangleq
        \frac{\beta (2 - \beta)}{2 L}
    }
    > 0,
    \quad \forall k \geq 0
\end{align*}

Thus, the iterates obey the sufficient decrease lemma.
