\section*{Question 5}

Let \( \mathbf{x}^{*} \) be the global minimum of a convex function \( f \in \mathcal{C}_{L}^{1} \).
Use \texttt{DESCENT} with \( \mathbf{u}^{(k)} = -\nabla f\left(\mathbf{x}^{(k)}\right) \).
Let \( \alpha_{k} = \frac{\beta}{L} \) where \( 0<\beta<2 \).
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Do all iterates lie in \( S = \left\{ \mathbf{x} \mid\left\Vert\mathbf{x}-\mathbf{x}^{*}\right\Vert \leq\left\Vert\mathbf{x}^{(0)}-\mathbf{x}^{*}\right\Vert \right\} \)?

    \item Do the iterates obey the sufficient decrease lemma?

    \item Find the minimum number of iterations required to ensure a point whose function value is at most \( .01 \) more than the value of global minimum value.
        Assume that \( \left\Vert\mathbf{x}^{(0)}-\mathbf{x}^{*}\right\Vert = 1 \).
\end{enumerate}

\subsection*{Solution}

\paragraph{Given:}
\begin{equation*}
    f \in \C^1_L,
    \quad
    \xstar = \argmin_{\x} \func{f}{\x},
    \quad
    \mathbf{u}^{(k)} = -\grad{f}{\xk},
    \quad
    \alpha_{k} = \frac{\beta}{L}, \; 0 < \beta < 2
\end{equation*}

\subsubsection*{(a)}

Since \( f \in \C^1_L \), we have
\begin{align*}
    \norm{\grad{f}{\x} - \grad{f}{\y}}
    & \leq
    L \, \norm{\x - \y},
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \norm{\grad{f}{\x}}
    & =
    \norm{\grad{f}{\x} - \grad{f}{\xstar}}
    \leq
    L \, \norm{\x - \xstar},
    \quad \forall \x \in \R^d
    \\
    \implies
    \norm{\grad{f}{\x}}^2
    & \leq
    L^2 \, \norm{\x - \xstar}^2,
    \quad \forall \x \in \R^d
\end{align*}
Now, by Cauchy-Schwarz inequality, we have
\begin{align*}
    \abs{\dotp{\x}{\y}}
    & \leq
    \norm{\x} \, \norm{\y},
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \abs{\dotp{\left( \grad{f}{\x} - \grad{f}{\y} \right)}{(\x - \y)}}
    & \leq
    \norm{\grad{f}{\x} - \grad{f}{\y}} \, \norm{\x - \y},
    \quad \forall \x, \y \in \R^d
    \\ & \leq
    L \, \norm{\x - \y}^2,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \abs{\dotp{\grad{f}{\x}}{(\x - \xstar)}}
    & \leq
    L \, \norm{\x - \xstar}^2,
    \quad \forall \x \in \R^d
\end{align*}
Now, we can see that
\begin{align*}
    \xkp
    & =
    \xk + \alpha_k \mathbf{u}^{(k)}
    =
    \xk - \frac{\beta}{L} \grad{f}{\xk}
    \\
    \implies
    \xkp - \xstar
    & =
    \xk - \xstar - \frac{\beta}{L} \grad{f}{\xk}
    \\
    \norm{\xkp - \xstar}^2
    & =
    \norm{\big( \xk - \xstar \big) - \frac{\beta}{L} \grad{f}{\xk}}^2
    \\ & =
    \norm{\xk - \xstar}^2
    + \norm{\frac{\beta}{L} \grad{f}{\xk}}^2
    - 2 \frac{\beta}{L} \dotp{\grad{f}{\xk}}{(\xk - \xstar)}
    \\ & \leq
    \norm{\xk - \xstar}^2
    + \frac{\beta^2}{\cancel{L^2}} \cancel{L^2} \norm{\xk - \xstar}^2
    - 2 \frac{\beta}{\cancel{L}} \cancel{L} \norm{\xk - \xstar}^2
    \\ & =
    (1 + \beta^2 - 2\beta) \, \norm{\xk - \xstar}^2
    =
    {(1 - \beta)}^2 \, \norm{\xk - \xstar}^2
\end{align*}

Since \( 0 < \beta < 2 \implies -2 < -\beta < 0 \implies -1 < 1 - \beta < 1 \implies {(1 - \beta)}^2 < 1 \), we have
\begin{equation*}
    \norm{\xkp - \xstar}^2
    < \norm{\xk - \xstar}^2,
    \quad \forall k \geq 0
\end{equation*}

By induction, we can see that all the iterates lie in the set \( S = \set{ \x \;\middle|\; \norm{\x - \xstar} \leq \norm{\x^{(0)} - \xstar} } \).

\subsubsection*{(b)}

Sufficient decrease lemma states
\begin{equation*}
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{\norm{\grad{f}{\xk}}^2}
    \geq
    R_k
    \geq
    R
    > 0
\end{equation*}

Since \( f \in \C^1_L \), we have
\begin{align*}
    \func{f}{\y}
    & \leq
    \func{f}{\x}
    + \dotp{\grad{f}{\x}}{(\y - \x)}
    + \frac{L}{2} \, \norm{\y - \x}^2,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \func{f}{\xkp}
    & \leq
    \func{f}{\xk}
    + \dotp{\grad{f}{\xk}}{\big( \xkp - \xk \big)}
    + \frac{L}{2} \, \norm{\xkp - \xk}^2
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xkp}
    & \geq
    - \dotp{\grad{f}{\xk}}{\big( \xkp - \xk \big)}
    - \frac{L}{2} \, \norm{\xkp - \xk}^2
    \\ & =
    \frac{\beta}{L} \norm{\grad{f}{\xk}}^2
    - \frac{\cancel{L}}{2} \frac{\beta^2}{L^{\cancel{2}}} \, \norm{\grad{f}{\xk}}^2
    \\
    \implies
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{\norm{\grad{f}{\xk}}^2}
    & \geq
    \frac{\beta}{L}
    - \frac{\beta^2}{2 L}
    =
    \frac{\beta (2 - \beta)}{2 L}
    \\
    \implies
    &
    R_k
    \triangleq
    \boxed{
        R
        \triangleq
        \frac{\beta (2 - \beta)}{2 L}
    }
    > 0,
    \quad \forall k \geq 0
\end{align*}

Thus, the iterates obey the sufficient decrease lemma.

\subsubsection*{(c)}

We want to find a minimum \( k \) such that
\begin{equation*}
    \func{f}{\xk}
    \leq
    \func{f}{\xstar}
    + 0.01
\end{equation*}
given that \( \norm{\x^{(0)} - \xstar} = 1 \).

Now, since \( f \in \C^1_L \), we have
\begin{align*}
    \func{f}{\y}
    & \leq
    \func{f}{\x}
    + \dotp{\grad{f}{\x}}{(\y - \x)}
    + \frac{L}{2} \, \norm{\y - \x}^2,
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \func{f}{\xk}
    & \leq
    \func{f}{\xstar}
    + \frac{L}{2} \, \norm{\xk - \xstar}^2
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xstar}
    & \leq
    \frac{L}{2} \, \norm{\xk - \xstar}^2
    \leq
    \frac{L}{2} {(1 - \beta)}^{2k} \, \norm{\x^{(0)} - \xstar}^2
    =
    \frac{L}{2} {(1 - \beta)}^{2k}
    \\
    \text{Now, if }
    \frac{L}{2} {(1 - \beta)}^{2k}
    & \leq
    0.01,
    \quad \text{then }
    \func{f}{\xk}
    \leq
    \func{f}{\xstar}
    + 0.01
    \\
    \implies
    {(1 - \beta)}^{2k}
    & \leq
    \frac{0.02}{L}
    \implies
    2k \ln{\abs{1 - \beta}}
    \leq
    \ln{\frac{0.02}{L}}
    =
    -\ln{50} - \ln{L}
    \\
    \implies
    k
    & \geq
    \boxed{
        \frac{-\ln{50} - \ln{L}}{2 \ln{\abs{1 - \beta}}}
    },
    \quad \because \ln{\abs{1 - \beta}} < 0
\end{align*}

Since \( f \) is convex, we have
\begin{align*}
    \func{f}{\y}
    & \geq
    \func{f}{\x}
    + \dotp{\grad{f}{\x}}{(\y - \x)},
    \quad \forall \x, \y \in \R^d
    \\
    \implies
    \func{f}{\xstar}
    & \geq
    \func{f}{\xk}
    + \dotp{\grad{f}{\xk}}{(\xstar - \xk)}
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xstar}
    & \leq
    -\dotp{\grad{f}{\xk}}{(\xstar - \xk)}
    =
    \dotp{\grad{f}{\xk}}{(\xk - \xstar)}
    \\ & \leq
    \abs{\dotp{\grad{f}{\xk}}{(\xk - \xstar)}}
    \leq
    \norm{\grad{f}{\xk}} \, \norm{\xk - \xstar}
    \\
    \because
    \norm{\xk - \xstar}
    & \leq
    \norm{\x^{(0)} - \xstar}
    = 1,
    \quad \forall k \geq 0
    \\
    \implies
    &
    \boxed{
        \func{f}{\xk} - \func{f}{\xstar}
        \leq
        \norm{\grad{f}{\xk}},
        \quad \forall k \geq 0
    }
    \\
    \because
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{\norm{\grad{f}{\xk}}^2}
    & \geq
    R
    > 0,
    \quad \forall k \geq 0
    \\
    \implies
    \norm{\grad{f}{\xk}}^2
    & \leq
    \frac{\func{f}{\xk} - \func{f}{\xkp}}{R},
    \quad \forall k \geq 0
    \\
    \implies
    \func{f}{\xk} - \func{f}{\xkp}
    & \geq
    R {\left( \func{f}{\xk} - \func{f}{\xstar} \right)}^2,
    \quad \forall k \geq 0
\end{align*}

Define \( \delta_k \triangleq \func{f}{\xk} - \func{f}{\xstar} \geq 0 \).
Then, we have
\begin{align*}
    \implies
    \delta_k - \delta_{k+1}
    & \geq
    R \, \delta_k^2,
    \quad \forall k \geq 0
    \\
    \implies
    \delta_{k+1}
    & \leq
    \delta_k - R \, \delta_k^2
    =
    \delta_k (1 - R \, \delta_k)
    \\
    \implies
    \frac{1}{\delta_{k+1}}
    & \geq
    \frac{1}{\delta_k (1 - R \, \delta_k)}
    =
    \frac{1}{\delta_k} \cdot \frac{1}{1 - R \, \delta_k}
    \\
    \because
    \sum_{i = 0}^{\infty} x^i
    =
    \frac{1}{1 - x}
    & \geq
    1 + x,
    \quad \forall x \in \R, \; \abs{x} < 1
    \\
    \implies
    \frac{1}{1 - R \, \delta_k}
    & \geq
    1 + R \, \delta_k,
    \quad \text{if } R \, \delta_k < 1
    \\
    \implies
    \frac{1}{\delta_{k+1}}
    & \geq
    \frac{1}{\delta_k} (1 + R \, \delta_k)
    =
    \frac{1}{\delta_k} + R,
    \quad \text{if } R \, \delta_k < 1
    \\
    \implies
    \frac{1}{\delta_k}
    & \geq
    \frac{1}{\delta_0} + k R
    \geq
    k R,
    \quad \text{if } R \, \delta_i < 1,
    \; \forall i < k
    \\
    \implies
    \delta_k
    & \leq
    \frac{1}{k R},
    \quad \text{if } R \, \delta_i < 1,
    \; \forall i < k
    \\
    \text{Now, }
    \because
    \delta_0
    =
    \func{f}{\x^{(0)}} - \func{f}{\xstar}
    & \leq
    \frac{L}{2} \, \norm{\x^{(0)} - \xstar}^2
    =
    \frac{L}{2}
    = \frac{1}{R} \cdot \frac{\beta (2 - \beta)}{4}
    < \frac{1}{R},
    \quad \because 0 < \beta < 2
    \\
    \implies
    R \, \delta_0
    & < 1
    \\
    \implies
    R \, \delta_i
    & < 1,
    \quad \forall i < k
    \\
    \implies
    &
    \boxed{
        \delta_k
        \leq
        \frac{1}{k R},
        \quad \forall k \geq 1
    }
    \\
    \text{Thus, if }
    \frac{1}{k R}
    & \leq
    0.01,
    \quad \text{then }
    \func{f}{\xk}
    \leq
    \func{f}{\xstar}
    + 0.01
\end{align*}
\begin{equation*}
    \implies
    k
    \geq
    \frac{1}{0.01 R}
    =
    \frac{100}{R}
    =
    \boxed{
        \frac{200 L}{\beta (2 - \beta)}
    }
\end{equation*}
