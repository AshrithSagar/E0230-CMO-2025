\section*{Question 1}

Answer True (T) or False (F)
\begin{enumerate}[label= (\alph*)]
    \item Let \( f:[-1,1] \rightarrow \mathbb{R} \) with the property that \( \left|f^{\prime \prime}(x)\right| \leq \frac{1}{2} \) for all \( x \in(-1,1) \).
        If the global minimum of \( f \) is attained in \( (-1,1) \) and \( \min _{[-1,1]} f(x) = 1 \) then \( f(x) \leq 2 \) for all \( x \in(-1,1) \).

        \begin{solutionbox}
            True.

            Let \( \hat{x} \in (-1,1) \) be the global minimizer, then we have \( f(\hat{x}) = 1, \; f^{\prime}(\hat{x}) = 0 \).
            Now, for any \( x \in (-1,1) \), by Taylor's theorem, there exists \( z \) between \( x \) and \( \hat{x} \) such that
            \begin{align*}
                f(x)
                & =
                f(\hat{x}) + f^{\prime}(\hat{x})(x-\hat{x}) + \frac{f^{\prime \prime}(z)}{2}{(x-\hat{x})}^{2}
                \\
                \implies
                f(x)
                & =
                1 + \frac{f^{\prime \prime}(z)}{2}{(x-\hat{x})}^{2} \leq 1 + \frac{1}{4}{(x-\hat{x})}^{2} \leq 2,
                \quad \because
                |x-\hat{x}| \leq 2
            \end{align*}
        \end{solutionbox}

    \item Let \( f(\mathbf{x}) = \mathbf{a}^{\top} \mathbf{x} \).
        The derivative of \( f \) is \( \mathbf{a} \).

        \begin{solutionbox}
            True.
            \begin{align*}
                f(x)
                & =
                \sum_{i = 1}^{d} a_{i} x_{i},
                \quad \mathbf{x} = {\left[x_{1}, x_{2}, \ldots, x_{d}\right]}^{\top},
                \; \mathbf{a} = {\left[a_{1}, a_{2}, \ldots, a_{d}\right]}^{\top}
                \\
                \implies
                \nabla f(\mathbf{x})
                & =
                {\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{d}}\right]}^{\top} = {\left[a_{1}, a_{2}, \ldots, a_{d}\right]}^{\top} = \mathbf{a}
            \end{align*}
        \end{solutionbox}

        \newpage
    \item The hessian of \( f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^{\top} A \mathbf{x}, A = \mathbf{u v}^{\top}, \mathbf{u} \neq \mathbf{v} \), is given by \( H = A \).

        \begin{solutionbox}
            False.
            \begin{align*}
                A
                & =
                \mathbf{u v}^{\top}
                \implies
                A^{\top}
                =
                {\left(\mathbf{u v}^{\top}\right)}^{\top}
                =
                \mathbf{v u}^{\top}
                \\
                f(\mathbf{x})
                & =
                \frac{1}{2} \mathbf{x}^{\top} A \mathbf{x}
                =
                \frac{1}{2} \mathbf{x}^{\top} \left( \mathbf{u v}^{\top} \right) \mathbf{x}
                =
                \frac{1}{2} {(\mathbf{u}^{\top} \mathbf{x})} (\mathbf{v}^{\top} \mathbf{x})
                \\
                \implies
                \nabla f(\mathbf{x})
                & =
                \frac{1}{2} {(\mathbf{u}^{\top} \mathbf{x})} \nabla {(\mathbf{v}^{\top} \mathbf{x})} + \frac{1}{2} {(\mathbf{v}^{\top} \mathbf{x})} \nabla {(\mathbf{u}^{\top} \mathbf{x})}
                \\ & =
                \frac{1}{2} {(\mathbf{u}^{\top} \mathbf{x})} \mathbf{v} + \frac{1}{2} {(\mathbf{v}^{\top} \mathbf{x})} \mathbf{u}
                =
                \frac{1}{2} (\mathbf{u v}^{\top} + \mathbf{v u}^{\top}) \mathbf{x}
                =
                \frac{1}{2} (A + A^{\top}) \mathbf{x}
                \\
                \implies
                H
                & =
                \nabla^{2} f(\mathbf{x})
                =
                \frac{1}{2} (A + A^{\top})
                \\
                \text{Now, } \quad
                &
                H = A
                \iff
                A = A^{\top}
                \\
                \text{Also, } \quad
                &
                \mathbf{u v}^{\top} = \mathbf{v u}^{\top}
                \iff
                \mathbf{u} = \lambda \mathbf{v}, \; \lambda \in \mathbb{R}
                \\
                \because
                \mathbf{u} \neq \lambda \mathbf{v}
                & \implies
                \mathbf{u v}^{\top} \neq \mathbf{v u}^{\top}
                \implies
                A \neq A^{\top}
                \implies
                H \neq A
            \end{align*}
            Should have been given \( \mathbf{u} \neq \lambda \mathbf{v} \) instead of \( \mathbf{u} \neq \mathbf{v} \).
        \end{solutionbox}

    \item For \( f \), a \( \mathcal{C}^{2} \) function, consider a point \( \hat{\mathbf{x}} \) such that \( \nabla f(\hat{\mathbf{x}}) = 0 \) and the Hessian is positive definite, \( H(\hat{\mathbf{x}}) \succ 0 \).
        Then \( \hat{\mathbf{x}} \) is global minimum of \( f \).

        \begin{solutionbox}
            False.

            From the given conditions, \( f \in \mathcal{C}^{2}, \nabla f(\hat{\mathbf{x}}) = 0, H(\hat{\mathbf{x}}) \succ 0 \), we can say that \( \hat{\mathbf{x}} \) is a strict local minimum of \( f \) by the second order necessary condition for optimalilty.

            To be able to say that \( \hat{\mathbf{x}} \) is a global minimum of \( f \), we need \( f \) to be convex, i.e., \( H(\mathbf{x}) \succeq 0, \; \forall \mathbf{x} \in \operatorname{dom}(f) \), as per the second order sufficient condition for optimality and the second order condition for convexity.
        \end{solutionbox}

    \item Consider the function \( f: \mathbb{R}^{2} \rightarrow \mathbb{R} \) with \( \mathbf{x} = {\left[x_{1}, x_{2}\right]}^{\top}, f(\mathbf{x}) = {\left(1+x_{1}\right)}^{2}+b{\left(x_{2}-x_{1}^{2}\right)}^{2} \) with \( b>0 \).
        The global minimum of \( f \) for \( b = 100 \) and for \( b = 1 \) are different.

        \begin{solutionbox}
            False.
            \vspace{-0.5em}
            \begin{align*}
                \implies
                \nabla f(\mathbf{x})
                & =
                {\left[2(1+x_{1}) - 4b x_{1}(x_{2}-x_{1}^{2}), \; 2b(x_{2}-x_{1}^{2})\right]}^{\top}
                \\
                \implies
                \nabla f(\hat{\mathbf{x}})
                & =
                \mathbf{0}
                \implies
                x_2 = x_1^2, \quad 1 + x_1 = 0
                \implies
                \hat{\mathbf{x}} = {\left[-1, 1\right]}^{\top}
                \\
                \implies
                f(\hat{\mathbf{x}})
                & =
                {(1-1)}^{2} + b{(1-{(-1)}^{2})}^{2} = 0, \quad \forall b > 0
            \end{align*}
            There is only one stationary point and the function value at that point doesn't depend on \( b \).
            Loosely, we can see that the global minimum would be independent of \( b \).
            (Provided with additional checks, which we skip here).

            The given function is the Rosenbrock function \( f(x, y) = {(a-x)}^{2}+b{(y-x^{2})}^{2} \), with \( a = -1, \; b > 0 \).
            It has a unique global minimum at \( (x, y) = (a, a^{2}), \; \forall b > 0 \).
        \end{solutionbox}

    \item Suppose global minimum, \( \hat{\mathbf{x}} \), exists for a function \( f \in \mathcal{C}^{1} \).
        \( \hat{\mathbf{x}} \in \{ \mathbf{x} \mid \nabla f(\mathbf{x}) = 0 \} \).

        \begin{solutionbox}
            True.

            Follows from the first order necessary condition for optimality.
        \end{solutionbox}

    \item Exact line search for minimizing Quadratic function can be implemented only when descent direction is along negative of the gradient.

        \begin{solutionbox}
            False.

            Exact line search can be implemented for any descent direction.
            We have a closed form expression for the case mentioned in the statement, but that doesn't mean we cannot implement exact line search for other descent directions.
        \end{solutionbox}

    \item Armijo-Goldstein procedure for inexact line search will not apply for Quadratic functions.

        \begin{solutionbox}
            False.

            Armijo-Goldstein procedure can be applied to any function.
        \end{solutionbox}

    \item Wolfe procedure for inexact line search is always more efficient than Armijo-Goldstein.

        \begin{solutionbox}
            False.

            There can be cases where Armijo-Goldstein can be more efficient than Wolfe.
        \end{solutionbox}

    \item Strongly convex functions have unique global minimum.

        \begin{solutionbox}
            True.

            Follows from the second order condition for strong convexity and the second order sufficient condition for optimality.

            Can be shown by contradiction, provided the function is non-constant.
        \end{solutionbox}
\end{enumerate}
