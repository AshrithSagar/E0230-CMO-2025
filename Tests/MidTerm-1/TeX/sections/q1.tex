\section*{Question 1}

Answer True (T) or False (F)
\begin{enumerate}[label= (\alph*)]
    \item Let \( f:[-1,1] \rightarrow \R \) with the property that \( \left|f^{\prime \prime}(x)\right| \leq \half \) for all \( x \in(-1,1) \).
        If the global minimum of \( f \) is attained in \( (-1,1) \) and \( \min _{[-1,1]} \func{f}{x} = 1 \) then \( \func{f}{x} \leq 2 \) for all \( x \in(-1,1) \).

        \begin{solutionbox}
            True.

            Let \( \hat{x} \in (-1,1) \) be the global minimizer, then we have \( f(\hat{x}) = 1, \; f^{\prime}(\hat{x}) = 0 \).
            Now, for any \( x \in (-1,1) \), by Taylor's theorem, there exists \( z \) between \( x \) and \( \hat{x} \) such that
            \begin{align*}
                \func{f}{x}
                & =
                f(\hat{x}) + f^{\prime}(\hat{x})(x-\hat{x}) + \frac{f^{\prime \prime}(z)}{2}{(x-\hat{x})}^2
                \\
                \implies
                \func{f}{x}
                & =
                1 + \frac{f^{\prime \prime}(z)}{2}{(x-\hat{x})}^2 \leq 1 + \frac{1}{4}{(x-\hat{x})}^2 \leq 2,
                \quad \because
                |x-\hat{x}| \leq 2
            \end{align*}
        \end{solutionbox}

    \item Let \( \func{f}{\x} = \mathbf{a}^\top \x \).
        The derivative of \( f \) is \( \mathbf{a} \).

        \begin{solutionbox}
            True.
            \begin{align*}
                \func{f}{\x}
                & =
                \sum_{i = 1}^{d} a_i x_i,
                \quad \x = {\left[x_{1}, x_{2}, \ldots, x_{d}\right]}^\top,
                \; \mathbf{a} = {\left[a_{1}, a_{2}, \ldots, a_{d}\right]}^\top
                \\
                \implies
                \grad{f}{\x}
                & =
                {\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{d}}\right]}^\top = {\left[a_{1}, a_{2}, \ldots, a_{d}\right]}^\top = \mathbf{a}
            \end{align*}
        \end{solutionbox}

        \newpage
    \item The hessian of \( \func{f}{\x} = \half \qf{\x}{A}, A = \outpbf{u}{v}, \mathbf{u} \neq \mathbf{v} \), is given by \( H = A \).

        \begin{solutionbox}
            False.
            \begin{align*}
                A
                & =
                \outpbf{u}{v}
                \implies
                A^\top
                =
                {\left(\outpbf{u}{v}\right)}^\top
                =
                \outpbf{v}{u}
                \\
                \func{f}{\x}
                & =
                \half \qf{\x}{A}
                =
                \half \qf{\x}{(\outpbf{u}{v})}
                =
                \half {(\dotpbf{u}{\x})} (\dotpbf{v}{\x})
                \\
                \implies
                \grad{f}{\x}
                & =
                \half (\dotpbf{u}{\x}) \nabla (\dotpbf{v}{\x})
                + \half (\dotpbf{v}{\x}) \nabla (\dotpbf{u}{\x})
                \\ & =
                \half (\dotpbf{u}{\x}) \mathbf{v}
                + \half (\dotpbf{v}{\x}) \mathbf{u}
                =
                \half (\outpbf{u}{v} + \outpbf{v}{u}) \x
                =
                \half (A + A^\top) \x
                \\
                \implies
                H
                & =
                \nabla^2 \func{f}{\x}
                =
                \half (A + A^\top)
                \\
                \text{Now, } \quad
                &
                H = A
                \iff
                A = A^\top
                \\
                \text{Also, } \quad
                &
                \outpbf{u}{v} = \outpbf{v}{u}
                \iff
                \mathbf{u} = \lambda \mathbf{v}, \; \lambda \in \R
                \\
                \because
                \mathbf{u} \neq \lambda \mathbf{v}
                & \implies
                \outpbf{u}{v} \neq \outpbf{v}{u}
                \implies
                A \neq A^\top
                \implies
                H \neq A
            \end{align*}
            Should have been given \( \mathbf{u} \neq \lambda \mathbf{v} \) instead of \( \mathbf{u} \neq \mathbf{v} \).
        \end{solutionbox}

    \item For \( f \), a \( \C^2 \) function, consider a point \( \xhat \) such that \( \grad{f}{\xhat} = 0 \) and the Hessian is positive definite, \( \hess{f}{\xhat} \succ 0 \).
        Then \( \xhat \) is global minimum of \( f \).

        \begin{solutionbox}
            False.

            From the given conditions, \( f \in \C^2, \grad{f}{\xhat} = 0, \hess{f}{\xhat} \succ 0 \), we can say that \( \xhat \) is a strict local minimum of \( f \) by the second order necessary condition for optimalilty.

            To be able to say that \( \xhat \) is a global minimum of \( f \), we need \( f \) to be convex, i.e., \( \hess{f}{\x} \succeq 0, \; \forall \x \in \operatorname{dom}(f) \), as per the second order sufficient condition for optimality and the second order condition for convexity.
        \end{solutionbox}

    \item Consider the function \( f: \R^2 \rightarrow \R \) with \( \x = {\left[x_{1}, x_{2}\right]}^\top, \func{f}{\x} = {\left(1+x_{1}\right)}^2+b{\left(x_{2}-x_{1}^2\right)}^2 \) with \( b > 0 \).
        The global minimum of \( f \) for \( b = 100 \) and for \( b = 1 \) are different.

        \begin{solutionbox}
            False.
            \vspace{-0.5em}
            \begin{align*}
                \implies
                \grad{f}{\x}
                & =
                {\left[2(1+x_{1}) - 4b x_{1}(x_{2}-x_{1}^2), \; 2b(x_{2}-x_{1}^2)\right]}^\top
                \\
                \implies
                \grad{f}{\xhat}
                & =
                \mathbf{0}
                \implies
                x_2 = x_1^2, \quad 1 + x_1 = 0
                \implies
                \xhat = {\left[-1, 1\right]}^\top
                \\
                \implies
                \func{f}{\xhat}
                & =
                {(1-1)}^2 + b{(1-{(-1)}^2)}^2 = 0, \quad \forall b > 0
            \end{align*}
            There is only one stationary point and the function value at that point doesn't depend on \( b \).
            Loosely, we can see that the global minimum would be independent of \( b \).
            (Provided with additional checks, which we skip here).

            The given function is the Rosenbrock function \( f(x, y) = {(a-x)}^2+b{(y-x^2)}^2 \), with \( a = -1, \; b > 0 \).
            It has a unique global minimum at \( (x, y) = (a, a^2), \; \forall b > 0 \).
        \end{solutionbox}

    \item Suppose global minimum, \( \xhat \), exists for a function \( f \in \C^{1} \).
        \( \xhat \in \{ \x \mid \grad{f}{\x} = 0 \} \).

        \begin{solutionbox}
            True.

            Follows from the first order necessary condition for optimality.
        \end{solutionbox}

    \item Exact line search for minimizing Quadratic function can be implemented only when descent direction is along negative of the gradient.

        \begin{solutionbox}
            False.

            Exact line search can be implemented for any descent direction.
            We have a closed form expression for the case mentioned in the statement, but that doesn't mean we cannot implement exact line search for other descent directions.
        \end{solutionbox}

    \item Armijo-Goldstein procedure for inexact line search will not apply for Quadratic functions.

        \begin{solutionbox}
            False.

            Armijo-Goldstein procedure can be applied to any function.
        \end{solutionbox}

    \item Wolfe procedure for inexact line search is always more efficient than Armijo-Goldstein.

        \begin{solutionbox}
            False.

            There can be cases where Armijo-Goldstein can be more efficient than Wolfe.
        \end{solutionbox}

    \item Strongly convex functions have unique global minimum.

        \begin{solutionbox}
            True.

            Follows from the second order condition for strong convexity and the second order sufficient condition for optimality.

            Can be shown by contradiction, provided the function is non-constant.
        \end{solutionbox}
\end{enumerate}
