\section*{Question 3}

Let \( v_{1}, v_{2} \in \R^{d} \) such that \( v_{1}^{\top} v_{2} = 0 \).
Consider \( \min _{\mathbf{x} \in \R^{d}} f(\mathbf{x})\left( = \frac{1}{2}{\left(\mathbf{x}^{\top} v_{1}\right)}^{2}+t v_{2}^{\top} \mathbf{x}\right) \) where \( t \in \R \) is a constant.
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Compute the Hessian?
        What are the eigenvalues and eigenvectors of the Hessian?
        Is this function convex?
        Justify.

    \item Compute the global minima of this function for all choices of \( t \in \R \)?
        (Your answer should clearly indicate the optimal value, \( f^{*} \) and the optimal point, \( \mathbf{x}^{*} \)).

    \item Define \( g(\mathbf{x}) = f(\mathbf{x})+\frac{1}{2} {\Vert \mathbf{x} \Vert}^{2} \).
        Repeat a and b.
\end{enumerate}

\subsection*{Solution}

\paragraph{Given:}
\begin{equation*}
    \min_{\x \in \R^d} \func{f}{\x},
    \quad
    \func{f}{\x}
    =
    \half {(\dotp{\x}{\mathbf{v}_1})}^2 + t \dotp{\mathbf{v}_2}{\x},
    \quad t \in \R,
    \; \mathbf{v}_1, \mathbf{v}_2 \in \R^d,
    \; \dotp{\mathbf{v}_1}{\mathbf{v}_2} = 0
\end{equation*}

\subsubsection*{(a)}

\begin{align*}
    \implies
    \func{f}{\x}
    & =
    \half \qf{\x}{(\outp{\mathbf{v}_1}{\mathbf{v}_1})} + \dotp{(t \mathbf{v}_2)}{\x}
    \\
    \implies
    \grad{f}{\x}
    & =
    (\outp{\mathbf{v}_1}{\mathbf{v}_1}) \x + t \mathbf{v}_2
    \\
    \implies
    \hess{f}{\x}
    & =
    \outp{\mathbf{v}_1}{\mathbf{v}_1}
\end{align*}

The eigenvalues of the Hessian are \( \norm{\mathbf{v}_1}^2 \) and \( 0 \) with eigenvectors \( \mathbf{v}_1 \) and any vector orthogonal to \( \mathbf{v}_1 \) respectively.

Since the eigenvalues of the Hessian are non-negative, the function \( f \) is convex by the second-order condition for convexity, i.e., \( \hess{f}{\x} \succeq 0, \; \forall \x \in \R^d \).
