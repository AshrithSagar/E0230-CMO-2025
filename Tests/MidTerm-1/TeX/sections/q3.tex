\section*{Question 3}

Let \( v_{1}, v_{2} \in \R^{d} \) such that \( v_{1}^{\top} v_{2} = 0 \).
Consider \( \min _{\mathbf{x} \in \R^{d}} f(\mathbf{x})\left( = \frac{1}{2}{\left(\mathbf{x}^{\top} v_{1}\right)}^{2}+t v_{2}^{\top} \mathbf{x}\right) \) where \( t \in \R \) is a constant.
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Compute the Hessian?
        What are the eigenvalues and eigenvectors of the Hessian?
        Is this function convex?
        Justify.

    \item Compute the global minima of this function for all choices of \( t \in \R \)?
        (Your answer should clearly indicate the optimal value, \( f^{*} \) and the optimal point, \( \mathbf{x}^{*} \)).

    \item Define \( g(\mathbf{x}) = f(\mathbf{x})+\frac{1}{2} {\Vert \mathbf{x} \Vert}^{2} \).
        Repeat a and b.
\end{enumerate}

\subsection*{Solution}

\paragraph{Given:}
\begin{equation*}
    \min_{\x \in \R^d} \func{f}{\x},
    \quad
    \func{f}{\x}
    =
    \half {(\dotp{\x}{\mathbf{v}_1})}^2 + t \dotp{\mathbf{v}_2}{\x},
    \quad t \in \R,
    \; \mathbf{v}_1, \mathbf{v}_2 \in \R^d,
    \; \dotp{\mathbf{v}_1}{\mathbf{v}_2} = 0
\end{equation*}

\subsubsection*{(a)}

\begin{align*}
    \implies
    \func{f}{\x}
    & =
    \half \qf{\x}{(\outp{\mathbf{v}_1}{\mathbf{v}_1})} + \dotp{(t \mathbf{v}_2)}{\x}
    \\
    \implies
    \grad{f}{\x}
    & =
    (\outp{\mathbf{v}_1}{\mathbf{v}_1}) \x + t \mathbf{v}_2
    \\
    \implies
    \hess{f}{\x}
    & =
    \outp{\mathbf{v}_1}{\mathbf{v}_1}
\end{align*}

The eigenvalues of the Hessian are \( \norm{\mathbf{v}_1}^2 \) and \( 0 \) with eigenvectors \( \mathbf{v}_1 \) and any vector orthogonal to \( \mathbf{v}_1 \) respectively.

Since the eigenvalues of the Hessian are non-negative, the function \( f \) is convex by the second-order condition for convexity, i.e., \( \hess{f}{\x} \succeq 0, \; \forall \x \in \R^d \).

\subsubsection*{(b)}

From the first-order necessary condition for optimality, we have
\begin{align*}
    \grad{f}{\xstar}
    & =
    \zero
    \implies
    (\outp{\mathbf{v}_1}{\mathbf{v}_1}) \xstar + t \mathbf{v}_2
    = \zero
    \\
    \implies
    (\dotp{\mathbf{v}_1}{\xstar}) \mathbf{v}_1
    & =
    -t \mathbf{v}_2,
    \quad
    \forall t \in \R,
    \; \mathbf{v}_1, \mathbf{v}_2 \in \R^d,
    \; \dotp{\mathbf{v}_1}{\mathbf{v}_2} = 0
\end{align*}

For \( t = 0 \), we have \( \xstar \in \set{ \x \in \R^d \;\middle|\; \dotp{\mathbf{v}_1}{\x} = 0 } \), and \( \func{f}{\xstar} = 0 \).

For \( t \neq 0 \), \( f \) is unbounded below.

\subsubsection*{(c)}

\begin{align*}
    \implies
    \func{g}{\x}
    & =
    \func{f}{\x} + \half {\norm{\x}}^2
    =
    \half \qf{\x}{(\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I)} + \dotp{(t \mathbf{v}_2)}{\x}
    \\
    \implies
    \grad{g}{\x}
    & =
    (\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I) \x + t \mathbf{v}_2
    \\
    \implies
    \hess{g}{\x}
    & =
    \outp{\mathbf{v}_1}{\mathbf{v}_1} + \I
\end{align*}

From Cayley-Hamilton theorem, it follows that the eigenvalues of the Hessian are \( \norm{\mathbf{v}_1}^2 + 1 \) and \( 1 \) with eigenvectors \( \mathbf{v}_1 \) and any vector orthogonal to \( \mathbf{v}_1 \) respectively.

The function \( g \) is strongly convex since the eigenvalues of the Hessian are strictly positive, i.e., \( \hess{g}{\x} \succeq \I \implies \hess{g}{\x} \succ 0, \; \forall \x \in \R^d \).

Now, we have
\begin{align*}
    \grad{g}{\xstar}
    & =
    \zero
    \implies
    (\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I) \xstar + t \mathbf{v}_2
    = \zero
    \implies
    \hess{f}{\xstar} \, \xstar
    =
    - t \mathbf{v}_2
    \\
    \implies
    \xstar
    & =
    -t \inv{(\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I)} \mathbf{v}_2,
    \quad
    \forall t \in \R
\end{align*}

Now, since
\begin{equation*}
    \hess{f}{\x} \, \mathbf{v}_2
    =
    (\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I) \mathbf{v}_2
    =
    \mathbf{v}_2
\end{equation*}
it follows that \( \mathbf{v}_2 \) is an eigenvector of \( \hess{f}{\x} \) with eigenvalue \( 1 \).

By Sherman-Morrison formula, we have
\begin{align*}
    \inv{(\mathbf{A} + \outp{\mathbf{u}}{\mathbf{v}})}
    & =
    \inv{\mathbf{A}} - \frac{\inv{\mathbf{A}} \outp{\mathbf{u}}{\mathbf{v}} \inv{\mathbf{A}}}{1 + \qf{\mathbf{v}}{\inv{\mathbf{A}}}[\mathbf{u}]},
    \\
    \implies
    \inv{(\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I)}
    & =
    \I - \frac{\outp{\mathbf{v}_1}{\mathbf{v}_1}}{\norm{\mathbf{v}_1}^2 + 1}
    \\
    \implies
    \xstar
    & =
    -t \left( \I - \frac{\outp{\mathbf{v}_1}{\mathbf{v}_1}}{\norm{\mathbf{v}_1}^2 + 1} \right) \mathbf{v}_2
    =
    -t \mathbf{v}_2 + \zero
    \\
    \implies
    &
    \boxed{
        \xstar
        =
        -t \mathbf{v}_2,
        \quad
        \forall t \in \R
    }
    \\
    \implies
    \func{g}{\xstar}
    & =
    \half \qf{\xstar}{(\outp{\mathbf{v}_1}{\mathbf{v}_1} + \I)} + t \dotp{\mathbf{v}_2}{\xstar}
    =
    \half \dotp{\xstar}{(-t \mathbf{v}_2)} + t \dotp{\mathbf{v}_2}{\xstar}
    =
    \half t \dotp{\mathbf{v}_2}{\xstar}
    \\
    \implies
    &
    \boxed{
        \func{g}{\xstar}
        =
        -\half t^2 \norm{\mathbf{v}_2}^2,
        \quad
        \forall t \in \R
    }
\end{align*}
