\section*{Question 4}

Consider \( f(\x) = \frac{1}{2} {\x}^{\top} A \x+b^{\top} \x+c \) where \( A = \sum_{i = 1}^{d} \mu_{i} \u_{i} {\u_{i}}^{\top}, \u_{i} \in \R^{d},{\left\Vert\u_{i}\right\Vert}^{2} = 1, {\u_{i}}^{\top} \u_{j} = 0, i \neq j i \in[d] \).
It is given that \( \mu_{1} \geq \ldots, \mu_{d}, \mu_{1} = 100, \mu_{d} = 1 \).
Note that \( b = \sum_{i = 1}^{n} \beta_{i} \u_{i}, \beta_{i}, c \in \R \).
Fix \( d = 100 \).
Let \( \xstar \) be the global minimum of \( f \).
Consider using \texttt{DESCENT} for minimizing \( f(\x) \) with \( \x^0 = 0, \u^k = -\nabla f\left(\x^k\right) \), and \( \alpha_{k} \) is chosen by exact linesearch.
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Compute \( \x^1 \) in terms of the parameters.
        State any equations which were used to justify the answer.

    \item Consider the expression
        \begin{equation*}
            \frac{f\left(\x^{100}\right)-f\left(\xstar\right)}{f\left(\x^0\right)-f\left(\xstar\right)} \leq r
        \end{equation*}
        What is the smallest value of \( r \) that can be guaranteed.
        Provide justifications.
        State clearly any results you may use.

    \item Consider the problem of minimizing \( h \),
        \begin{equation*}
            h(\y) = f(U \y), U = \left[\u_{1}, \u_{2}, \ldots, \u_{d}\right]
        \end{equation*}
        What is the best value of \( h(\y)-h\left(\y^{*}\right) \) you can obtain using at most \( d \) exact line searches.
\end{enumerate}

\subsection*{Solution}

\subsubsection*{(a)}

\begin{align*}
    \implies
    \x^1
    & =
    \x^0 + \alpha_0 \u^0
    =
    \zero - \alpha_0 \grad{f}{\x^0}
    =
    - \alpha_0 \grad{f}{\zero}
    \\
    \implies
    \grad{f}{\x}
    & =
    \A \x + \b
    \implies
    \grad{f}{\x^0}
    =
    \grad{f}{\zero}
    =
    \A \x^0 + \b
    =
    \b
    \\
    \implies
    \alpha_k
    & =
    \argmin_{\alpha \in \R} \func{f}{\x^{k+1}}
    =
    \argmin_{\alpha \in \R} \func{f}{\x^k + \alpha \u^k}
    =
    - \frac{\dotp{\grad{f}{\x^k}}{\u^k}}{\qf{\u^k}{\A}}
    \\
    \implies
    \alpha_0
    & =
    - \frac{\dotp{\grad{f}{\x^0}}{\u^0}}{\qf{\u^0}{\A}}
    =
    - \frac{\dotp{\b}{(-\b)}}{\qf{(-\b)}{\A}}
    =
    \frac{\norm{\b}^2}{\qf{\b}{\A}}
    \\ & =
    \frac{\norm{\sum_{i = 1}^{d} \beta_i \u_i}^2}{\qf{\left( \sum_{i = 1}^{d} \beta_i \u_i \right)}{\left( \sum_{j = 1}^{d} \mu_j \outp{\u_j}{\u_j} \right)}}
    =
    \frac{\sum_{i = 1}^{d} \beta_i^2}{\sum_{i = 1}^{d} \mu_i \beta_i^2}
    \\
    \implies
    \x^1
    & =
    - \alpha_0 \b
    \implies
    \boxed{
        \x^1
        =
        - \frac{\sum_{i = 1}^{d} \beta_i^2}{\sum_{i = 1}^{d} \mu_i \beta_i^2}
        \sum_{i = 1}^{d} \beta_i \u_i
    }
\end{align*}

\subsubsection*{(b)}

We know,
\begin{equation*}
    \frac{\func{f}{\x^k} - \func{f}{\xstar}}{\func{f}{\x^0} - \func{f}{\xstar}}
    \leq
    {\left( \frac{\kappa - 1}{\kappa + 1} \right)}^{2k},
    \quad \kappa = \frac{\mu_1}{\mu_d}
\end{equation*}

\begin{align*}
    \implies
    \kappa
    & =
    \frac{\mu_1}{\mu_d}
    =
    \frac{100}{1}
    =
    100
    \\
    \implies
    r
    & =
    {\left( \frac{100 - 1}{100 + 1} \right)}^{200}
    =
    \boxed{
        {\left( \frac{99}{101} \right)}^{200}
    }
\end{align*}

\subsubsection*{(c)}

\begin{align*}
    \implies
    h(\y)
    & =
    f(\U \y)
    =
    \half
    \qf{(\U \y)}{\A}
    + \dotp{\b}{(\U \y)}
    + c
    =
    \half
    \qf{\y}{(\qf{\U}{\A})}
    + \dotp{(\dotp{\U}{\b})}{\y}
    + c
\end{align*}

Now, \( \qf{\U}{\A} = \operatorname{diag}(\mu_1, \mu_2, \ldots, \mu_d) \) since \( \U \) is orthogonal, i.e., \( \dotp{\U}{\U} = \outp{\U}{\U} = \I \).

And \( \dotp{\U}{\b} = \sum_{i = 1}^{d} \beta_i \dotp{\U}{\u_i} = \sum_{i = 1}^{d} \beta_i \mathbf{e}_i = {[\beta_1, \beta_2, \ldots, \beta_d]}^\top \).

Now, with \( \y = {[y_1, y_2, \ldots, y_d]}^\top \), we have
\begin{align*}
    \implies
    h(\y)
    & =
    \half
    \sum_{i = 1}^{d} \mu_i y_i^2
    + \sum_{i = 1}^{d} \beta_i y_i
    + c
    \\
    \implies
    \grad{h}{\y}
    & =
    {[\mu_1 y_1 + \beta_1, \mu_2 y_2 + \beta_2, \ldots, \mu_d y_d + \beta_d]}^\top
    \\
    \implies
    \grad{h}{{\y}^\ast}
    & =
    \zero
    =
    {[\mu_1 y_1^\ast + \beta_1, \mu_2 y_2^\ast + \beta_2, \ldots, \mu_d y_d^\ast + \beta_d]}^\top
    \\
    \implies
    {\y}^\ast
    & =
    {\left[ \frac{- \beta_1}{\mu_1}, \frac{- \beta_2}{\mu_2}, \ldots, \frac{- \beta_d}{\mu_d} \right]}^\top
\end{align*}

An exact line search along each coordinate direction \( \mathbf{e}_i \) will give the optimal value along that direction.
Thus, after \( d \) exact line searches along the coordinate directions, we will reach the optimal point \( {\y}^\ast \).
Thus, the best value of \( h(\y) - h({\y}^\ast) \) we can obtain using at most \( d \) exact line searches is \( 0 \).
