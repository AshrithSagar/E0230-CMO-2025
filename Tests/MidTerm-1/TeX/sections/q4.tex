\section*{Question 4}

Consider \( f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^{\top} A \mathbf{x}+b^{\top} \mathbf{x}+c \) where \( A = \sum_{i = 1}^{d} \mu_{i} \mathbf{u}_{i} \mathbf{u}_{i}^{\top}, \mathbf{u}_{i} \in \R^{d},{\left\Vert\mathbf{u}_{i}\right\Vert}^{2} = 1, \mathbf{u}_{i}^{\top} \mathbf{u}_{j} = 0, i \neq j i \in[d] \).
It is given that \( \mu_{1} \geq \ldots, \mu_{d}, \mu_{1} = 100, \mu_{d} = 1 \).
Note that \( b = \sum_{i = 1}^{n} \beta_{i} \mathbf{u}_{i}, \beta_{i}, c \in \R \).
Fix \( d = 100 \).
Let \( \mathbf{x}^{*} \) be the global minimum of \( f \).
Consider using \texttt{DESCENT} for minimizing \( f(\mathbf{x}) \) with \( \mathbf{x}^{(0)} = 0, \mathbf{u}^{(k)} = -\nabla f\left(\mathbf{x}^{(k)}\right) \), and \( \alpha_{k} \) is chosen by exact linesearch.
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Compute \( \mathbf{x}^{(1)} \) in terms of the parameters.
        State any equations which were used to justify the answer.

    \item Consider the expression
        \begin{equation*}
            \frac{f\left(\mathbf{x}^{(100)}\right)-f\left(\mathbf{x}^{*}\right)}{f\left(\mathbf{x}^{(0)}\right)-f\left(\mathbf{x}^{*}\right)} \leq r
        \end{equation*}
        What is the smallest value of \( r \) that can be guaranteed.
        Provide justifications.
        State clearly any results you may use.

    \item Consider the problem of minimizing \( h \),
        \begin{equation*}
            h(\mathbf{y}) = f(U \mathbf{y}), U = \left[\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{d}\right]
        \end{equation*}
        What is the best value of \( h(\mathbf{y})-h\left(\mathbf{y}^{*}\right) \) you can obtain using at most \( d \) exact line searches.
\end{enumerate}

\subsection*{Solution}

\subsubsection*{(a)}

\begin{align*}
    \implies
    \x^{(1)}
    & =
    \x^{(0)} + \alpha_0 \mathbf{u}^{(0)}
    =
    \zero - \alpha_0 \grad{f}{\x^{(0)}}
    =
    - \alpha_0 \grad{f}{\zero}
    \\
    \implies
    \grad{f}{\x}
    & =
    \mathbf{A} \x + \mathbf{b}
    \implies
    \grad{f}{\x^{(0)}}
    =
    \grad{f}{\zero}
    =
    \mathbf{A} \x^{(0)} + \mathbf{b}
    =
    \mathbf{b}
    \\
    \implies
    \alpha_k
    & =
    \argmin_{\alpha \in \R} \func{f}{\xkp}
    =
    \argmin_{\alpha \in \R} \func{f}{\xk + \alpha \mathbf{u}^{(k)}}
    =
    - \frac{\dotp{\grad{f}{\xk}}{\mathbf{u}^{(k)}}}{\qf{\mathbf{u}^{(k)}}{\mathbf{A}}}
    \\
    \implies
    \alpha_0
    & =
    - \frac{\dotp{\grad{f}{\x^{(0)}}}{\mathbf{u}^{(0)}}}{\qf{\mathbf{u}^{(0)}}{\mathbf{A}}}
    =
    - \frac{\dotp{\mathbf{b}}{(-\mathbf{b})}}{\qf{(-\mathbf{b})}{\mathbf{A}}}
    =
    \frac{\norm{\mathbf{b}}^2}{\qf{\mathbf{b}}{\mathbf{A}}}
    \\ & =
    \frac{\norm{\sum_{i = 1}^{d} \beta_i \mathbf{u}_i}^2}{\qf{\left( \sum_{i = 1}^{d} \beta_i \mathbf{u}_i \right)}{\left( \sum_{j = 1}^{d} \mu_j \outp{\mathbf{u}_j}{\mathbf{u}_j} \right)}}
    =
    \frac{\sum_{i = 1}^{d} \beta_i^2}{\sum_{i = 1}^{d} \mu_i \beta_i^2}
    \\
    \implies
    \x^{(1)}
    & =
    - \alpha_0 \mathbf{b}
    \implies
    \boxed{
        \x^{(1)}
        =
        - \frac{\sum_{i = 1}^{d} \beta_i^2}{\sum_{i = 1}^{d} \mu_i \beta_i^2}
        \sum_{i = 1}^{d} \beta_i \mathbf{u}_i
    }
\end{align*}

\subsubsection*{(b)}

We know,
\begin{equation*}
    \frac{\func{f}{\xk} - \func{f}{\xstar}}{\func{f}{\x^{(0)}} - \func{f}{\xstar}}
    \leq
    {\left( \frac{\kappa - 1}{\kappa + 1} \right)}^{2k},
    \quad \kappa = \frac{\mu_1}{\mu_d}
\end{equation*}

\begin{align*}
    \implies
    \kappa
    & =
    \frac{\mu_1}{\mu_d}
    =
    \frac{100}{1}
    =
    100
    \\
    \implies
    r
    & =
    {\left( \frac{100 - 1}{100 + 1} \right)}^{200}
    =
    \boxed{
        {\left( \frac{99}{101} \right)}^{200}
    }
\end{align*}

\subsubsection*{(c)}

\begin{align*}
    \implies
    h(\y)
    & =
    f(\mathbf{U} \y)
    =
    \half
    \qf{(\mathbf{U} \y)}{\mathbf{A}}
    + \dotp{\mathbf{b}}{(\mathbf{U} \y)}
    + c
    =
    \half
    \qf{\y}{(\qf{\mathbf{U}}{\mathbf{A}})}
    + \dotp{(\dotp{\mathbf{U}}{\mathbf{b}})}{\y}
    + c
\end{align*}

Now, \( \qf{\mathbf{U}}{\mathbf{A}} = \operatorname{diag}(\mu_1, \mu_2, \ldots, \mu_d) \) since \( \mathbf{U} \) is orthogonal, i.e., \( \dotp{\mathbf{U}}{\mathbf{U}} = \outp{\mathbf{U}}{\mathbf{U}} = \I \).

And \( \dotp{\mathbf{U}}{\mathbf{b}} = \sum_{i = 1}^{d} \beta_i \dotp{\mathbf{U}}{\mathbf{u}_i} = \sum_{i = 1}^{d} \beta_i \mathbf{e}_i = {[\beta_1, \beta_2, \ldots, \beta_d]}^\top \).

Now, with \( \y = {[y_1, y_2, \ldots, y_d]}^\top \), we have
\begin{align*}
    \implies
    h(\y)
    & =
    \half
    \sum_{i = 1}^{d} \mu_i y_i^2
    + \sum_{i = 1}^{d} \beta_i y_i
    + c
    \\
    \implies
    \grad{h}{\y}
    & =
    {[\mu_1 y_1 + \beta_1, \mu_2 y_2 + \beta_2, \ldots, \mu_d y_d + \beta_d]}^\top
    \\
    \implies
    \grad{h}{\y^\ast}
    & =
    \zero
    =
    {[\mu_1 y_1^\ast + \beta_1, \mu_2 y_2^\ast + \beta_2, \ldots, \mu_d y_d^\ast + \beta_d]}^\top
    \\
    \implies
    \y^\ast
    & =
    {\left[ \frac{- \beta_1}{\mu_1}, \frac{- \beta_2}{\mu_2}, \ldots, \frac{- \beta_d}{\mu_d} \right]}^\top
\end{align*}

An exact line search along each coordinate direction \( \mathbf{e}_i \) will give the optimal value along that direction.
Thus, after \( d \) exact line searches along the coordinate directions, we will reach the optimal point \( \y^\ast \).
Thus, the best value of \( h(\y) - h(\y^\ast) \) we can obtain using at most \( d \) exact line searches is \( 0 \).
