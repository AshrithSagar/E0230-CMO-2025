\section*{Question 4}

Consider \( f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^{\top} A \mathbf{x}+b^{\top} \mathbf{x}+c \) where \( A = \sum_{i = 1}^{d} \mu_{i} \mathbf{u}_{i} \mathbf{u}_{i}^{\top}, \mathbf{u}_{i} \in \R^{d},{\left\Vert\mathbf{u}_{i}\right\Vert}^{2} = 1, \mathbf{u}_{i}^{\top} \mathbf{u}_{j} = 0, i \neq j i \in[d] \).
It is given that \( \mu_{1} \geq \ldots, \mu_{d}, \mu_{1} = 100, \mu_{d} = 1 \).
Note that \( b = \sum_{i = 1}^{n} \beta_{i} \mathbf{u}_{i}, \beta_{i}, c \in \R \).
Fix \( d = 100 \).
Let \( \mathbf{x}^{*} \) be the global minimum of \( f \).
Consider using \texttt{DESCENT} for minimizing \( f(\mathbf{x}) \) with \( \mathbf{x}^{(0)} = 0, \mathbf{u}^{(k)} = -\nabla f\left(\mathbf{x}^{(k)}\right) \), and \( \alpha_{k} \) is chosen by exact linesearch.
Answer the following with justification.
\begin{enumerate}[label= (\alph*)]
    \item Compute \( \mathbf{x}^{(1)} \) in terms of the parameters.
        State any equations which were used to justify the answer.

    \item Consider the expression
        \begin{equation*}
            \frac{f\left(\mathbf{x}^{(100)}\right)-f\left(\mathbf{x}^{*}\right)}{f\left(\mathbf{x}^{(0)}\right)-f\left(\mathbf{x}^{*}\right)} \leq r
        \end{equation*}
        What is the smallest value of \( r \) that can be guaranteed.
        Provide justifications.
        State clearly any results you may use.

    \item Consider the problem of minimizing \( h \),
        \begin{equation*}
            h(\mathbf{y}) = f(U \mathbf{y}), U = \left[\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{d}\right]
        \end{equation*}
        What is the best value of \( h(\mathbf{y})-h\left(\mathbf{y}^{*}\right) \) you can obtain using at most \( d \) exact line searches.
\end{enumerate}

\subsection*{Solution}

\subsubsection*{(a)}

\begin{align*}
    \implies
    \x^{(1)}
    & =
    \x^{(0)} + \alpha_0 \mathbf{u}^{(0)}
    =
    \zero - \alpha_0 \grad{f}{\x^{(0)}}
    =
    - \alpha_0 \grad{f}{\zero}
    \\
    \implies
    \grad{f}{\x}
    & =
    A \x + b
    \implies
    \grad{f}{\x^{(0)}}
    =
    \grad{f}{\zero}
    =
    A \x^{(0)} + b
    =
    b
    \\
    \implies
    \alpha_k
    & =
    \argmin_{\alpha \in \R} \func{f}{\xkp}
    =
    \argmin_{\alpha \in \R} \func{f}{\xk + \alpha \mathbf{u}^{(k)}}
    =
    - \frac{\dotp{\grad{f}{\xk}}{\mathbf{u}^{(k)}}}{\qf{\mathbf{u}^{(k)}}{A}}
    \\
    \implies
    \alpha_0
    & =
    - \frac{\dotp{\grad{f}{\x^{(0)}}}{\mathbf{u}^{(0)}}}{\qf{\mathbf{u}^{(0)}}{A}}
    =
    - \frac{\dotp{b}{(-b)}}{\qf{(-b)}{A}}
    =
    \frac{\norm{b}^2}{\qf{b}{A}}
    \\ & =
    \frac{\norm{\sum_{i = 1}^{d} \beta_i \mathbf{u}_i}^2}{\qf{\left( \sum_{i = 1}^{d} \beta_i \mathbf{u}_i \right)}{\left( \sum_{j = 1}^{d} \mu_j \outp{\mathbf{u}_j}{\mathbf{u}_j} \right)}}
    =
    \frac{\sum_{i = 1}^{d} \beta_i^2}{\sum_{i = 1}^{d} \mu_i \beta_i^2}
    \\
    \implies
    \x^{(1)}
    & =
    - \alpha_0 b
    \implies
    \boxed{
        \x^{(1)}
        =
        - \frac{\sum_{i = 1}^{d} \beta_i^2}{\sum_{i = 1}^{d} \mu_i \beta_i^2}
        \sum_{i = 1}^{d} \beta_i \mathbf{u}_i
    }
\end{align*}

\subsubsection*{(b)}

We know,
\begin{equation*}
    \frac{\func{f}{\xk} - \func{f}{\xstar}}{\func{f}{\x^{(0)}} - \func{f}{\xstar}}
    \leq
    {\left( \frac{\kappa - 1}{\kappa + 1} \right)}^{2k},
    \quad \kappa = \frac{\mu_1}{\mu_d}
\end{equation*}

\begin{align*}
    \implies
    \kappa
    & =
    \frac{\mu_1}{\mu_d}
    =
    \frac{100}{1}
    =
    100
    \\
    \implies
    r
    & =
    {\left( \frac{100 - 1}{100 + 1} \right)}^{200}
    =
    \boxed{
        {\left( \frac{99}{101} \right)}^{200}
    }
\end{align*}

\subsubsection*{(c)}

\begin{align*}
    \implies
    h(\y)
    & =
    f(U \y)
    =
    \half
    \qf{(U \y)}{A}
    + \dotp{b}{(U \y)}
    + c
    =
    \half
    \qf{\y}{(\qf{U}{A})}
    + \dotp{(\dotp{U}{b})}{\y}
    + c
\end{align*}
