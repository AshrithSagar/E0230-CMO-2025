# src/optimisers.jl

import .core
using LinearAlgebra: norm, dot, Symmetric
using ForwardDiff: gradient, hessian

export GradientDescent, ConjugateGradient

# ===== Gradient Descent Optimiser =====
mutable struct GradientDescent <: Optimiser
    Î±::Real
end

mutable struct GradientDescentState <: OptimiserState
    ð±::AbstractVector{<:Real}
    f::Real
    k::Unsigned
    converged::Bool
end

function OptimiserStart(
        opt::GradientDescent,
        func::Function,
        ð±â‚€::AbstractVector{<:Real};
        grad::Union{Nothing, Function} = nothing,
        hess::Union{Nothing, Function} = nothing
)::GradientDescentState
    if grad !== nothing
        âˆ‡f = grad(ð±â‚€)
    else
        âˆ‡f = gradient(func, ð±â‚€)
    end

    return GradientDescentState(
        ð±â‚€, func(ð±â‚€), 0, false
    )
end

function OptimiserStep(
        opt::GradientDescent,
        func::Function,
        state::GradientDescentState;
        grad::Union{Nothing, Function} = nothing,
        hess::Union{Nothing, Function} = nothing
)::GradientDescentState
    ð±â‚– = state.ð±
    Î± = opt.Î±

    if grad !== nothing
        âˆ‡f = grad(ð±â‚–)
    else
        âˆ‡f = gradient(func, ð±â‚–)
    end

    ð±â‚–â‚Šâ‚ = ð±â‚– - Î± * âˆ‡f

    converged = norm(âˆ‡f) < 1e-6

    return GradientDescentState(
        ð±â‚–â‚Šâ‚, func(ð±â‚–â‚Šâ‚), state.k + 1, converged
    )
end

# ===== Conjugate Gradient Optimiser =====
mutable struct ConjugateGradient <: Optimiser end

mutable struct ConjugateGradientState <: OptimiserState
    ð±::AbstractVector{<:Real}
    f::Real
    k::Unsigned
    converged::Bool

    ð::Symmetric{<:Real}
    ð«::AbstractVector{<:Real}
    ð©::AbstractVector{<:Real}
end

function OptimiserStart(
        opt::ConjugateGradient,
        func::Function,
        ð±â‚€::AbstractVector{<:Real};
        grad::Union{Nothing, Function} = nothing,
        hess::Union{Nothing, Function} = nothing
)::ConjugateGradientState
    if grad !== nothing
        âˆ‡f = grad(ð±â‚€)
    else
        âˆ‡f = gradient(func, ð±â‚€)
    end

    if hess !== nothing
        âˆ‡Â²f = hess(ð±â‚€)
    else
        âˆ‡Â²f = hessian(func, ð±â‚€)
    end

    ð = Symmetric(âˆ‡Â²f)
    ð«â‚€ = âˆ‡f
    ð©â‚€ = -âˆ‡f

    return ConjugateGradientState(
        ð±â‚€, func(ð±â‚€), 0, false, ð, ð«â‚€, ð©â‚€
    )
end

function OptimiserStep(
        opt::ConjugateGradient,
        func::Function,
        state::ConjugateGradientState;
        grad::Union{Nothing, Function} = nothing,
        hess::Union{Nothing, Function} = nothing
)::ConjugateGradientState
    ð±â‚– = state.ð±
    ð = state.ð
    ð«â‚– = state.ð«
    ð©â‚– = state.ð©

    Î±â‚– = dot(ð«â‚–, ð«â‚–) / dot(ð©â‚–, ð, ð©â‚–)
    ð±â‚–â‚Šâ‚ = ð±â‚– + Î±â‚– * ð©â‚–
    ð«â‚–â‚Šâ‚ = ð«â‚– + Î±â‚– * ð * ð©â‚–
    Î²â‚–â‚Šâ‚ = dot(ð«â‚–â‚Šâ‚, ð«â‚–â‚Šâ‚) / dot(ð«â‚–, ð«â‚–)
    ð©â‚–â‚Šâ‚ = -ð«â‚–â‚Šâ‚ + Î²â‚–â‚Šâ‚ * ð©â‚–

    converged = norm(ð«â‚–â‚Šâ‚) < 1e-6

    return ConjugateGradientState(
        ð±â‚–â‚Šâ‚, func(ð±â‚–â‚Šâ‚), state.k + 1, converged, ð, ð«â‚–â‚Šâ‚, ð©â‚–â‚Šâ‚
    )
end
